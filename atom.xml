<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[machty's thoughtz]]></title>
  <link href="http://machty.github.com/atom.xml" rel="self"/>
  <link href="http://machty.github.com/"/>
  <updated>2015-01-13T16:39:08-05:00</updated>
  <id>http://machty.github.com/</id>
  <author>
    <name><![CDATA[Alex Matchneer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A life of possibilities]]></title>
    <link href="http://machty.github.com/blog/2015/01/07/a-life-of-possibilities/"/>
    <updated>2015-01-07T14:07:00-05:00</updated>
    <id>http://machty.github.com/blog/2015/01/07/a-life-of-possibilities</id>
    <content type="html"><![CDATA[<h2>More actions shit</h2>

<p>Scenario: mobile app, I have a back button, I don&#8217;t want it to cause
another transition if the app is in mid-transition.</p>

<p>Mid-transition means:</p>

<ul>
<li>data has already started loading for a target route, or</li>
<li>some animation is underway, e.g. liquid fire</li>
</ul>


<p>Present day Ember:</p>

<ul>
<li><code>willTransition</code> hook to check some &#8220;isTransitioning&#8221; state (which you
have to remember to properly set and unset (though ember could/should
probably provide this for you))</li>
<li>some other router reopen hook, override transitionTo behavior. Pretty
crappy.</li>
</ul>


<p>React w react-router:</p>

<pre><code>Router.run(routes, function (Handler) {
  // custom data-loading logic, construct your own chain of promises
  // ...
  // maybe use flatMapLatest so that you're only responding to the
  // most recent transition, and then eventually:
  // React.render(&lt;Handler/&gt;, document.body);

  // this could be modeled by a stream 
});
</code></pre>

<p>Hmm so I want to be able to push values into a stream (let&#8217;s call it raw
stream), but have it only come out the other side if some other&#8230;
stream? predicate? allows it.</p>

<h2>Rx/Bacon Diagrams</h2>

<ul>
<li>Bacon: <code>flatMapConcat</code> (or <code>flatMapWithConcurrencyLimit(1)</code>)</li>
<li>Rx: <code>concatAll</code></li>
</ul>


<p>Flatten, preserve order of previously started streams (buffers later streams)</p>

<pre><code>...{1}
......{2........3}
.........{}
............{4}
{...1..2........3.4}
</code></pre>

<ul>
<li>Bacon: <code>flatMap</code> (or <code>flatMapWithConcurrencyLimit(infinity)</code>)</li>
<li>Rx: <code>mergeAll</code></li>
</ul>


<p>Flatten, don&#8217;t preserve order (no buffering). (limiting concurrency
means buffering).</p>

<pre><code>...{1}
......{2........3}
.........{}
............{4}

produces
{...1..2.....4..3}
</code></pre>

<ul>
<li>Bacon: <code>flatMapLatest</code></li>
<li>Rx: <code>switchLatest</code></li>
</ul>


<p>Flatten, unsubscribe from previous streams, RIGHT when the new stream
begins, not on its first value!</p>

<pre><code>...{1}
......{2........3}
............{4}

produces
{...1..2.....4}
</code></pre>

<p>and</p>

<pre><code>...{1}
......{2........3}
............{.........4}

produces
{...1..2.....4}
</code></pre>

<h2>CircleCI uses ClojureScript/Om</h2>

<p>http://blog.circleci.com/local-state-global-concerns/</p>

<h2>MRR</h2>

<p>Monthly recurring revenue: a way to calculate monthly revenue based off
of multiple dissimilar subscription terms.</p>

<h2>Observable of 1 is a promise</h2>

<p>http://www.infoq.com/presentations/async-programming-netflix</p>

<p>Promises can&#8217;t be cancelled.</p>

<ul>
<li>Work is already being done (in flight)</li>
<li>No way to stop it</li>
</ul>


<p>Observables are lazy. You can build a giant nested logical chunk of
observable but nothing happens until, say, a <code>forEach</code> subscribes to the
result of an observable that all the internal logic kicks into place.
Not true for promises (unless someone implements a lazy thennable).</p>

<p>TODO: hot vs cold observables distinction? does this exist in bacon?</p>

<p>This is how the <code>retry</code> operator works&#8230; in the same way you can call
<code>forEach</code> N times on an observable and kick start all the internal
logic/event handlers, retry will internally do this until it gets a
non-error response.</p>

<p>This is unless promises, which have the <code>.then</code> operator:</p>

<ul>
<li>Regardless of whether you call <code>.then</code>, the promise logic has already started</li>
<li>Calling <code>.then</code> at the same time just subscribes to the single,
already running promise, and <code>.then</code> on an already resolved promise
just returns that.</li>
</ul>


<!--more-->


<h2>phone it in</h2>

<p>http://www.urbandictionary.com/define.php?term=phone+it+in</p>

<blockquote><p>Perform an act in a perfunctory, uncommitted fashion, as if it didn&#8217;t matter.</p>

<p>She sang the National Anthem, but she was just phoning it in as far as I could tell.</p></blockquote>

<h2>ENZ in Trojan ENZ</h2>

<p>https://answers.yahoo.com/question/index?qid=20070211111727AAnr6Kn</p>

<p>It refers to the reservoir tip at the end (ENZ = ends) to prevent fluids
from leaking from rubbing.</p>

<h2>Clojure: &#8220;classes&#8221; not coupled to namespace</h2>

<p>In Clojure, given you have, say, a Person record, you can extend Person
to two different protocols that have the same method name (so long as
those protocols are in two different namespaces. This is a nice because
in, say, Java, you might be implementing methods from N different
interfaces, yet (possibly unnecessarily) sharing the same private data
and other items on the same namespace.</p>

<h2>defrecord</h2>

<pre><code>(ns alex/wat)

(defprotocol ALEX (lol [a]))

(defrecord AlexThing [a b c]
  ALEX
  (lol [a] (println a)))
</code></pre>

<p>err clearly i&#8217;m doing something wrong&#8230; this is null pointer
exceptioning.</p>

<h2>can&#8217;t use recur and get polymorphism</h2>

<p>Unlikely to bite, but worth pointing out that whatever you pass as the
first arg to <code>recur</code> won&#8217;t dynamically dispatch, because <code>recur</code> is
really just a looping construct. I can&#8217;t see how this would actually
matter though? Maybe I don&#8217;t get it.</p>

<h2>ISeq, IPersistentStack</h2>

<p><code>I</code> stands for interface.</p>

<h2>Dynamic Dispatch</h2>

<p>Dynamic dispatch is the selection of which method to run in a
polymorphic setting. Which implementation of <code>foo</code> should run here?</p>

<pre><code>(foo thing)
</code></pre>

<p>From what I understand, Clojure will look for protocols that specify
<code>foo</code> and check if <code>thing</code> is part of that. If a protocol is found but
<code>thing</code> doesn&#8217;t implement it, then boom, but Clojure will also try and
find a multi-method. (TODO: figure out the order of resolution?)</p>

<p>Multimethods can be simple and dispatch on the type of some map
property, or it can run a more complicated functions to determine which
to run. I believe the difference between this and switch-on-type
inheritance is blah blah blah write this later.</p>

<h2>famo.us</h2>

<p>http://famo.us/university/</p>

<p>Baby&#8217;s first famous.</p>

<p>TODO: Engines, Contexts, and Surfaces.</p>

<p>Surfaces</p>

<ul>
<li>Seems to pos abs, 100% width and height by default; size actually
inherits from parent surface/context.</li>
</ul>


<p>State Modifiers</p>

<ul>
<li>seem to just wrap transform matrices</li>
</ul>


<p>http://devchat.tv/js-jabber/128-jsj-famo-us-with-steve-newcomb</p>

<ul>
<li>Rendering in the browser is faster than rendering in the browser

<ul>
<li>Multiplying matrices is CPU-bound</li>
<li>Browsers were meant to lay out text docs w links, etc., that&#8217;s
highly optimized</li>
<li>Layout engine optimized for matrix-based layouts; famo.us layout
engine &#8220;14x&#8221; faster than using CSS-based browser matrices. Why?
Because lots of browser layout stuff <em>isn&#8217;t</em> GPU stuff, but rather
the static text layout stuff that <em>is</em> highly optimized.</li>
</ul>
</li>
<li>Create the scene in JS

<ul>
<li>Post to DOM

<ul>
<li>used for context, text/titles</li>
</ul>
</li>
<li>Post to SVG</li>
<li>Post to WebGL

<ul>
<li>high end graphics</li>
</ul>
</li>
</ul>
</li>
<li>e.g. write in famo.us, use their render tree, render to each thing</li>
<li>vs ionic

<ul>
<li>ionic still relies on DOM</li>
<li>famo.us puts way more into JavaScript, render tree / physics engine
all lives in JS.</li>
<li>future-feasible to run ionic templates in famo.us</li>
</ul>
</li>
<li>rendering in famo.us

<ul>
<li>render the RenderTree</li>
<li>based on that, produce flattest dom possible

<ul>
<li>important to minimize DOM and manage it wisely, else reflow, frame
rate volatility</li>
</ul>
</li>
</ul>
</li>
<li>Physics Engines

<ul>
<li>Unreal / iOS</li>
<li>Native-feeling animations; prevent the feeling of html5</li>
</ul>
</li>
<li><p>Docker</p>

<ul>
<li>docker containers</li>
<li>containing a system, making it highly portable</li>
<li>famous wants a front-end container, e.g. write a carousel container,
implementations might swap as time goes on, fetching all the
libraries to present it happens automatically, etc.</li>
</ul>
</li>
<li><p>align</p>

<ul>
<li>sets anchor point on the element (defaults to upper left corner)</li>
</ul>
</li>
<li>origin

<ul>
<li>sets default 0,0 point that children will be inserted into.</li>
</ul>
</li>
</ul>


<h2>wtf is a graphics context</h2>

<p>Wtf is context is general. Is it unavoidable that this word be used all
the time?</p>

<p>https://developer.apple.com/library/mac/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_overview/dq_overview.html#//apple_ref/doc/uid/TP30001066-CH202-TPXREF131</p>

<p>It comes up in so many graphicsy shit. It kinda just means &#8220;where all
state is stored&#8221;.</p>

<p>You have an Engine, where all the API logic (and maybe hardware
interfacing logic) lives, and you have to feed it different contexts to
operate on.</p>

<p>Both Quartz and famo.us describe the context as a &#8220;drawing destination&#8221;;
in Quartz you could apply the same butterfly-drawing routines to both a
page that you want to print on, or a bitmap graphics context, or a
window context (obtained by your application from the window server).</p>

<h2>Opqaue Data Type</h2>

<p>A data type whose interface is incompletely specified, and only
internal, private fns/methods have access to its internals. e.g. the
Quartz 2D graphics context is an opaque data type; you don&#8217;t/can&#8217;t care
what its internals are but you know you have to supply it to all API
calls (or like OpenGL you might be able to swap out the current graphics
context).</p>

<p>Usually the context includes the canvas that you&#8217;re drawing to, current
fill color, depth of field settings, etc.</p>

<h2>Rx Marbles</h2>

<p>http://rxmarbles.com/</p>

<ul>
<li>sample: Use one stream to control the sample rate of another.</li>
<li>startWith: create a new stream that immediately emits a value</li>
<li>distinct: remember past values, prevent refires</li>
<li>distinctUntilChanged: pretty much just &#8220;uniq&#8221;? prevent duplicate
values in a row</li>
<li>findIndex: return the index of the stream</li>
<li>pausable: zip two stream, true/false values of one starts/stops the
flow of the other: HINT HINT HINT can totally use this for pausing
streams during animations / route transitions / any async shit!</li>
<li>pausableBuffered: remember/buffer the paused items, fire them once
&#8220;true&#8221; comes in!!!!!!!</li>
<li>skip/take: skips or takes the first N elements</li>
<li>skipLast/takeLast: skips or takes the last N elements of the stream;
is smart enough to start emitting values if the buffered size exceeds
N, rather than naively waiting for the stream to run to completeion</li>
<li>skipUntil: ignore shits until the thing comes in.</li>
</ul>


<h2>Reactive MVC</h2>

<p>http://futurice.com/blog/reactive-mvc-and-the-virtual-dom</p>

<ul>
<li>React inspired by reactive programming, ended up mix of interactive
and reactive patterns: TODO what is interactive?</li>
<li>If a change in <code>foo</code> affects <code>bar</code>, and <code>foo</code> is responsible for
telling <code>bar</code>, it&#8217;s interactive; reactive would be <code>bar</code> just
magically responding&#8230;</li>
<li>Interactive: &#8220;module X defines which others modules X affects&#8221;</li>
<li>Reactive: &#8220;module X defines which other modules affects X&#8221;

<ul>
<li>often implemented w EventEmitter (see below)</li>
</ul>
</li>
<li>Controllers are inherently interactive, so get rid of em</li>
<li>Models should export Observables, views should subscribe to model
events.</li>
<li>Fill the gap of controller:</li>
</ul>


<h2>EventEmitter</h2>

<pre><code>#!/usr/bin/env node

var EventEmitter = require('events').EventEmitter;

var ee = new EventEmitter();

ee.on('wattles', function(foo) {
  console.log("received some serious wattles", foo);
});

console.log("numlisteners:", ee.listeners('wattles').length)

setTimeout(function() {
  ee.emit('wattles', 123);
}, 2000);
</code></pre>

<h2>Kefir.js</h2>

<p>http://pozadi.github.io/kefir/</p>

<p>FRP JS lib based on Bacon and Rx but focused on high perf, low mem
usage.</p>

<p>Like Bacon, it distinguishes b/w Properties and EventStreams.</p>

<h2>Cycle.js</h2>

<p>https://github.com/staltz/cycle</p>

<p>This framework is based on the seminal blog
http://futurice.com/blog/reactive-mvc-and-the-virtual-dom</p>

<p>and these slides http://staltz.com/dont-react</p>

<h2>Ideas</h2>

<p>I believe Cycle.js has no concept of a component. The idea that even a
little business / stateful logic lives in the component riles that guy&#8217;s
feathers, but components are crucial for simplifying the mental model
and easily packaging / reusing things. Seems that nuts hasn&#8217;t been
cracked in Cycle.js yet.</p>

<p>UPDATE: components are custom elements in cycle.js
https://github.com/staltz/cycle/blob/master/src/custom-elements.js</p>

<p>(There&#8217;s also unanswered questions about minimizing use of globals,
dependency injection, etc.)</p>

<p>It also seems pretty bad that there&#8217;s lots of examples of reaching in
and grabbing the <code>data-wat-id</code> out of the event target.</p>

<p>So it still seems pretty good that the job of a component is to
translate DOM events into component domain events. A DayCellComponent
in a calendar should translate click/tap events&#8230;</p>

<h2>Taps/Clicks, who&#8217;s responsible?</h2>

<p>Is it up to a component to discern b/w a tap and a click? Some other way
of using input stream?</p>

<p>In Ember we could do something like override &#8220; to mean
different things in different contexts, tap vs click (which I use).</p>

<p>It&#8217;s pretty nice being able to do that; I guess in React you&#8217;d use
<code>withContext</code>?</p>

<p>This seems like a nice hookable thing for event delegation&#8230; oh wait it
already is in ember.</p>

<p>Maybe &#8220; should declare an input stream that you can hook into
n the component?</p>

<p>HERE IS a very concrete question I&#8217;d like to resolve:</p>

<p>Given:</p>

<ul>
<li>You have a list of items</li>
<li>Selecting an item expands an item</li>
<li>Only one item can be expanded at a time</li>
<li>In a desktop setting, &#8220;selections&#8221; happen via clicks, in a mobile
setting, &#8220;selections&#8221; happen via taps</li>
</ul>


<p>You&#8217;ll probably model this with:</p>

<ul>
<li>a ListComponent</li>
<li>ItemComponents</li>
</ul>


<p>My beef (and TODO list):</p>

<ul>
<li>Does it make sense for <code>ItemComponent</code>s to attach their own event
listeners?

<ul>
<li>What if they just got a stream of input? That means something
outside could:

<ul>
<li>decide whether taps or clicks were to be used for selection,
and child components would just receive a selectionEventStream
that they could respond to

<ul>
<li>Hmmm, but given the constraint</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Note that iOS let&#8217;s you suspend/resume touch interactions in general:
http://iosdevelopertips.com/event-handling/suspend-touch-events.html
But I think we want something more granular.</p>

<p>There seems to be a tug of war between re-render everything and
streamify everything. E.g. you diff the data and get per item stream
updates of isExpanded streams&#8230; so that you can re-render only those
items&#8230;? What&#8217;s the goal of this?</p>

<p>If you have virtual dom and re-render all the time, then when you do
want streams?</p>

<ul>
<li>Business logic, series of queries, async, and updating properties
along the way. Maybe UI isn&#8217;t much of a place for streams?
Unless I can show that there is UI not elegantly handled by virtual
DOM, I should drop trying to force streams into UI somehow&#8230;</li>
<li>It still seems like there&#8217;s a place for it though in app state in
general, particularly in actions management.

<ul>
<li>In particular, I never want to have to set some isLoading flag every
again. I should never have to check timerIds and cancel them ever
again. I should never have a bunch of transient properties every
again. Actions should jump start some process and you can listen to
their completion. Everything else: virtual dom (and virtual dom
can/should depend on these properties that you expose). Virtual DOM
state is explicit.</li>
</ul>
</li>
</ul>


<p>OK WE ARE GETTING CLOSE: handlebars/htmlbars templates have the benefit
of declaring loud and clear which properties are depended on for
rendering. If any of those properties change, it amounts to an implicit
setState. Otherwise there&#8217;s no need to re-render (this is all still
implying that KVO exists in Ember somehow).</p>

<pre><code>dismissStream = timer(2000).map(() =&gt; false)
newActionStreams = ... something that fires when a new event starts
// newActionStreams = filter(UIevents, () =&gt; actionIsUnderway???)
postActionMessageStream = race(dismissStream, newActionStreams)
</code></pre>

<p>OK OK OK so what I want is a way to express</p>

<ul>
<li>progression of time in a template</li>
<li>current values, certain flags, etc, without setting boolean properties</li>
</ul>


<p>Let&#8217;s say we have a stream of status on some complex multi-step async
process:</p>

<pre><code>observable = someUIEvent.flatMapLatest(some ajax with retries)
                        .followedBySomeOtherAsync
                        .followedBySomeOtherThing

// maybe...
observable = someUIEvent(someAjaxThing).flatMapLatest(some ajax with retries).state("stepOne")
                        .followedBySomeOtherAsync.state("stepTwo")
                        .followedBySomeOtherThing.state("stepThree")
</code></pre>

<p>There&#8217;s not really a nice mechanism for representing this in a
template without a lot of manual <code>.set</code>s. It&#8217;s like we want some kind of
switch statement?</p>

<pre><code>    &lt;h3&gt;Loading  items from server&lt;/h3&gt;

    &lt;p&gt;
      Here are all the step one things.
    &lt;/p&gt;

    &lt;h3&gt;Please select from the following options:&lt;/h3&gt;

    &lt;ul&gt;

        &lt;li &gt;&lt;/li&gt;

    &lt;/ul&gt;
</code></pre>

<p>Does this make sense? Maybe we just want / need a switch statement in
general. This would also have to work with LiquidFire as well&#8230;</p>

<p>But what the above demonstates is:</p>

<ul>
<li>scoped properties linked to progressive states within an
observable/promises</li>
<li>no global/manual flag management</li>
<li>no timer ids to set and reset</li>
</ul>


<p>TODO:</p>

<ul>
<li>Just have to find the sweet spot with plugging this stuff into ember
in a way people prefer.</li>
<li>How to you go back to a previous &#8220;state&#8221;? What is a &#8220;state&#8221;? How do
people normally model state machines in rx?</li>
</ul>


<p>https://github.com/logicalguess/rx-state-machine</p>

<h2>The silk road</h2>

<p>http://www.wired.com/2015/01/why-silk-road-trial-matters/</p>

<ul>
<li>Ross Ulbricht

<ul>
<li>aka Dread Pirate Roberts</li>
<li>accused of running silk road</li>
<li>accused of hiring hitmen for 2-6 killings, none which were carried out</li>
<li>defended by attorney Joshua Dratel, financed by a bitcoin mogul</li>
</ul>
</li>
<li>Silk Road

<ul>
<li>anonymized by bitcoin and Tor</li>
<li>copycat markets: Evolution and Agora</li>
</ul>
</li>
<li>Trial

<ul>
<li>Will highlight/expose vulnerabilities for other sites to learn from</li>
<li>Will highlight/set precedent of whether screenshots are admissible
for evidence</li>
</ul>
</li>
<li>Fourth amendment:

<ul>
<li>The Fourth Amendment (Amendment IV) to the United States Constitution is the part of the Bill of Rights that prohibits unreasonable searches and seizures and requires any warrant to be judicially sanctioned and supported by probable cause.</li>
<li>Warrantless searches apply</li>
<li>FBI pretty much admitted to hacking the site without warrant to
obtain IP, find dread pirate</li>
<li>FBI investigators likely to be questions, 4th amendment repeated again</li>
</ul>
</li>
<li>Linked in profile: https://www.linkedin.com/in/rossulbricht</li>
<li>Breaking Bad

<ul>
<li>Ulbricht first sold shrooms on silk road, then</li>
<li>expansion into anarchist community / drug market</li>
<li>hiring hitmen?</li>
</ul>
</li>
</ul>


<h2>React keys vs refs</h2>

<p>Clojure has a concept of refs (and similar concurrency primitives) to
add the concept of identity to a value that changes over time (given
that Clojure has immutable data types but still needs to synchronize
and share data b/w threads).</p>

<p>In React, you use keys to distinguish array values so that your
renders don&#8217;t do weird things; how would this work given immutable
datatypes?</p>

<ul>
<li>Well, if only the array changes but the items in that array are the
same, then you&#8217;re fine</li>
<li>But if, say, the selected item has modified, then the previous
reference is invalid/points to an older version of the thing</li>
</ul>


<p>Maybe there&#8217;s a sexy way we could handle this in ember&#8230; maintain a ref
to a changing value.</p>

<h2>Observables and Observables</h2>

<ul>
<li>Observable

<ul>
<li>changing values over time</li>
</ul>
</li>
<li>Observer

<ul>
<li>consumes observers</li>
</ul>
</li>
</ul>


<p>Observables don&#8217;t do anything on their own until they&#8217;re consumed in
some way. SO WEIRD. But the following doesn&#8217;t do anything&#8230;</p>

<pre><code>var obs = Rx.Observable.return('wat').do(function(a) {
    console.log("do", a);
});
</code></pre>

<p>until you add</p>

<pre><code>function K() {}
obs.forEach(K);
obs.forEach(K);
</code></pre>

<p>So what I&#8217;d like is a statemachine with its own stream events, w auto
desubscribe/dispose/etc</p>

<pre><code>var defaultState = {
};

var defaultState = {
};

var states = {
  defaultState: {
  },

  editingItem: function() {
    // TODO: define streams/actions for leaving this state
    // tapping another item leaves it....
  },

  savingItem: {
  }
};
</code></pre>

<h2>Bacon state machine</h2>

<p>http://jsbin.com/qihabo/2/edit?html,js,console,output</p>

<pre><code>Bacon.fromArray([1,2,3]).withStateMachine(0, function(sum, e) {
   if (e.hasValue()) {
       return [sum + e.value(), []];
   } else if (e.isEnd()) {
       return [sum, [new Bacon.Next(function() { return sum; }), e]];
   } else {
       alert("asd")
       return [sum, []];
   }
}).log();
</code></pre>

<p>TL;DR it&#8217;s a state machine only for computing the next value of stream,
it&#8217;s not some implementation of a state machine or anything like that.</p>

<p>Could be useful in the rare cases where you can&#8217;t represent what you
want in the form of other stream operators.</p>

<h2>Sequences of Coincidence</h2>

<p>http://www.introtorx.com/Content/v1.0.10621.0/17_SequencesOfCoincidence.html#SequencesOfCoincidence</p>

<p>I think this is what I&#8217;ve been looking for; handling actions differently
if they occur within the &#8220;window&#8221; of another event, e.g. &#8220;handle these
actions if they happen while a given route is active (and the router is
stable / not loading transitions)&#8221;.</p>

<p>Buffers are conceptually grouped with window operators:</p>

<p>It converts a stream of values into a stream of buffered arrays of
values, buffered according to time, or some buffer size, or whatever.
In other words buffer takes <code>IObservable&lt;T&gt;</code> and returns
<code>IObservable&lt;IList&lt;T&gt;&gt;</code>.</p>

<pre><code>source|-0-1-2-3-4-5-6-7-8-9|
result|-----0-----3-----6-9|
            1     4     7
            2     5     8
</code></pre>

<p>The analogy is that a window is open when Buffer receives first value,
and window closes when buffer&#8217;s full or time elapses.
Windows return <code>IObservable&lt;IObservable&lt;T&gt;&gt;</code></p>

<p>So the subscribe handler of a window passes you the observable, rather
than the already flattened List that Buffer would give to you.</p>

<p>So in rxjs: http://jsbin.com/colora/3/edit</p>

<pre><code>var source = Rx.Observable.timer(0, 50)
  .window(function () { return Rx.Observable.timer(125); })
  .take(4)
  .flatMap(function (x) { return x.toArray(); });

var subscription = source.subscribe(
  function (x) {
    console.log('Next: ' + x);
  },
  function (err) {
    console.log('Error: ' + err);
  },
  function () {
    console.log('Completed');
  });
</code></pre>

<p>produces</p>

<pre><code>"Next: 0,1,2"
"Next: 3,4,5"
"Next: 6,7"
"Next: 8,9,10"
</code></pre>

<p>Why 6,7? Because the initial timer w 50ms period never stops firing:</p>

<pre><code>starting timer: 0----50---100--150--200--250--300--350--400--450--500--550--600--650
close timer:    0-----------125----------250---------375----------500
window1:        0    1     2 |
window2:                        3    4   5|
window3:                                       6   7  |
window4:                                                 8   9  10|
</code></pre>

<p>The 50ms period never stops firing and it&#8217;s on the third iteration where
because of the relative late start of the next period timer firing after
the beginning of the 125ms close interval, it can only get 2 events in
before the stream closes.</p>

<p>SO WTF do I want to do with this: something about windows for
currently active route states? Does that make sense?</p>

<p>There&#8217;s apparently a version that lets you overlap windows&#8230; why would
this be useful? I don&#8217;t know! And it doesn&#8217;t seem like it&#8217;s implemented
in RxJS, but the idea is you provide an observable that fires when a
window should be opened, and a close observer that runs for each new
window, so if your close fires at a larger interval than opens happen,
then you&#8217;ll have overlapping streams, which I guess means, say, a &#8220;3&#8221; or
&#8220;4&#8221; might be fired on multiple streams sprung up.</p>

<h2>RxJs Joins</h2>

<p>Note that all of this is part of the <code>coincidence</code> lib of Rx.</p>

<p>Join operator joins too sequence; zip is kind of a join, but zip is based
on index, and join does sequences.</p>

<p>zip accepts</p>

<ul>
<li>a &#8220;left&#8221; observable: each value produced opens another window</li>
<li>a &#8220;right&#8221; observable: produces values that pair open with the values
of left, while left stream is still open.</li>
<li>a &#8220;left duration&#8221; fn: returns an observable whose first value or
closing will close the left-generated observable (note: it doesn&#8217;t
even matter what type the closing value/observable is)</li>
</ul>


<p>So &#8220;right&#8221; values that start after &#8220;left&#8221; values will still be joined
with cached, previously opened windows. e.g.</p>

<pre><code>L 0-1-2-3-4-5-
</code></pre>

<p>plus</p>

<pre><code>R --A---B---C-
</code></pre>

<p>produces</p>

<pre><code>0, A
1, A
0, B
1, B
2, B
3, B
0, C
1, C
2, C
3, C
4, C
5, C
</code></pre>

<p>Interesting:</p>

<blockquote><p>Now it seems fairly obvious that, if I immediately closed the window by returning Observable.Empty<Unit>, or perhaps Observable.Return(0), windows would never be opened thus no pairs would ever get produced. However, what could I do to make sure that these windows did not overlap- so that, once a second value was produced I would no longer see the first value? Well, if we returned the left sequence from the leftDurationSelector, that could do the trick. But wait, when we return the sequence left from the leftDurationSelector, it would try to create another subscription and that may introduce side effects. The quick answer to that is to Publish and RefCount the left sequence. If we do that, the results look more like this.</p></blockquote>

<p>In Rx the default is to always create a new subscription; it&#8217;s NOT like
a promise whereby a singleton promise has already started firing.
Solution: use Publish.</p>

<p>https://github.com/Reactive-Extensions/RxJS/blob/master/doc/api/core/operators/publish.md</p>

<p>Publish allows for the sharing of subscriptions.</p>

<p>So my question is&#8230; does publishing on its own start its own
subscription? Naw, I think you need to call <code>connect</code> once you&#8217;ve added
all your subscribers.</p>

<p>http://jsbin.com/colora/6/edit</p>

<pre><code>var obs = Rx.Observable.return('wat').do(function(a) {
  console.log("do", a);
});

var p = obs.publish();

function K() {}
p.forEach(K);
p.forEach(K);gma


// uncomment this to actually fire side effects
//p.connect();
</code></pre>

<p>refCount is hard to describe&#8230;</p>

<p>http://stackoverflow.com/questions/7509503/how-can-i-create-an-rx-observable-which-stops-publishing-events-when-the-last-ob/7510245#7510245</p>

<p>Publish as in pub sub&#8230; you want to stop consuming resources once
everyone&#8217;s stopped subscribing?</p>

<p>http://www.zerobugbuild.com/?p=144</p>

<p>Publish creates a hot subscription, refCount makes it close when
subscribers are zero.</p>

<p>If you don&#8217;t do <code>refCount</code>, you have to do <code>connect</code>. If you don&#8217;t do
<code>connect</code> or <code>refCount</code>, I think the subscription never gets activated,
or something? I still don&#8217;t get it&#8230;</p>

<p>Anyway, back to join:</p>

<blockquote><p>The quick answer to that is to Publish and RefCount the left sequence. If we do that, the results look more like this:</p></blockquote>

<pre><code>left  |-0-1-2-3-4-5|
right |---A---B---C|
result|---1---3---5
          A   B   C
</code></pre>

<p>TL;DR once you start using right duration selector, you can implement
other operators.</p>

<p>TODO: watch
http://channel9.msdn.com/Shows/Going+Deep/Bart-De-Smet-MinLINQ-The-Essence-of-LINQ</p>

<h2>console.log printf</h2>

<p>Saw this in Rx docs: https://github.com/Reactive-Extensions/RxJS/blob/master/doc/api/core/operators/timer.md</p>

<pre><code>console.log("wat %s", 3);
</code></pre>

<p>Prints &#8220;wat 3&#8221; (but jsbin&#8217;s console.log override is not smart enough to
substitute).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Everything and nothin]]></title>
    <link href="http://machty.github.com/blog/2015/01/04/everything-and-nothin/"/>
    <updated>2015-01-04T22:26:00-05:00</updated>
    <id>http://machty.github.com/blog/2015/01/04/everything-and-nothin</id>
    <content type="html"><![CDATA[<h2>Indemnity</h2>

<p>http://en.wikipedia.org/wiki/Indemnity</p>

<p>Where indemnitor A pays B a sum of money to cover losses suffered by B
(regardless of whether A caused/was responsible for the loss).
Car insurance is an example. Life insurance is not, since the payout is
not based on any &#8220;valuation&#8221; of the life..</p>

<p>HMOs basically get involved with health care options to try and secure
efficient/cheap means of good healthcare, rather than traditional
indemnity insurance, which would, uhh, just pay out after the fact and
not get involved beforehand?</p>

<h2>HMO act of 1973</h2>

<p>http://en.wikipedia.org/wiki/Health_Maintenance_Organization_Act_of_1973</p>

<ul>
<li>signed by nixon</li>
<li>intended to curb medical inflation</li>
<li>provided grants/loans to start an HMO, removed certain state
restrictions for federally qualified HMOs</li>
<li>required employers w 25+ employees to offer federally certified HMO
options <em>if</em> the employer was already offering traditional insurance</li>
<li>did NOT require employers to offer health insurance</li>
<li>established/solidied the term HMO</li>
</ul>


<h2>Humble Bundle</h2>

<p>http://en.wikipedia.org/wiki/Humble_Bundle</p>

<p>Series of collections (bundles) of digital thingers, often games,
distributed via pay-what-you-want. The more you pay, the more that&#8217;s
opened up. Configurable, but most pay goes to developers, some to
charity, some to Humble.</p>

<h2>What is JLD in Empire Blue?</h2>

<p>JLD is a network code representing the Pathway Enhanced network of
doctors, pharmaceuticals, etc.</p>

<p>The ID number on the card is network + member ID.</p>

<p>1-866-755-2680 Tech support for the website.</p>

<p>Even though they don&#8217;t put it on the card this year, my co-pay is $30,
which I always pay. After I blast through deductible, co-insurance kicks
in. But preventative visits to PCP are freeeeee. But if it&#8217;s not
preventative, the doctor will submit it to insurance as a different
code, which I have to pay for out of deductible. Otherwise shit is
freeeee.</p>

<p>What&#8217;s the difference between Empire and Anthem?</p>

<p>There&#8217;s Empire BlueCross BlueShield and Anthem BlueCross BlueShield?</p>

<p>OK OK OK BlueCross BlueShield is a federation (bigass organization where
smaller groups have decent autonomy). Was originally Blue Cross in 1929,
and a separate Blue Shield in 1948, and they merged in 1982.</p>

<p>Anthem is their only publicly traded company, and within New York they
operate as &#8220;Empire&#8221; BlueCross. Jesus Christ these companies are too
fucking huge.</p>

<ul>
<li>Blue Cross Blue Shield

<ul>
<li>Anthem (publicly traded)

<ul>
<li>Empire Blue Cross Blue Shield (operates out of New York)</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>Difference b/w renter&#8217;s and home owner&#8217;s insurance</h2>

<p>http://coverhound.com/insurance-learning-center/homeowners-vs-renters-insurance-a-big-difference</p>

<ul>
<li>Renters

<ul>
<li>You insure belongings, not the building</li>
</ul>
</li>
<li>Homeowners

<ul>
<li>You insure the building, and the belongings

<ul>
<li>Hence it&#8217;s way more expensive</li>
</ul>
</li>
<li>Insurance is often mandatory if you have a mortgage, but probably
not legally required if you straight up own the whole thing.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OmniFocus et al]]></title>
    <link href="http://machty.github.com/blog/2015/01/02/omnifocus-et-al/"/>
    <updated>2015-01-02T16:27:00-05:00</updated>
    <id>http://machty.github.com/blog/2015/01/02/omnifocus-et-al</id>
    <content type="html"><![CDATA[<h2>OmniFocus</h2>

<ul>
<li>Quick action add: ctrl+option+space</li>
<li>Send to inbox: Ctrl+F6

<ul>
<li>Note this is a Service that some reason not all apps implement?
But it works in Chrome so I think we&#8217;re good</li>
</ul>
</li>
<li>Once you assign an inbox item to a project, it&#8217;s removed from the project</li>
<li>Someday maybe:

<ul>
<li>Put in single action project, mark project as on hold.</li>
</ul>
</li>
<li>So projects can have an On Hold status&#8230; but there&#8217;s also a waiting context?

<ul>
<li>Ah, both projects and contexts can choose what status they are&#8230;
from the same list of possible statuses.</li>
</ul>
</li>
<li>How does the repeat work? &#8220;1 week&#8221;? Why does it say multiple values?

<ul>
<li>Ah I should probably do this one at a time. The way it works is once
you complete, it&#8217;ll immediately schedule another one.</li>
</ul>
</li>
</ul>


<p>OmniFocus is implements GDT: <a href="http://gettingthingsdone.com/">Getting Things Done</a></p>

<h2>Elephants in Trees</h2>

<p>Why do you never see elephants hiding in trees?</p>

<p>Because they&#8217;re very good at it.</p>

<!--more-->


<h2>ASasidoasidj</h2>

<p>http://www.consumerreports.org/cro/2013/12/do-you-need-to-register-new-products-you-buy/index.htm</p>

<p>http://abcnews.go.com/Business/register-product-christmas/story?id=21348313</p>

<p>For software it probably makes more sense since they&#8217;ll have your name
on file to let you know of any major security updates / breaches and
what not, but often warranties don&#8217;t actually require registering a
product. It&#8217;s decently likely the company just wants to sell info to
marketers, making money off of you and subjecting you to spam.</p>

<h2>DirectInput vs XInput</h2>

<p>http://en.wikipedia.org/wiki/DirectInput#DirectInput_vs_XInput</p>

<p>Relevant because I just got a wireless gamepad that may or may not work
with Mac. Apparently Mac only supports DirectInput, but either way these
shits are Microsoft API for interacting with input, DirectInput being
the deprecated one, XInput being the new.</p>

<p>This is the most useless shit I could have blogged / reminded myself
about. Why do I do it?</p>

<h2>Pulmonology</h2>

<p>http://en.wikipedia.org/wiki/Pulmonology</p>

<p>Medical specialty dealing in the heart.</p>

<h2>Insurance, carriers, etc</h2>

<p>What&#8217;s a carrier? What&#8217;s the relationship b/w carrier and, e.g.
BlueCross BlueShield? What&#8217;s a PPO?</p>

<p>http://www.ehealthinsurance.com/health-plans/ppo/</p>

<p>http://www.ehealthinsurance.com/health-plans/hmo/;</p>

<p>HMO: Health Maintenance Organization: broad network of healthcare
services; broader range of preventative care than something else. You
choose a Primary Care Physician, and they must refer you to specialists.</p>

<p>Managed care: http://en.wikipedia.org/wiki/Managed_care</p>

<p>It&#8217;s a US-only term describing a bunch of techniques to keep health care
costs down; incentives for physicians and patients alike, preventative
care, etc. Spurred on by HMO act of 1973</p>

<p>https://nfb.org/images/nfb/publications/vodold/mngdcare.htm</p>

<p>Managed care:</p>

<ul>
<li>explicit standards for selecting providers</li>
<li>formal programs for ongoing quality review / improvement</li>
<li>preventative care, keeping enrollees healthy to reduce use of services</li>
<li>financial incentives for enrollees to use providers and procedures
covered by the plan.</li>
</ul>


<p>Managed care integrates financing and delivery of health care services.
It is implemented/used/referenced by:</p>

<ul>
<li>HMOs (Health Maintenance Organizations): provides wide range of services
for fixed, periodic payment (like monthly insurance payments)</li>
<li>PPOs (Preferred Provider Organizations): groups of hospitals,
physicians, etc., who contract with insurer, employer, 3rd party
admin, etc, to provide health care to all insured</li>
<li>POS plans (point of service): combo of HMO+PPO&#8230;?</li>
</ul>


<p>Features common to managed care:</p>

<ul>
<li>pre-authorization (let insurer know before you do some surgery, etc)</li>
<li>rigorous utilization review&#8230;?</li>
<li>emphasis on use of primary physicians (why? what does this offer?)</li>
</ul>


<p>OK I TAKE THIS ALL BACK this was a shitty little abstract to link to.</p>

<p>Indemnity:</p>

<ul>
<li>security or protection against a loss or other financial burden</li>
<li>security against or exception of legal responsibility of one&#8217;s actions</li>
<li>payment to victor of war as a condition for PEACE</li>
</ul>


<h2>Family Medicine</h2>

<p>http://en.wikipedia.org/wiki/Family_medicine</p>

<p>It means a physician for people of all ages.</p>

<h2>Quorum</h2>

<p>the minimum number of members of an assembly or society that must be present at
any of its meetings to make the proceedings of that meeting valid.</p>

<p>e.g. House of Cards quorum.</p>

<h2>Random fucking World MasterCard benefits</h2>

<ul>
<li>Auto rental collision damage waiver

<ul>
<li>Must use applicable card for entire rental transaction</li>
<li>Decline rental company&#8217;s collision waiver</li>
<li>&#8220;For most cards, the coverage is secondary, meaning that if you have car insurance, you have to file a claim there first (and your premium may go up). But your credit card should step in and pick up where your auto insurer leaves off, paying the tab for your deductible, towing charges and other fees. However, as many frustrated cardholders have learned, the fine print can be tricky. Credit card companies have their own restrictions and exclusions and they, too, often refuse to pay some types of fees levied by car rental companies.&#8221;</li>
<li>If you <em>don&#8217;t</em> have primary insurance, you can use CDW as primary so
long as you have a notarized report at time of incident that you had
no insurance. Why notarized? Not just because Chase is a dick but
because people might not want to report to their primary insurance
because their rates will go up.</li>
</ul>
</li>
</ul>


<p>What&#8217;s a declaration page?</p>

<p>It&#8217;s info from the car insurance company summarizing benefits, which car
they apply to, which drivers, address of driver, summary of coverage,
liability limits.</p>

<ul>
<li><p>Purchase protection</p>

<ul>
<li>Items purchased w card covered in case of theft, damage, accidental loss</li>
<li>Will replace/repair/reimburse up to $500 per claim, $50,000 per account</li>
<li>Secondary&#8230; if you have preexisting applicable insurance, that must come first</li>
<li>Not covered:

<ul>
<li>plants/animals, antiques, boats/cars/autos/motorized vehicles,
software, items purchased for resale, &#8220;mysteriously disappearing&#8221;
items (where no evidence of wrongful act), <em>items lost by common
carrier (like USPS)</em>, jewelry/watches stolen from non-carry-on
luggage, items lost from war/terrorism/rebellion/confiscation/earthquakes,
medical equipment, perishables/consumables/perfumes/cosmetics,
travelers checks/cash/tickets, <em>used/pre-owned items</em></li>
</ul>
</li>
<li>Needed for reimbursement:

<ul>
<li>claim form</li>
<li>copy of card receipt</li>
<li>copy of itemized store receipt</li>
<li>copy of police report (made within 48 hours of incident), or
whatever kind of report</li>
</ul>
</li>
<li>If you have personal/primary insurance:

<ul>
<li>You must use it (homeowner&#8217;s/renter&#8217;s/automabile), and the CC
company will cover any leftover deductible, or even the item
itself if you&#8217;ve exhausted the insurance.</li>
</ul>
</li>
</ul>
</li>
<li><p>Trip Cancellation/Interruption</p>

<ul>
<li>Trip cancellation is before the trip starts, interruption is during, derp</li>
</ul>
</li>
<li><p>Price Protection:</p>

<ul>
<li>e.g you buy something, and then a printed ad lists it for less, then
within 90 days of purchase you can make a claim and get reimbursed
the difference.</li>
<li>Pretty cool, too lazy to describe the fine print</li>
</ul>
</li>
<li><p>Lost luggage</p>

<ul>
<li>Applies to both checked or carry on</li>
</ul>
</li>
<li><p>Baggage delay</p>

<ul>
<li>if baggage delayed 6+ hours</li>
<li>You&#8217;re reimbursed for the emergency purchase of &#8220;essential items&#8221;</li>
<li>Up to $100 per day, max 3 days</li>
<li>Emergency items:

<ul>
<li>clothing, toiletries, charging cables</li>
</ul>
</li>
<li>Not covered:

<ul>
<li>contacts, hearing aids, artificial teeth, cash, checks,
securities, jewelry,</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>By a person or persons</h2>

<p>Legalese. I wanna use this is a sentence.</p>

<h2>iOS app manager view</h2>

<p>It has Recent contacts at the top, and swipe it right and you can see
Favorites! Why don&#8217;t I use this? If you tap it, it reveals option to
Call or Message. SO GOOD.</p>

<h2>What&#8217;s Rust named after?</h2>

<p>A RobUST fungus:</p>

<p>http://www.reddit.com/r/rust/comments/27jvdt/internet_archaeology_the_definitive_endall_source/</p>

<h2>Sky Ferreira and Charli XCX doin shit</h2>

<p>http://www.musictimes.com/articles/10302/20140916/sky-ferreira-reveals-charli-xcx-collaboration-will-sound-very-tatu.htm</p>

<p>t.A.T.u. is a Russian duo.</p>

<p>http://en.wikipedia.org/wiki/T.A.T.u.</p>

<p>Sky Ferreira, Charli XCX, and Grimes. They&#8217;re all buds.</p>

<h2>A skillet is a fucking frying pan</h2>

<p>And the difference between a fry(ing) pan/skillet and a saute pan is
that saute pans have 90 degree angle sides, where&#8217;s skillets are sloped.
Skillets aren&#8217;t designed to hold a lot of liquid and are optimized for
reaching in and flipping shit, like eggs and pancakes n stuff.</p>

<p>Some people think there <em>is</em> a difference b/w skillet and frying pan,
that pans are flimsy but skillets are thick.</p>

<h2>Colanders and Strainers</h2>

<p>Colanders are the often plastic bowls with holes for draining noodles
and rice. I can&#8217;t believe I didn&#8217;t know that. <code>say colander</code>. Colanders
usually stand up on their own; they have a <em>footed base</em>.</p>

<p>Strainers are generally wire mesh, have a handle, and are used for, say,
catching lemon seeds when squeezing a lemon.</p>

<h2>Sinecure</h2>

<p>a position requiring little or no work but giving the holder status or financial benefit.</p>

<p>Reference: http://consultingbyrpm.com/blog/2014/12/timid-guys-finish-last.html</p>

<p>Cushy sinecures. White guys getting cushy sinecures.</p>

<p>Sinecure : sine cura : &#8220;without care&#8221;</p>

<h2>Easy ass awesome spicy noodles</h2>

<p>http://www.budgetbytes.com/2012/08/spicy-noodles/</p>

<h2>JSCS</h2>

<p>JSHint is going to get rid of style rules, and just test things like
shadowing, hoisting, syntactical things.</p>

<p>Enter JSCS to fill in the style spot.</p>

<p>https://github.com/jscs-dev/node-jscs</p>

<h2>Instacart</h2>

<p>https://www.instacart.com</p>

<p>Seems pretty badass? I guess? Shop from multiple</p>

<h2>AWOL</h2>

<p>Absent Without Official Leave.</p>

<h2>kip</h2>

<p>British for &#8216;nap&#8217;.</p>

<h2>The spirit is willing</h2>

<p>but the flesh is weak, or spongey and bruised.</p>

<p>http://biblehub.com/matthew/26-41.htm</p>

<blockquote><p>&#8220;Watch and pray so that you will not fall into temptation. The spirit is willing, but the flesh is weak.&#8221;</p></blockquote>

<h2>MFA</h2>

<p>Museum of Fine Arts in Boston.</p>

<p>http://www.mfa.org/about</p>

<h2>My dumb thing</h2>

<p>12 seconds gold</p>

<p>6 second repetition, so use the gold one&#8217;s motor/governor.</p>

<h2>Bacon.js</h2>

<ul>
<li>EventStream: distinct events that happen over time</li>
<li>Property: value that changes over time</li>
</ul>


<p>TODO: Why have both?</p>

<ul>
<li>EventStreams have no initial value</li>
</ul>


<p><code>combine</code>: basically map two streams/properties into a Property (why not
event stream? what&#8217;s the EventStream equivalent?). You can pass a method
name as the second arg and it&#8217;ll be called on the&#8230; I don&#8217;t know. In
the case of <code>.scan</code> it&#8217;s the accumulator. What makes sense for combine?
Proably on the first arg, but that&#8217;s weird. NO it&#8217;s not, it gets called
on the current property of course. Let&#8217;s test this theory.</p>

<p>Actually no this makes no sense, they have an example</p>

<pre><code>var out = a.combine(b, ".concat")
</code></pre>

<p>It makes sense that it&#8217;d use the current value of the output property to
call <code>.concat</code> on, such as an array, but <code>.concat</code> mutates the
underlying array&#8230; is that ok? Seems fucky, seems like someone who got
a previous value from <code>out</code>, array, might have their array value
mutated. Let&#8217;s find out why this would not be the case&#8230;</p>

<p>Fuck it too lazy right now.</p>

<p>Convert promise to EventStream: <code>Bacon.fromPromise</code>.</p>

<p>So, flat map exists whenever you&#8217;d like to map to some asynchronous
value where order can&#8217;t be preserved&#8230; so for instance if you had a
serialize of values and you wanted to map them to some other value that
could be determined synchronously, you&#8217;d just map, but if you can&#8217;t
return something synchronously, each &#8220;map&#8221; should essentially return a
new stream, case in point: AJAX-based auto-completes. You translate text
field updates into ajax queries, each of which have their own unique
async.</p>

<pre><code>textUpdate -&gt; map -&gt; map -&gt; mapToAjax
                                -&gt; ajax promise
                                -&gt; ajax promise
                                -&gt; ajax promise
</code></pre>

<p>Need to reconcile these back into a single stream.</p>

<p>The pattern is expand into async via ajax and coalesce back into values
with flatMap.</p>

<ul>
<li><code>flatMap</code>: does NOT preserve ordering</li>
<li><code>flatMapLatest</code>: discards/forgets previous values if new ones come
back first</li>
</ul>


<p>Question: is there a flatMap that preserves order without discarding?</p>

<p><code>fromBinder</code>: create your own custom stream. The callback accepts a
<code>sink</code> function which is how you emit events.</p>

<p>What does <code>sink</code> mean? I guess it means a consumer of a stream? Why the
stream sinks into.</p>

<p><code>interval</code> vs <code>fromPoll</code>:</p>

<ul>
<li><code>interval</code> returns a stream that emits an optional value that you pass
to it.</li>
<li><p><code>fromPoll</code> has no concept of a value, it just runs your callback</p>

<p>  var stream = Bacon.fromBinder(function(sink) {</p>

<pre><code>sink([new Bacon.Next("2nd"), new Bacon.Next("3rd")])
sink(["4th", "5th"])
sink(["6th", new Bacon.Next("7th")])
sink([new Bacon.Next("8th"), "9th"])
return function() {
   // unsub functionality here, this one's a no-op
}
</code></pre>

<p>  })
  stream.log()</p></li>
</ul>


<p>Output:</p>

<pre><code>2nd
3rd
["4th", "5th"]
6th
7th
[b, "9th"]
</code></pre>

<p>You can use <code>Bacon.Next</code> to convert an array into sequentially emitted
values, rather than the value emitted being an array. It&#8217;s like a
flatten decided by the emitter?</p>

<p>Bus: https://github.com/baconjs/bacon.js#bus</p>

<p>A Bus an an EventStream that you can push values into.
I think: <code>promise:defer::eventStream:bus</code>, except that Deferreds
don&#8217;t have the promise&#8217;s <code>.then</code> api whereas a Bus <em>is</em> an event stream
with the same event stream API, plus a <code>.push</code> method.</p>

<p>Properties have optional starting values.</p>

<p>If provided, <code>onValue</code> emits a value immediately, else it only updates
on the first thing.</p>

<ul>
<li><code>EventStream.toProperty()</code>: convert to property</li>
<li><code>Property.change()</code>: convert to event stream</li>
</ul>


<p><code>Bacon.update</code>: create Property from initial value, update value based
on multiple inputs.</p>

<p>INTERESTING. This is kinda what we&#8217;ve been talking about in Emberland.
EXCEPT that <code>Bacon.update</code> doesn&#8217;t expose the means of updating the
property, it just encapsulates all updates into a single property, and
you need to provide the write-able buses via some other mechanism.</p>

<p>I <em>think</em> Property and EventStreams are both Observables. c/d?</p>

<h2>Synchronizing streams, Bacon.when</h2>

<p>You&#8217;d like to zip together streams into a single stream, but when do you
fire an event? When both have since fired? When only one? <code>Bacon.when</code>
to the rescue to disambiguate and let you decide!</p>

<pre><code>Bacon.when(seed
</code></pre>

<h2>Braindumpin</h2>

<p>http://baconjs.github.io/tutorials.html#content/tutorials/4_Building_Applications_Out_Of_Bacon</p>

<p>Goal: understand Ember action bubbling vs Flux dispatcher pattern among
other things, grand unifying FRP theory.</p>

<p>Mutable state is an oxymoron, it&#8217;s hard to reason about programs with
mutable data structures, hence React is nice because renders are
stateless, allowing functional/immutability patterns.</p>

<p>It&#8217;s more and more frowned upon to do</p>

<p>and allow <code>x-foo</code> to directly change <code>blah</code>, which in present-day/legacy
Ember style would flow the data back upward. The data-down-actions-up
approach would involve:</p>

<p>and <code>updateBlah</code> would perform the set. It&#8217;s really just moving the
<code>set</code> back up to the parent so in that way it might just seem like
needless indirection, but this allows:</p>

<ul>
<li>easier reasonability/testability</li>
<li>can override <code>updateBlah</code> to do whatever and asyncly update <code>value</code>,
after some validation. It&#8217;s reaaaally hard/awkward/annoying to sub out
the <code>set</code> behavior</li>
</ul>


<h2>Flux et al</h2>

<p>What are the core concepts?</p>

<pre><code>Views ---&gt; (actions) ----&gt; Dispatcher ---&gt; (registered callback) ---&gt; Stores -------+
Ʌ                                                                                   |
|                                                                                   V
+-- (Controller-Views "change" event handlers) ---- (Stores emit "change" events) --+
</code></pre>

<p>First off, a store is like what&#8217;s traditionally called a model; it has
app state and app logic. A store might directly represent a single model
(i.e. what might be called a UserStore might just be a User model in
other MVCs), but in general stores can represent many objects, scoped to
a particular domain of the application.</p>

<p>If you click a button in a component, it never directly invokes some
method on a store. Why? Because it would be hard to trace the
dependencies? Any new component could come along and start calling
methods on some random store? Yes, confirm, you don&#8217;t want to do that.
You want a message bus. The dispatcher is that bus.</p>

<pre><code>action ---&gt;
action ---&gt;
action ---&gt;---&gt; dispatcher
action ---&gt;
action ---&gt;
</code></pre>

<p>So in your tests you can stub the dispatcher and assert that expected
methods were called on it. Easy peasy I guess? Doesn&#8217;t it mean
you&#8217;ll have some giant switch statement that&#8217;s gotta divvy out events in
the dispatcher? Often: yes, it does, which is desirable for small enough
stuff, issues with scalability though.</p>

<p>The dispatcher model does have the benefit of being able to intercept
events and filter/debounce/coalesce them; this would be a nice place to
ensure that a stray tap in the middle of a sideways transition in a
mobile app didn&#8217;t fire some event.</p>

<p>Anyway, calling action helper fns on a dispatcher doesn&#8217;t do anything if
stores haven&#8217;t subscribed to dispatcher events.</p>

<h2>Reflux</h2>

<p>https://github.com/spoike/refluxjs</p>

<p>Seems like an improvement; gets rid of the switch statements; makes
actions listenable; makes stores listenable in case some stores depend
on others (replaces <code>waitFor</code>)</p>

<p>The actions being listenable is good because it gets rid of stores
receiving ALL actions and having to selectively respond to them via big
switch statement matching by string name.</p>

<p>Actions are functors, you can call them directly and that passes through
the payload to everyone listening.</p>

<pre><code>var statusUpdate = Reflux.createAction();
statusUpdate(data); // Invokes the action statusUpdate
statusUpdate.triggerAsync(data); // same effect as above
</code></pre>

<p>or</p>

<pre><code>var statusUpdate = Reflux.createAction({ sync: true });
statusUpdate(data); // Invokes the action statusUpdate SYNChronously
statusUpdate.trigger(data); // same effect as above
</code></pre>

<p>var Actions = Reflux.createActions([
  &#8220;statusUpdate&#8221;,
  &#8220;statusEdited&#8221;,
  &#8220;statusAdded&#8221;
]);</p>

<p>There&#8217;s lots of features for async and converting an action into async
based on its children.</p>

<p>Blah blah blah back to other ideas:</p>

<p>So if you wanted to do something like Flux in Ember, you&#8217;d have to do
something like</p>

<p>TODO: https://github.com/spoike/refluxjs#joining-parallel-listeners-with-composed-listenables</p>

<p>So my whole better actions idea was so that you could quickly reason
about action state.</p>

<pre><code>var TopLevelComponent = Ember.Component.extend({

  // Better Actions API
  submitForm: action(function() {
    return ajaxPromise(whatever);
  })
});
</code></pre>

<p>This worked <em>ok</em> ish but was horrible for lists because it&#8217;s a
singleton.</p>

<p>Use case: TODO MVC with slow async deletes; additional deletes should be
prevented if animating out of this route. There is a loading spinner if
at least one item is being deleted. Version 1 lets you delete multiple
items at the same time, Version 2 prevents this.</p>

<p>My goals is that I want to see how stream composition can address the
actions problem. Got async actions? Don&#8217;t want them firing at the same
time? Debounce those shits, or flatmap latest. (TODO: what&#8217;s the stream
approach for letting a previous action finish first before starting
another one? I think the answer is flatMapFirst)</p>

<p>For the &#8220;additional deletes should be prevented if animating out&#8221;
requirement, there should be some concept of an EventStream that
dilineates the start and end of when a route is active. If you&#8217;re using
LiquidFire this should be after a route has finished transitioning in
and right before the route has started transitioning out
(animation-wise). The idea being that by default actions are filtered
through that window. But how do you do that?</p>

<p>You want a stream that only emits events per some predicate&#8230;</p>

<p>Actually this is probably a good use case for takeWhile&#8230; takeWhile
route is active. http://jsbin.com/tuzifo/2/edit</p>

<p>But given that we might have components already being displayed, but
we&#8217;re not ready to receive their actions, we need to figure out how to
hook them up&#8230;</p>

<p>So what the fuck does <code>&lt;button &gt;Click me&lt;/button&gt;</code>
do?</p>

<pre><code>TopLevelComponent
  MidLevelComponent
    LeafComponent
      &lt;button &gt;Click me&lt;/button&gt;
</code></pre>

<p>Maybe components implicitly pass down an actions stream? And
route-driven components just get a decorated one from the router? And
then Liquid Fire could provide one for components rendered within an
outlet?</p>

<p>actually expands to</p>

<p>&#8230;?</p>

<p>What am I trying to accomplish here? I&#8217;m so lost.</p>

<p>TODO: replace bullshit with something more meaningful.</p>

<p>https://github.com/baconjs/bacon.js#observable-zip</p>

<p>zip only fires when both streams produce a value, else
backpressure/buffering can occur. Difference between these two is that
back pressure occurs when consumers/sinks can&#8217;t consume fast enough;
buffering can occur for other reasons on the source side of things, e.g.
only one stream in zip is producing values and hence buffering up
values. This can be alleviated by choosing a different join pattern than
the zip default.</p>

<p>Use Bacon.when biaaatch.</p>

<h2>Blood pressure</h2>

<p>110/70. What&#8217;s it mean.</p>

<blockquote><p>If your blood pressure is 120/80, 120 represents systolic pressure, or the pressure of blood against artery walls when the heart beats. Eighty represents diastolic pressure, or the pressure between beats.</p></blockquote>

<p>So it&#8217;s like a max min range, pressure will be the highest in the middle
of a heart beat, lowest between.</p>

<p>Systolic = during heart beat. Diastolic = pressure between beats. High
systolic is generally more serious.</p>

<h2>Make no promises</h2>

<p>http://swannodette.github.io/2013/08/23/make-no-promises/</p>

<h2>Buses as an anti-pattern</h2>

<p>http://baconjs.github.io/tutorials.html#content/tutorials/4_Building_Applications_Out_Of_Bacon</p>

<pre><code>var shoppingCartBus = new Bacon.Bus()
$.ajax('/api/cart').done(cart =&gt; shoppingCartBus.push(cart))
...
shoppingCartBus.onValue(cart =&gt; renderCart(cart))
</code></pre>

<p>What happens if the ajax push occurs before the <code>onValue</code> is set? It&#8217;s
missed, nothing gets rendered. It&#8217;s the same as</p>

<pre><code>var wat = new Wat();
// ...
wat.foo = new Foo();
</code></pre>

<p>What if someone does something depending on <code>.foo</code> being present? Boom.</p>

<p>Solution: inject your dependencies. Pass in buses. Pass in your inputs.
Pass in your actions.</p>

<p>This is kinda fucky:</p>

<pre><code>function ShoppingCartView(cart) {
  return {
    cartView: ...
    removeItemStream: $('#shopping-cart').asEventStream('click', '.remove-item')
      .map(function(e) { return $(e.currentTarget).data('id') })
}
</code></pre>

<p>So creating a view not only just creates the view but also returns the
&#8220;outputs&#8221; of the view, e.g. the actions being emitted from the view.
But this causes cyclic dependencies because you want to pass in a cart
model which provides inputs to the view but you want to provide the
outputs of that view back to the model, but also avoid the temporal
construction issues.</p>

<h2>RX vs Bacon</h2>

<p>https://github.com/baconjs/bacon.js#for-rxjs-users</p>

<p>Bacon offers EventStreams and Properties, subclasses of Observables,
whereas RxJS just has Observables. So how do current values work in RxJS
or do you always have to save it to some state?</p>

<h2>RX hot/cold observables</h2>

<p>https://github.com/Reactive-Extensions/RxJS/blob/master/doc/gettingstarted/creating.md#cold-vs-hot-observables</p>

<p>TODO read about this.</p>

<h2>Hemipenes</h2>

<p>http://en.wikipedia.org/wiki/Hemipenis</p>

<p>Snakes have two penises, or penes, called hemepenes. Only one is used.
It&#8217;s a spiky weird thing that bloats out when aroused. Other similarly
animals have them, like lizards. Sharks have two &#8220;penises&#8221; but they&#8217;re
called claspers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learnin Clojure]]></title>
    <link href="http://machty.github.com/blog/2014/12/17/learnin-clojure/"/>
    <updated>2014-12-17T12:52:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/12/17/learnin-clojure</id>
    <content type="html"><![CDATA[<h2>Joy of Clojure</h2>

<ul>
<li>Imperative

<ul>
<li>a sequence of statements mutates program state</li>
</ul>
</li>
<li>maenad (maenadic): wild and unrestrained: a female follower of
Bacchus, traditionally associated with divine possession and frenzied
rites</li>
<li>expression problem

<ul>
<li>&#8220;The Expression Problem is a new name for an old problem. The
goal is to define a datatype by cases, where one can add new cases
to the datatype and new functions over the datatype, without
recompiling existing code, and while retaining static type safety
(e.g., no casts).&#8221;</li>
<li>Java requires these to be declared up front, suffers</li>
<li>Why the &#8220;expression&#8221; problem? &#8216;The label &#8220;Expression Problem&#8221; puns on expression = &#8220;how much can your language express&#8221; and expression = &#8220;the terms you are trying to represent are language expressions&#8221;.&#8217;</li>
</ul>
</li>
</ul>


<!--more-->


<h2>Protocols</h2>

<p>Almost like defining global functions but require that first arg(?)
adheres to a given protocol.</p>

<pre><code>(defprotocol Searchable
  (search [this query]))
</code></pre>

<p>then</p>

<pre><code>(extend-type String
  Searchable
  (search [this] 
    (search [this query]
      (someshit))))
</code></pre>

<blockquote><p>The resulting functions dispatch on the type of their first argument, and thus must have at least one argument</p></blockquote>

<p>MAKES SENSE.</p>

<h2>Expression Problem</h2>

<p>You have</p>

<ul>
<li>a set of abstract methods you&#8217;d like to implement&#8230;</li>
<li>&#8230;for an existing concrete class&#8230;</li>
<li>&#8230;without changing the code for either the existing abstract methods
or the concrete class</li>
</ul>


<p>Ruby&#8217;s solution means reopening the class, possibly monkey patching.
Clojure&#8217;s solution is protocols&#8230; defining functions that dispatch
based on the type of the first arg.</p>

<p>Ruby suffers a namespacing issue, hence the danger of monkey patching.
In Ruby everything is a message, easy to accidentally overwrite a method
handler. Maybe there&#8217;s a namespace issue in Clojure too, if you&#8217;re not
careful, but whatever I just need to learn more shit.</p>

<h2>Clojure: namespaces</h2>

<p><code>(ns dumb.shit)</code></p>

<p>and you have <code>defn-</code> macros to declare private methods.</p>

<p>Namespaces encapsulate, but you can also use lexical closures for
encapsulation.</p>

<h2>Keywords</h2>

<p><code>:asd</code> is not a symbol, it&#8217;s a &#8220;keyword&#8221;. <code>asd</code> is a symbol, and is
substituted with some underlying value.</p>

<h2>Collections</h2>

<h3>Lists</h3>

<p><code>(a b c)</code> is just a list. When it gets evaluated, <code>a</code> will be treated
as a function, macro, or operator.</p>

<p>Vectors&#8230; don&#8217;t get evaluated? Why don&#8217;t they make a bigger deal about
this?</p>

<pre><code>[1 2 3]
(vector 1 2 3)
</code></pre>

<h2>Commas are whitespace</h2>

<pre><code>user=&gt; { 1 1 2, 2 }
{1 1, 2 2}
</code></pre>

<h2>prefix/infix</h2>

<pre><code>+ 1 2 3 4 5 6 7
</code></pre>

<p>vs</p>

<pre><code>1 + 2 + 3 + 4 + 5 + 6 + 7
</code></pre>

<p>Prefix allows any number of args.</p>

<p>Also, when everything is prefix, no question of operator precedence.</p>

<h2>Vars</h2>

<p>don&#8217;t need to be bound</p>

<pre><code>user=&gt; (def y)
#'user/y
user=&gt; y
#&lt;Unbound Unbound: #'user/y&gt;
</code></pre>

<h2>Special form</h2>

<p>A Clojure expression that&#8217;s part of core language but not created via
normal functions, types, or macros.</p>

<h2>Lisp</h2>

<p>stands for &#8220;LISt Processing&#8221;</p>

<h2>def defn</h2>

<p><code>def</code> associates a symbol w Clojure data (could be some primitive, or a
function, or whatever).</p>

<pre><code>(def a (fn [] (println "wat")))
</code></pre>

<p><code>defn</code> is a macro that wraps the above (and also lets you apply
documentation).</p>

<pre><code>(defn a "i am a stupid fn" [] (println "wat"))
(defn a [] (println "wat"))
</code></pre>

<p>You can use <code>defn</code> for multi-arity fns</p>

<pre><code>user=&gt; (defn a ([a] #{a}) ([a b] #{a b}))
#'user/a
user=&gt; (a 2)
#{2}
user=&gt; (a 2 3)
#{3 2}
user=&gt; (a 2 3 4)
ArityException Wrong number of args (3) passed to: user/a  clojure.lang.AFn.throwArity (AFn.java:429)
</code></pre>

<p>What&#8217;s weird about this is <code>([n] #{n})</code>&#8230; if you&#8217;re evaluating this,
won&#8217;t it try to immediate run <code>[n]</code> as if it were a fn? Yes! But we&#8217;re
not evaluating&#8230; we&#8217;re in a macro! Eval rules don&#8217;t apply while shit is
getting reorganized by the macro code. Kiiiinda fuckin weird.</p>

<p>Multi-arity accomplished via</p>

<pre><code>(defn a [&amp; x] (apply vector x))
</code></pre>

<h2>Homoiconicity</h2>

<p>http://en.wikipedia.org/wiki/Homoiconicity</p>

<p>It means your program&#8217;s AST is the same as its syntax, basically.
Homoiconicity is a feature of Clojure.</p>

<h2>Reader features</h2>

<p>Like C++ preprocessor macros, rearrange shit (but somehow isn&#8217;t a
macro?). <code>#()</code> is an inplace function reader feature that gets replaced
with special form <code>def</code>.</p>

<pre><code>user=&gt; (def a #(vector %1 %3))
#'user/a
user=&gt; (a 1 2 3)
[1 3]
</code></pre>

<p>The above example shows that you can skip an numbered arg and still wind
up with an arity of 3.</p>

<pre><code>(def a #(vector % % %))
user=&gt; (a 1)
[1 1 1]
</code></pre>

<h2>Blocks</h2>

<p>do blocks, usually side-effecty</p>

<pre><code>user=&gt; (do
  #_=&gt; (println "wat")
  #_=&gt; (println "omg")
  #_=&gt; 3)
wat
omg
3
</code></pre>

<h2>Locals (not variables)</h2>

<p>Useless:</p>

<pre><code>(let [] (println "wat"))
</code></pre>

<p>Useful</p>

<pre><code>(let [a 5] (println "a is" a))
</code></pre>

<p>Why a vector and not a map? I guess because let internally needs to loop
through each thing?</p>

<p>Note the use of &#8220;form&#8221;. It&#8217;s <code>let</code> form, it&#8217;s a <code>do</code> form. <code>def</code> is a
special form&#8230; still need to tease this out. I guess form just means
language pattern building block that depends on basic Closure types
(functions, primitives, etc).</p>

<h2>Loops</h2>

<p>Use tail recursion. <code>recur</code> is a special form.</p>

<p>BEFORE I MENTION THAT THOUGH I figured out how to rewrite a fn that runs
a callback twice</p>

<pre><code>user=&gt; (defn twice ([some-fn] (twice some-fn true)) ([some-fn first] (some-fn) (when (true? first) (twice some-fn false) ) ))
#'user/twice
user=&gt; (twice #(println "hello"))
hello
hello
</code></pre>

<p>Could have also used special form <code>fn</code>:</p>

<pre><code>(twice (fn [] (println "hello")))
</code></pre>

<h2>Android camera bug</h2>

<p>Only on Nexus 5, we had a bug where switching between front and back
camera failed, reason being something to do with the fact that there&#8217;s
no guarantee that front and back will have the same aspect ratio /
resolution, so you need to reinitialize some shits. ISN&#8217;T ANDROID
FUN?!?!?!?!?!</p>

<h2>Clojure <code>set!</code></h2>

<pre><code>(def a (java.awt.Point. 0 0))
(.-x a)
; 0
(set! (.-x a) 1)
; 1
(.-x a)
; 1
</code></pre>

<p>So you can think of <code>(.-x a)</code> as a descriptor that <code>set!</code> can operate
on. Hmm does that mean <code>set!</code> is a macro?</p>

<p>http://clojure.org/java_interop#Java%20Interop-The%20Dot%20special%20form-%28set!%20%28.%20Classname-symbol%20staticFieldName-symbol%29%20expr%29</p>

<p>Apparently it&#8217;s a special form.</p>

<h2>Abstract Window Toolkit</h2>

<p>java.awt stands for Abstract Window Toolkit.</p>

<h2>Capture output</h2>

<pre><code>(with-out-str (println "fuckles"))
; "fuckles"
</code></pre>

<p>You can also use this to capture the output of <code>doc</code>, which gets
docstring:</p>

<pre><code>(defn a "a dumb thing" [] (println()))
(doc a)
(with-out-string (doc a))
</code></pre>

<h2>What are all the special forms?</h2>

<p>They&#8217;re like special hooked-into compiler macros that do special things
you couldn&#8217;t do with macros.</p>

<pre><code>clojure.lang.Compiler/specials
{&amp; nil, monitor-exit #&lt;Parser clojure.lang.Compiler$MonitorExitExpr$Parser@14483946&gt;, case* #&lt;Parser clojure.lang.Compiler$CaseExpr$Parser@3826c025&gt;, try #&lt;Parser clojure.lang.Compiler$TryExpr$Parser@1a54177e&gt;, reify* #&lt;ReifyParser clojure.lang.Compiler$NewInstanceExpr$ReifyParser@5d473d44&gt;, finally nil, loop* #&lt;Parser clojure.lang.Compiler$LetExpr$Parser@770f1aba&gt;, do #&lt;Parser clojure.lang.Compiler$BodyExpr$Parser@86b7485&gt;, letfn* #&lt;Parser clojure.lang.Compiler$LetFnExpr$Parser@5e223ed2&gt;, if #&lt;Parser clojure.lang.Compiler$IfExpr$Parser@10668c26&gt;, clojure.core/import* #&lt;Parser clojure.lang.Compiler$ImportExpr$Parser@3636d04a&gt;, new #&lt;Parser clojure.lang.Compiler$NewExpr$Parser@39c8704e&gt;, deftype* #&lt;DeftypeParser clojure.lang.Compiler$NewInstanceExpr$DeftypeParser@17f1292f&gt;, let* #&lt;Parser clojure.lang.Compiler$LetExpr$Parser@59c6d8f0&gt;, fn* nil, recur #&lt;Parser clojure.lang.Compiler$RecurExpr$Parser@75c7b6d7&gt;, set! #&lt;Parser clojure.lang.Compiler$AssignExpr$Parser@77a7ed63&gt;, . #&lt;Parser clojure.lang.Compiler$HostExpr$Parser@615e29b9&gt;, var #&lt;Parser clojure.lang.Compiler$TheVarExpr$Parser@7c8deca3&gt;, quote #&lt;Parser clojure.lang.Compiler$ConstantExpr$Parser@79dce7a9&gt;, catch nil, throw #&lt;Parser clojure.lang.Compiler$ThrowExpr$Parser@cbff27f&gt;, monitor-enter #&lt;Parser clojure.lang.Compiler$MonitorEnterExpr$Parser@567fbf5a&gt;, def #&lt;Parser clojure.lang.Compiler$DefExpr$Parser@3a450e&gt;}
</code></pre>

<p>Or just</p>

<pre><code>(sort (keys clojure.lang.Compiler/specials))
(&amp; . case* catch def deftype* do finally fn* if let* letfn* loop* monitor-enter monitor-exit new quote recur reify* set! throw try var clojure.core/import*)
</code></pre>

<h2>Java <code>..</code> interop macro</h2>

<pre><code>(.. (java.util.Date.) toString (endsWith "2014"))
</code></pre>

<p>Could also write</p>

<pre><code>(.. (java.util.Date.) (toString) (endsWith "2014"))
</code></pre>

<p>TLDR the <code>..</code> macro will unwrap each argument and treat as fn
invocation.</p>

<h2>ligsigsegv</h2>

<p>http://www.gnu.org/software/libsigsegv/</p>

<p><code>clisp</code> uses this&#8230; it&#8217;s a way you can implement virtual memory in user
space. Probably some clever shit that clisp and other zany interpreters
use.</p>

<h2>Namespace</h2>

<p>Cutting to that Chase™.</p>

<h2>What is &amp;?</h2>

<pre><code>user=&gt; [&amp; asd]
CompilerException java.lang.RuntimeException: Unable to resolve symbol: &amp; in this context, compiling:(NO_SOURCE_PATH:0:0)
</code></pre>

<p>so when can you use it?</p>

<pre><code>(fn [&amp; asd])
</code></pre>

<p>this is a fn that takes any number of args:</p>

<pre><code>((fn [&amp; asd]))
((fn [&amp; asd]) 1)
((fn [&amp; asd]) 1 2)
</code></pre>

<p>Basically fn is a macro and can rearrange &amp;, which is just a symbol.</p>

<h2>Anaphoric Macros</h2>

<p>http://amalloy.hubpages.com/hub/Unhygenic-anaphoric-Clojure-macros-for-fun-and-profit</p>

<p>TL;DR useful in some cases for code generation.</p>

<p>Basically you should always use backtick <code>\`` (for fully qualified
symbols) rather thaaaaaaan boring ol single quote</code>&#8217;`.</p>

<h2>Destructuring</h2>

<p>Programming language concept of breaking apart some data structure into
assigned variables. Pattern matching in Haskell is a form of this.
Ruby&#8217;s array destructuring assignment has</p>

<pre><code>a,b = [1,2]
</code></pre>

<p>Apparently even Firefox has some o dat shit</p>

<pre><code>var a,b;
[a,b] = [1,2];
</code></pre>

<p>In Clojure, you don&#8217;t have pattern matching a la Haskell but you still
have parameter destructuring, the difference being (I think) that
vanilla Clojure won&#8217;t match an argument signature and decide which
version of the fn to run (unless you add the <code>core.match</code> module, which
might be a core feature at some point).</p>

<p>Basically Clojure has similar features to Ruby.</p>

<h2>npm pretest</h2>

<pre><code>"pretest": "ember build",
"test": "bin/run-tests.js",
</code></pre>

<p>TL;DR when testing ember repo (and the server is already running), just
directly run <code>/bin/run-tests.js</code>.</p>

<h2>:keys</h2>

<pre><code>{:keys [a b c]}
</code></pre>

<p>This isn&#8217;t meaningful on its own; it&#8217;s just a map. But it matches one of
the patterns expected by the <code>let</code> macro.</p>

<pre><code>(let [{:keys [a b c]} {:a "wat" :b "the" :c "hell"}]
  (println a b c))
</code></pre>

<p>That doesn&#8217;t rename anything, but if you wanna do that:</p>

<pre><code>(let [{x :a y :b z :c} {:a "wat" :b "the" :c "hell"}]
  (println x y z))
</code></pre>

<p>The key thing to remember is that the symbol needs to be on the left:
<code>{x :a y :b z :c}</code>.</p>

<p>Means you could also do</p>

<pre><code>(defn a [{a :first-name b :last-name} h]
  (println "HAHA" a " " b))
</code></pre>

<p>WRONG because of <code>h</code>. These are fn params, not <code>let</code> args that are
presented up front, so we actually want:</p>

<pre><code>(defn a [{a :first-name b :last-name}]
  (println "HAHA" a " " b))
</code></pre>

<p>followed by</p>

<pre><code>(a {:first-name "Wat" :last-name "McMatchneer"})
</code></pre>

<h2>rationalize</h2>

<p>Converts, say, a float to a fraction.</p>

<h2>Clojure keywords as fns</h2>

<pre><code>user=&gt; (def a { :a 123 })
#'user/a
user=&gt; (:a a)
123
</code></pre>

<p>If a keyword is in fn position, it does a lookup of itself on the hash
arg passed to it. Otherwise you&#8217;d have to do:</p>

<pre><code>user=&gt; (get a :a)
123
</code></pre>

<h2>So many seqs</h2>

<p><code>Collection</code>: a composite data type: sequence, map, or set.</p>

<p><code>Sequential</code>: ordered series of values (as opposed to, say, a set or a
map)</p>

<p><code>Sequence</code>: a sequential collection that may or may not exist yet, e.g.
it could be a vector or it could be a lazy-seq of yet-to-be-computed
thunks.</p>

<p><code>Seq</code>: an API for navigating collections: <code>first</code>, <code>rest</code>, nil, and
<code>()</code>. Basically, vectors, lists, PersistentLists are all <code>seq</code>s.</p>

<p><code>seq</code>, of <code>clojure.core/seq</code>, is a fn that returns an object
implementing <code>Seq</code> API, or nil if collection is empty. <code>seq</code> returns &#8220;an
immutable variant of an iterator/enumerator`</p>

<h2>Equality partition</h2>

<p>Collections are partitioned into the following categories:</p>

<ul>
<li>Sequentials</li>
<li>Map</li>
<li>Set</li>
</ul>


<p><code>(=)</code> of two objects from different partitions is always false; e.g.
vector compared w hash w same objs is false.</p>

<p>Within a partition, different concrete types might equal each other:</p>

<pre><code>(= [:a :b :c] '(:a :b :c))
true
</code></pre>

<h2>cons-cell</h2>

<p>Two-cell structure upon which Lisps build their data types. Maps to
first and rest in Clojureland.</p>

<p>Pretty sure this is how sequences are implemented (immutable/persistent
data structures).</p>

<h2>(class) and .getClass</h2>

<p>Same thing? Maybe?</p>

<pre><code>user=&gt; (class (java.awt.Point.))
java.awt.Point
user=&gt; (.getClass (java.awt.Point.))
java.awt.Point
</code></pre>

<h2>map</h2>

<p>GOD this shit is so cool.</p>

<p><code>map</code> takes a fn and any number of collections afterward, applies a
function to the first items, then second items, etc.</p>

<p>It&#8217;s like a zip. But with a map. SO COOL.</p>

<pre><code>(map #(apply + %&amp;) [1 2 3] [4 5 6] '(7 8 9))
</code></pre>

<p>or</p>

<pre><code>(map vector (range 5) (range 6) (range 7))
([0 0 0] [1 1 1] [2 2 2] [3 3 3] [4 4 4])
</code></pre>

<h2>vec or vector</h2>

<pre><code>user=&gt; (vec [1 2 3 4 5])
user=&gt; (apply vector '(1 2 3 4 5))
</code></pre>

<p>Basically <code>vec</code> accepts a single collection arg whereas <code>vector</code> accepts
multiple args. Both create vectors.</p>

<p>Hence</p>

<pre><code>(vec (map char (range 65 76)))
</code></pre>

<p>Many ways to look up a value in vector.</p>

<ul>
<li>nth</li>
<li>get</li>
<li>vector as fn</li>
</ul>


<p>BARF BARF BARF</p>

<pre><code>user=&gt; (assoc [4 5 6] 0 "wat")
["wat" 5 6]
</code></pre>

<p>REMEMBER that defn let&#8217;s you define multiple fn bodies per arity.
You see this when the arg vector doesn&#8217;t immediately follow the name,
but a list (starting w arg vector) does.</p>

<p>e.g.</p>

<pre><code>(defn mach-inc
  ([n] (mach-inc n 1))
  ([n i] (+ n i)))
</code></pre>

<p>So now I&#8217;ve made an inc fn that takes an optional increment amount, but
defaults to 1. If it were just a single-body fn, it&#8217;d be all</p>

<pre><code>(defn mach-inc-single [n]
  (+ n 1))

user=&gt; (mach-inc-single 5)
6
</code></pre>

<p>How to curry? Use partial. This adds two to everything.</p>

<pre><code>(map (partial + 2) (range 10))
</code></pre>

<p>Rad. Radsauce!</p>

<p>Vectors as stacks: <code>conj</code> and <code>pop</code>. Use <code>peek</code> a lot since <code>pop</code>
returns new immutable fucker.</p>

<h2><code>map</code> in Lisps, conj vs cons</h2>

<p>Apparently Clojure&#8217;s mapping over collections is more efficient because
it kinda lets you cheat w vectors. In Lisps without vectors you just
have lisps, and as you push to an accumulator</p>

<p>Ah ok let&#8217;s talk about cons vs conj.</p>

<p><code>cons</code> unshifts and returns a seq (or a <code>clojure.lang.Cons</code>?):</p>

<pre><code>user=&gt; (cons 4 [1 2 3])
(4 1 2 3)
user=&gt; (class (cons 4 [1 2 3]))
clojure.lang.Cons
</code></pre>

<p>Same thing if you pass in a list</p>

<pre><code>ArityException Wrong number of args (1) passed to: core/cons  clojure.lang.AFn.throwArity (AFn.java:429)
user=&gt; (cons 4 `(1 2 3))
(4 1 2 3)
</code></pre>

<p>WAAT apparently backtick does some implicit cons</p>

<pre><code>user=&gt; (class '(1 2 3))
clojure.lang.PersistentList
user=&gt; (class `(1 2 3))
clojure.lang.Cons
user=&gt; (class (quote (1 2 3)))
clojure.lang.PersistentList
</code></pre>

<p>Anyyyyway, <code>conj</code> returns a collection as close to the input collection
as possible. And it &#8220;push&#8221;es.</p>

<pre><code>user=&gt; (conj #{1 2 3} 4)
#{1 4 3 2}
user=&gt; (conj [1 2 3] 4)
[1 2 3 4]
user=&gt; (conj '(1 2 3) 4)
(4 1 2 3)
</code></pre>

<p>Ah HAH! In fact it DOESN&#8217;T always push&#8230; it inserts at the end that
makes the most sense given the data type (&#8220;in the most efficient way&#8221;):</p>

<pre><code>user=&gt; (conj [1 2 3] 4)
[1 2 3 4]
user=&gt; (conj '(1 2 3) 4)
(4 1 2 3)
</code></pre>

<p>SOOO</p>

<ul>
<li>cons

<ul>
<li>NOT homogeneous; no guarantee <code>next</code> chain will be list</li>
<li>only guarantee is that <code>next</code> is a seq. Which means you can append
to the beginning of any stupid thing. That&#8217;s why there&#8217;s
<code>clojure.lang.Cons</code> as its own type; it could point to anything.
<code>(class (next some-cons))</code> could be aaaany number of things
depending on how you composed the <code>cons</code>.</li>
</ul>
</li>
<li>conj

<ul>
<li>homogeneous: <code>next</code> is always the same type as wha you created it
with, whether list or vector or chunked seq from a vector, etc.</li>
<li><code>nil</code> starting list defaults to a PersistentList (but an empty vec
will be a vec, lol)</li>
</ul>
</li>
</ul>


<p>Unfortunately all seqs print w parentheses, but they could by all sorts
of different types of shit.</p>

<pre><code>user=&gt; (class (seq [1 2 3]))
clojure.lang.PersistentVector$ChunkedSeq
user=&gt; (class (seq '(1 2 3)))
clojure.lang.PersistentList
</code></pre>

<h2>recur</h2>

<p>http://clojure.org/special_forms#recur</p>

<blockquote><p>Note that recur is the only non-stack-consuming looping construct in Clojure. There is no tail-call optimization and the use of self-calls for looping of unknown bounds is discouraged. recur is functional and its use in tail-position is verified by the compiler.</p></blockquote>

<p>Interesting.</p>

<h2>No shift/unshift</h2>

<p>Why? Because internally the fns that manipulate the left side of a
vector return <code>seq</code>s, not vectors, which severs the ability to use
vector fns in an optimized way, reusing datastructures, etc.</p>

<p>So if you have</p>

<pre><code>user=&gt; (vec (range 5))
[0 1 2 3 4]
</code></pre>

<p>then doing</p>

<pre><code>user=&gt; (rest (vec (range 5)))
(1 2 3 4)

user=&gt; (class (rest (vec (range 5))))
clojure.lang.PersistentVector$ChunkedSeq
</code></pre>

<p>So you have a sequence <em>based</em> on a vector, but it&#8217;s not the original
vector, so you don&#8217;t have the same vetor fns available. If you want to
convert to vector again to get all those tasty shits, it&#8217;ll be O(n),
probably both time and space.</p>

<p>You can&#8217;t subvec a vec unexpect the original vec to be garbage collected
because subvec maintains a reference.</p>

<h2>PersistentList</h2>

<ul>
<li>singly-linked</li>
<li>every node knows distance from end</li>
<li><p>can only traverse from left to right</p>

<p>  (defmethod print-method clojure.lang.PersistentQueue</p>

<pre><code>[q w]
(print-method '&lt;- w)
(print-method (seq q) w)
(print-method '-&lt; w))
</code></pre></li>
</ul>


<h2>Shitty things</h2>

<p>It doesn&#8217;t seem like Clojure will yell at you if you mess up and use the
wrong kind of function for a collection. e.g. <code>rest</code> and <code>pop</code> do
similar things to a PersistentQueue, but <code>rest</code> yields a <code>seq</code>, and
<code>pop</code> yields a Queue, the former which gets rid of a lot of performance
improvements. The behavior you&#8217;ll see, since all <code>seq</code>s seem to
implement the same fns</p>

<h2>What the hell</h2>

<pre><code>(some #( if (even? %) % nil ) [1 3 4])
</code></pre>

<p>Sets and maps can behave like predicates.</p>

<pre><code>(#{4 9 10} 9)
9
user=&gt;     (#{4 9 10} 11)
nil
</code></pre>

<p>So you can use this idiom:</p>

<pre><code>(some #{:b} [:a 1 :b 2])
</code></pre>

<p>this runs through each collection on the right and checks to see if each
element is in the collection on the left, because the collection on the
left acts like a predicate.</p>

<h2>APR</h2>

<p>Annual Percentage Rate. Based on prime rate, e.g. add 12.74% to prime
rate to determine purchase annual percentage rate.</p>

<p>You can have a credit card and not pay interest if you pay in full all
the time. You must always pay the minimum rate, or else you get a
penalty. But the idea is that you can pay your minimum rate, and
anything above that is subject to an APR. Penalty APR might apply
indefinitely. Again, doesn&#8217;t matter (other than a shitty credit score)
if you always pay the full balance after that (APR only affects the
unpaid balance).</p>

<p>Balance transfers are transferring outstanding balances between cards,
likely from one w a higher APR to one w lower. New cardholders might be
offered 0% balance transfer API with the idea that you&#8217;ll transfer money
to it right away because you&#8217;re so hopelessly in debt. APR applies
differently to different types of debits on your account, and balance is
one kind of debit, so if you had a balance transfer APR 5% higher than
purchase APR, that&#8217;s basically discouraging you from putting balance
transfers into that new account (makes sense, it&#8217;s kind of a risky thing
if you&#8217;re just adopting new CCs and constantly transferring balances to
them; then again it&#8217;ll also be limited by your max credit line. There&#8217;s
also per-transaction fees, e.g. <code>max($5, 3%)</code>.</p>

<p>Cash transfer is when you debit one card, credit another. Different than
balance transfer because in one you&#8217;re trading a debt, in the other
you&#8217;re trading credit (cash). Per-transaction fee is <code>max($10, 3%)</code>.</p>

<p>Overdraft Advance is the ability to automatically withdraw from a credit
card if your debit card / check bounces. You have to go through the
hoops and set it up online, linking to a checking account.</p>

<p>US Prime Rate is 300 points above federal funds rate.</p>

<blockquote><p>In general, the United States prime rate runs approximately 300 basis points (or 3 percent) above the federal funds rate. The Federal Open Market Committee (FOMC) meets eight times per year wherein they set a target for the federal funds rate. Other rates, including the prime rate, derive from this base rate.</p></blockquote>

<p>Credit cards actually talk about referencing the prime rate printed in
the Wall Street Journal; &#8220;If WSJ stops publishing the Prime Rate, we
will select a similar preference rate.&#8221;</p>

<p>Variable rate just means it might change if the prime rate changes, or
other reasons.</p>

<p>Cash advances mean ATM withdrawals among other things.</p>

<p>Any amount over the minimum payment account gets applied to payment APR
first, and descending to other APR types (which might be higher or
lower). It trickles down.</p>

<h2>Interest charges</h2>

<p>Daily balance approach is common:</p>

<ul>
<li>take starting balance and add

<ul>
<li>interest charge from prior day (compounding interest)</li>
<li>new transactions/debits</li>
</ul>
</li>
<li>subtract payments/credits</li>
<li>daily balance = max(0, balance) (net credit balance treated as zero)</li>
<li>Daily interest rate = APR/365.</li>
<li>Multiply each type of APR by applicable balance type (payments are
different than balance transfers, etc)</li>
<li>If interest > 0, interest = max(MinimumInterest, interest)

<ul>
<li>MinimumInterest is often 0</li>
</ul>
</li>
</ul>


<h2>Clojure: into</h2>

<p>Pour values into collections:</p>

<pre><code>(into {} {:a 1 :b 2})
{:b 2, :a 1}
(into {} [[:a 1] [:b 2]])
{:a 1, :b 2}
</code></pre>

<p>The latter stems from the fact that providing a map to <code>seq</code> generates a
shit:</p>

<pre><code>(seq {:a 1 :b 2})
([:b 2] [:a 1])
</code></pre>

<h2>Clojure maps</h2>

<p><code>{}</code> is actually ArrayMap, but most common is HashMap:</p>

<pre><code>(hash-map :a 123)
</code></pre>

<p><code>zipmap</code> takes a keys and values array</p>

<pre><code>(zipmap [:k0 :k1] [:v0 :v1])
{:k1 :v1, :k0 :v0}

(zipmap [:k0 :k1] [:v0 :v1 :v2])
{:k1 :v1, :k0 :v0}

(zipmap [:k0 :k1 :k2] [:v0 :v1])
{:k1 :v1, :k0 :v0}
</code></pre>

<p>The latter two demonstrate that it stops at the smallest array.</p>

<h2>second</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>user=&gt; (doc second)
</span><span class='line'>-------------------------
</span><span class='line'>clojure.core/second
</span><span class='line'>([x])
</span><span class='line'>  Same as (first (next x))</span></code></pre></td></tr></table></div></figure>


<h2>invariant</h2>

<p>Assertions in functional land.</p>

<h2>equality</h2>

<p>Meaningless in a mutable system. If you&#8217;re not guaranteed to be equal to
another object forever, then you&#8217;re not equal.</p>

<p>Ooo unrelated by <code>let</code> locals can refer to previous ones in the same let
binding vector:</p>

<pre><code>(let [v [4 5 6] a (first v)] (println a))
4
</code></pre>

<p>Anyway, immutable objects share data:</p>

<pre><code>(let [v [4 5 6] v0 (cons 1 v) v1 (cons 2 v)] (= (rest v0) (rest v1)))
</code></pre>

<p>aaaaand they&#8217;re actually identical:</p>

<pre><code>(let [v [4 5 6] v0 (cons 1 v) v1 (cons 2 v)] (identical? (rest v0) (rest v1)))
false
</code></pre>

<p>WHOOPSY DAISY: this is apparently wrong because I&#8217;m using a vector and
not a list&#8230; why? Let&#8217;s see what class they are</p>

<pre><code>(let [v [4 5 6] v0 (cons 1 v) v1 (cons 2 v)] (map class (map next [v0 v1])) )
(clojure.lang.PersistentVector$ChunkedSeq clojure.lang.PersistentVector$ChunkedSeq)
</code></pre>

<p>So <code>cons</code> casts the provided collection to a <code>seq</code>, and the vector
obliges by giving it a chunked sequence, which loses all the vector-y
stuff about it. No casting needs to take place if it&#8217;s already a seq
though, hence lists work.</p>

<p>CONFUSING SHIT:</p>

<pre><code>user=&gt; (def h {:a 1})
#'user/h
user=&gt; (h :a)
1
user=&gt; (:a h)
1
</code></pre>

<p>Basically, both keywords and collections themselves can behave like fns.</p>

<h2>&#8220;changes&#8221; in immutable structures</h2>

<p>A new root &#8220;node&#8221; is always returned, whether you&#8217;re pushing to a
vector, etc, but it might point to the same things.</p>

<h2>lazy-seq, thunks</h2>

<p>A thunk is a description of an operation, usually one step in an
iteration. Lazy-seq presumably rearranges your eager seq operations
into lazy thungs.</p>

<p>If you hold onto the head of a lazy sequence though, you can lose all
the benefits because the head (and the rest of the sequence) won&#8217;t be
garbage collected. <code>lazy-seq</code> is only useful when intermediate
calculations can be forgotten, hence the difference between the
following:</p>

<pre><code>(let [r (range 1e10)]
  (first r) 
  (last r))

(let [r (range 1e10)]
  (last r)   ; r still needed for following calc, so clojure doesn't clear it
  (first r))
</code></pre>

<p>Basically we broke the rule of holding onto to the head, which means
things might not get garbage collected the way we&#8217;d like; this can
happen in any case where we do bindings, whether let or fn arg bindings.</p>

<p>So while Clojure&#8217;s smart enough to aggressively clear out (for GC)
locals it no longer needs, it won&#8217;t rearrange code for us because it has
no way of knowing whether <code>first</code> and <code>last</code> are pure, which might make
it unsafe to re-order; note that Haskell doesn&#8217;t have this problem
because it&#8217;s purely functional.</p>

<h2>iterate</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>user=&gt; (doc iterate)
</span><span class='line'>-------------------------
</span><span class='line'>clojure.core/iterate
</span><span class='line'>([f x])
</span><span class='line'>  Returns a lazy sequence of x, (f x), (f (f x)) etc. f must be free of side-effects</span></code></pre></td></tr></table></div></figure>


<p>Ah so x doesn&#8217;t even need to be a liast, could just be a number.</p>

<pre><code>(second (iterate inc 1))
2 
</code></pre>

<h2><code>-&gt;</code></h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>clojure.core/-&gt;
</span><span class='line'>([x & forms])
</span><span class='line'>Macro
</span><span class='line'>  Threads the expr through the forms. Inserts x as the
</span><span class='line'>  second item in the first form, making a list of it if it is not a
</span><span class='line'>  list already. If there are more forms, inserts the first form as the
</span><span class='line'>  second item in second form, etc.</span></code></pre></td></tr></table></div></figure>


<p>I have no fucking idea.</p>

<h2>take</h2>

<pre><code>user=&gt; (take 5 (iterate inc 1))
(1 2 3 4 5)
user=&gt; (take 5 (range))
(0 1 2 3 4)
user=&gt; (take 5 (filter even? (range)))
(0 2 4 6 8)
</code></pre>

<p>Ruby/Rails kept making me thing something like <code>.first(5)</code>, but <code>take</code>
is the functional thing I was looking for.</p>

<p>Unrelated, but</p>

<pre><code>user=&gt; (/ 5)
1/5
</code></pre>

<blockquote><p>If no denominators are supplied, returns 1/numerator,</p></blockquote>

<h2>Quicksort</h2>

<p>I forgot quicksort, so here I go remembering it:</p>

<ul>
<li>Take your array and choose an element in the array (the pivot)</li>
<li>loop through O(n) and swap elements such that all <code>&lt;</code> elements are to
the left of the pivot and all <code>&gt;=</code> elements are to the right.</li>
<li><p>Recurse into the unsorted partitions and do to the same thing.</p>

<p>  4 9 1 3 7 2 8</p>

<pre><code>          P
        &lt; P
      &lt;   P
    &lt;     P
  &lt;       P
&gt;         P
2       8 9
</code></pre>

  4 2 1 3 7 8 9
  &lt;         P

<pre><code>        |
        | 9   -- sorted
    &lt; P |
  &lt;   P |
&lt;     P |
</code></pre>

  &lt;       P |
  4 2 1 3 7 8 9

<pre><code>  &lt; P | |
&lt;   P | |
</code></pre>

<blockquote><pre><code>P | |
</code></pre>

<p>  1 2 3 4 | |
  &#8230;blahblahblah&#8230;
  1 2 3 4 7 8 9</p></blockquote></li>
</ul>


<p>It is sorrrrted. Divide and conquer babeh, like mergesort.</p>

<h2><code>list</code> and <code>list*</code></h2>

<pre><code>user=&gt; (list 1 2 3 4)
(1 2 3 4)
user=&gt; (list [1 2 3 4])
([1 2 3 4])
user=&gt; (list (range 5))
((0 1 2 3 4))
user=&gt; (apply list (range 5))
(0 1 2 3 4)
user=&gt; (list* () () () ())
(() () ())
user=&gt; (list* () () () '(6 7 8))
(() () () 6 7 8)
user=&gt; (list* 3 2 1 '(6 7 8))
(3 2 1 6 7 8)
</code></pre>

<p>So <code>list*</code> is just <code>list</code> where the final arg is a sequence.</p>

<p>unrelated:</p>

<pre><code>user=&gt; (let [a 3 fn #(&lt; % a)] (filter fn '(1 2 3 6)))
(1 2)
user=&gt; (let [a 3 fn #(&lt; % a)] (remove fn '(1 2 3 6)))
(3 6)

(defn sort-parts [work]
  (lazy-seq
    (loop [[part &amp; parts] work]
      (println "work:" work)
      (println "part:" part)
      (if-let [[pivot &amp; xs] (seq part)]
        (let [smaller? #(&lt; % pivot)]
          (recur (list*
            (filter smaller? xs)
            pivot
            (remove smaller? xs)
            parts)))
      (when-let [[x &amp; parts] parts]
        (cons x (sort-parts parts)))))))

(sort-parts (list [9 1 4 2 6]))

work: ([9 1 4 2 6])
part: [9 1 4 2 6]
work: ([9 1 4 2 6])
part: (1 4 2 6)
work: ([9 1 4 2 6])
part: ()
work: ((4 2 6) 9 ())
part: (4 2 6)
work: ((4 2 6) 9 ())
art: (2)
work: ((4 2 6) 9 ())
part: ()
work: (() 4 (6) 9 ())
part: ()
work: ((6) 9 ())
part: (6)
work: ((6) 9 ())
part: ()
work: (() 9 ())
part: ()
work: (())
part: ()
</code></pre>

<p>AH HAH this is fucked up because I was thinking <code>recur</code> was going to
somehow rewrite the value of <code>work</code>. It won&#8217;t. It&#8217;ll change the
&#8220;l-values&#8221;, whereas <code>work</code> is just meant as the initial value of the
&#8220;l-values&#8221;. So it should actually be:</p>

<pre><code>(defn sort-parts [work]
  (lazy-seq
    (loop [[part &amp; parts] work]
      (println "part/recur'd min:" part)
      (println "parts:           " parts)
      (if-let [[pivot &amp; xs] (seq part)]
        (let [smaller? #(&lt; % pivot)]
          (recur (list*
            (filter smaller? xs)
            pivot
            (remove smaller? xs)
            parts)))
      (when-let [[x &amp; parts] parts]
        (println "FULLRECURSE this has been sorted:" x)
        (cons x (sort-parts parts)))))))

(sort-parts (list [9 1 4 2 6]))
</code></pre>

<p>Ahh so the final <code>(cons x (sort-parts parts))</code> is how we return one
sorted element at a time, because <code>cons</code> returns a Cons and is capable
of being lazy. Can you do a lazy-seq with a recur? Probably not, since
recur is almost just a looping construct. The cons as a return value
makes sense, because it returns (cons x lazy-seq).</p>

<p>REMEMBER: destructuring means the &#8220;rest&#8221; list will be nil if nothing&#8217;s
there.</p>

<pre><code>user=&gt; (let [[x &amp; xs] '(1)] (println xs))
nil
</code></pre>

<p>What&#8217;s the difference between a PersistentList <code>()</code> and a seq?</p>

<p>A seq is any collection that implements the <code>Seq</code> api, i.e. you can call
<code>first</code> and <code>rest</code> on it.</p>

<pre><code>user=&gt; (class (seq '(1)))
clojure.lang.PersistentList
user=&gt; (class '(1))
clojure.lang.PersistentList
</code></pre>

<p>Sometimes collections need to be cast in some way into some derivative
class to efficiently implement <code>seq</code>, e.g. vectors:</p>

<pre><code>user=&gt; (class (seq [1]))
clojure.lang.PersistentVector$ChunkedSeq

user=&gt; (class (rest [1 2 3 4]))
clojure.lang.PersistentVector$ChunkedSeq
</code></pre>

<p>So that <code>recur (list *...</code> bit&#8230; <code>list*</code> won&#8217;t flatten the args you
pass to it, so if the filters/removes yield <code>()</code>, then it might recur
with <code>() 5 ()</code>, which then gets split into <code>part=() parts=(5 ())</code>.
Then the <code>if-let</code> will fail because <code>(seq ())</code> yields nil, and then
when-let does&#8230; wait</p>

<p>What&#8217;s the difference between <code>if</code> and <code>when</code>?</p>

<p>https://groups.google.com/forum/#!topic/clojure/WDIZ6VDhSr0</p>

<p>Basically, <code>if</code> is sa special form with format <code>(if test then else?)</code>.
You must always pass a <code>then</code>, but <code>else</code> is optional, but else is
always the arg right after then. So if you wanted to do multiple things
in <code>then</code>, you&#8217;d need to use a <code>do</code>. e.g.</p>

<pre><code>(if true
  (do
    (println "holy")
    (println "shit")))

(if false
  (do
    (println "holy")
    (println "shit"))
  (println "FALSE!!!"))
</code></pre>

<p>But the format of <code>when</code> doesn&#8217;t have an else based on the number of
args because there&#8217;s an implicit do. It&#8217;s a macro. <code>(source when)</code>:</p>

<pre><code>(defmacro when
  "Evaluates test. If logical true, evaluates body in an implicit do."
  {:added "1.0"}
  [test &amp; body]
  (list 'if test (cons 'do body)))
</code></pre>

<h2>cond :else</h2>

<p>http://stackoverflow.com/a/6323249/914123</p>

<p>Things to remember:</p>

<ol>
<li>:else only seems to idiomatically show up with <code>cond</code></li>
<li><p>:else could be replaced by any truthy thing; Clojure / cond don&#8217;t
specifically look out for it, but because it&#8217;s truthy, it&#8217;ll get
evaluated.</p>

<p> user=>     (cond true 123 true 456)
 123
 user=>     (cond false 123 true 456)
 456</p>

<p> user=> (cond (= 4 5) &#8220;haha&#8221; (= 7 9) &#8220;lol&#8221; :else &#8220;HORSES ASS&#8221;)
 &#8220;HORSES ASS&#8221;
 user=> (cond (= 4 5) &#8220;haha&#8221; (= 7 9) &#8220;lol&#8221; :IAMAMONKEYSASSHOLE &#8220;HORSES ASS&#8221;)
 &#8220;HORSES ASS&#8221;
 user=> (cond (= 4 5) &#8220;haha&#8221; (= 7 9) &#8220;lol&#8221; false &#8220;HORSES ASS&#8221;)
 nil</p></li>
</ol>


<h2>Moscow Mule</h2>

<p>A vodka + ginger beer + lemon juice mixed drink in a copper mug.</p>

<p>http://en.wikipedia.org/wiki/Moscow_mule</p>

<p>Invented in the 50s. There was a vodka craze in the 50s.</p>

<p>http://www.thenibble.com/reviews/main/cocktails/vodka-history.asp</p>

<p>Apparently The Mule kicked off the vodka craze. Then came 60s Bond
saying Vodka Martini, shaken not stirred.</p>

<h2>A fish rots from the head down</h2>

<p>I guess it means that whoever&#8217;s in charge sets the example.</p>

<p>http://www.zdnet.com/article/lennart-poetterings-linus-torvalds-rant/</p>

<blockquote><p>&#8220;If Linux had success, then that certainly happened despite, not because of this [Torvald&#8217;s] behavior. I am pretty sure the damage being done by this is quite obvious, it not only sours the tone in the Linux community, it is also teaches new contributors to adopt the same style, but that&#8217;s only if it doesn&#8217;t scare them away in the first place. In other words: A fish rots from the head down.&#8221;</p></blockquote>

<p>Or &#8220;When an organization or state fails, it is the leadership that is
the root cause.&#8221;</p>

<p>And it&#8217;s not true; guts rot before anything near the fish&#8217;s head.</p>

<h2>We aren&#8217;t all comely young women</h2>

<p>http://www.reddit.com/r/pics/comments/2qdubq/we_arent_all_comely_young_women_some_of_are_old/</p>

<p>comely: pleasant to look at</p>

<h2>systemd</h2>

<p>http://en.wikipedia.org/wiki/Systemd</p>

<p>systemd replaces the <code>init</code> daemon, but also refers to the suite of low
level software surrounding this low level tool for</p>

<h2>Clojure: seq cons list</h2>

<ul>
<li>(seq): returns a seq on the passed-in collection. A seq is just
something that&#8217;s guaranteed to implement <code>first</code> and <code>rest</code>, which is
all you need to iterate through a collection. KEY THING: it doesn&#8217;t
append anything, it just converts to a seq. Note that seq returns
whatever&#8217;s passed to it if it&#8217;s already a seq:
<code>(let [a []] (= (seq a) (seq (seq a))))</code></li>
<li><p>(cons): returns a new seq where x is first el and seq is the rest.</p>

<p>  user=> (def a (lazy-seq (range)))
  user=> (class a)
  clojure.lang.LazySeq</p></li>
<li><p>(list) isn&#8217;t just an API of first/rest but is an actual datatype, a
singly linked list where every item knows its distance from the end.</p>

<p>  user=> (let [a (list 1)] (= a ( seq (seq a))))
  true</p></li>
</ul>


<p>Ughhh how is this possible? It seems like a list is already a seq and
hence doesn&#8217;t need to change in any way to be a seq? A list is a
concrete implementation of seq, one that is so close that no translation
needs to take place in the way a vector would.</p>

<h2>Collections as functions</h2>

<pre><code>([6 7 8] 2)
({:a 123} :a)
</code></pre>

<p>Collections act as functions. Not only can collections act as functions,
but functions can act as data, dem FIRST CLASS SHITS.</p>

<p>First class means</p>

<ul>
<li>createable on demand</li>
<li>can be stored in data structure</li>
<li>can be passed into a fn</li>
<li>can be returned from fn</li>
</ul>


<p>Compose w comp</p>

<pre><code>(def fifth (comp first rest rest rest))
</code></pre>

<p>Create on demand nth fn</p>

<pre><code>(defn fnth [n]
  (apply comp
     (cons first
           (take (dec n) (repeat rest)))))

((fnth 5) '[a b c d e])
</code></pre>

<p>So why is this ok?</p>

<pre><code>user=&gt; (class '[123])
clojure.lang.PersistentVector
user=&gt; (class [123])
clojure.lang.PersistentVector
</code></pre>

<p>Basically the quote means things won&#8217;t be evaluated. You&#8217;re quoting.
Which means you can return symbols without them being evaluated.</p>

<p>Anyway, you apply to comp a cons of (first rest rest rest rest)</p>

<pre><code>user=&gt; (map {:a 3 :b 5} [:a :b])
(3 5)
user=&gt; (map (comp keyword #(.toLowerCase %) name) '(a B C))
(:a :b :c)
</code></pre>

<h2>CURRYING in clojure</h2>

<p>Use <code>partial</code>, but partial isn&#8217;t exactly what you might think.
Specifically, it is not currying because currying keeps returning new
functions until concrete args have been provided for all of its args,
whereas with partial, there&#8217;s no concept of a fixed number of args, but
once you&#8217;ve constructed a function with partial, once you invoke that
partial, it immediately tries to evaluate.</p>

<h2>Complement</h2>

<pre><code>((complement even?) 3)
((comp not even?) 3)
(#(not (even? %)) 3)
</code></pre>

<h2>metadata</h2>

<pre><code>(defn join
  {:test (fn []
            (assert
              (= (join "," [1 2 3]) "1,3,3")))}
  [sep s]
  (apply str (interpose sep s)))
</code></pre>

<p>That&#8217;s kinda rad. There&#8217;s also shorthand:</p>

<pre><code>(defn ^:woot ^:yeah fn-name [] (println "yeah"))

(defn fn-name 
  { :woot true, :yeah true }
  [] (println "yeah"))
</code></pre>

<h2>Higher order fn</h2>

<p>one that either</p>

<ul>
<li>takes 1+ fns as args</li>
<li>returns a fn</li>
</ul>


<h2>holy shit</h2>

<pre><code>user=&gt;    (sort-by second [[:a 7] [:c 31] [:b 21]])
([:a 7] [:b 21] [:c 31])
</code></pre>

<p>or comparing different types as if they were strings</p>

<pre><code>(sort-by str [:b :a {} 5 1 2 "c" "a"])
</code></pre>

<p>or treating keywords as fns</p>

<pre><code>(sort-by :lastname [{ :lastname "Match" } { :lastname "Borf" } { :lastname "Sally" }])
</code></pre>

<p>Remember the function-like things! Like keywords.</p>

<h2>EFT</h2>

<p>Electronic Funds Transfer, a means of payment for various sites that
often just means withdrawing from your checking account, so you gotta
enter routing number account number blah blah blah.</p>

<h2>Pure fn</h2>

<ul>
<li>always returns same result given same arguments</li>
<li>no side effects</li>
</ul>


<h2>referential transparency</h2>

<p>The reference to the function is transparent to time. Time has no
effect. It always does the same thing. But if it changes some internal
(or external) state, then you lose it.</p>

<h2>recursion</h2>

<ul>
<li>mundane: explicitly named recursion, rather than mutual or recur</li>
<li>linear: e.g. one stack per iteration</li>
</ul>


<p>Tail-recursive pow</p>

<pre><code>(defn pow [base exp]
  (letfn [(kapow [base exp acc]
             (if (zero? exp)
                acc
                (recur base (dec exp) (* base acc))))]
         (kapow base exp 1)))
</code></pre>

<p>Two techniques for mundane recursion to tail recursion:</p>

<ul>
<li>use internal helper fn</li>
<li>helper fn uses an accumulator</li>
</ul>


<p>Another way to avoid stack overflow exceptions with mundane recursion is
to pair w lazy-seq.</p>

<h2>Arrow</h2>

<p>The &#8220;thread-first&#8221; macro (AKA &#8220;thrush&#8221; operator?)</p>

<pre><code>http://clojuredocs.org/clojure.core/-%3E

user=&gt; (+ 3 (+ 4 (+ 5)))
12
user=&gt; (-&gt; 5
  #_=&gt;     (+ 4)
  #_=&gt;     (+ 3))
12
</code></pre>

<p>It basically inverts nestings:</p>

<pre><code>user=&gt; (-&gt; "a b c d" 
           .toUpperCase 
           (.replace "A" "X") 
           (.split " ") 
           first)
</code></pre>

<p>Useful for pulling values out of deeply nested shits:</p>

<pre><code>user=&gt; (let [h {:a {:b {:c 123}}}] (-&gt; h :a :b :c))
123
</code></pre>

<h2>Partition</h2>

<pre><code>user=&gt; (partition 2 [1 2 3])
((1 2))
user=&gt; (partition 2 [1 2 3 4])
((1 2) (3 4))
</code></pre>

<p>It&#8217;s like <code>in_groups_of</code>.</p>

<h2>letfn</h2>

<p>Special form. Like <code>let</code> (also a special form), but let bindings are
serial (later bindings can refer to previous bindings, but there are no
forward bindings). <code>letfn</code> bindings can all refer to each other.</p>

<pre><code>(letfn [(a [] (b)) 
        (b [] (println "b"))] 
          (a))
</code></pre>

<p>Prints &#8216;b&#8217;.</p>

<h2>trampoline</h2>

<p>If you have an algo where you&#8217;d like to keep returning 0-arity fns until
finally it returns a non-fn value, trampoline is your source for
non-stack-consuming mutual recursion. Mutual recursion means rather <code>a</code>
calling <code>a</code>, <code>a</code> might call <code>b</code> might call <code>a</code> might call <code>b</code>, etc.
It&#8217;s actually not that crazy of a fn to understand via <code>(source trampoline)</code>.</p>

<pre><code>(defn trampoline
  ([f]
     (let [ret (f)]
       (if (fn? ret)
         (recur ret)
         ret)))
  ([f &amp; args]
     (trampoline #(apply f args))))
</code></pre>

<p><code>trampoline</code> is meant to accept a 0-arity fn. The 2 lines at the bottom
just convert a 2+arity call to trampoline to a call with a zero-arity
fn. Trampoline will</p>

<ul>
<li>call <code>f</code> (with supplied args)</li>
<li>if value returned is a fn, recur (preventing stack consumption)</li>
<li>else return value</li>
</ul>


<p>Wow preeeeeetty freakin simple, and recur makes stuff pretty obvious.</p>

<p>So if I do</p>

<pre><code>(defn do-some-crap []
  (letfn [(a [x] (b x))
          (b [x] (c x))
          (c [x] (d x))
          (d [x] x) ]
            (trampoline a 123)))
</code></pre>

<p>This returns 123 and the chain of a-b-c-d only consumes one stack frame.
Pretty cool. Ahh nevermind I&#8217;m an idiot, i&#8217;m returning values, not
functions, fixed:</p>

<pre><code>(defn do-some-crap []
  (letfn [(a [x] #(b x))
          (b [x] #(c x))
          (c [x] #(d x))
          (d [x] x) ]
            (trampoline a 123)))
</code></pre>

<p>Useful for implementing state machines.</p>

<h2>identity</h2>

<p>Returns its arg:</p>

<pre><code>(identity 5)
</code></pre>

<h2>FICO</h2>

<p>FICO credit scores. Fair Isaac Corporation. Started in 58. Fannie and
Freddie started using their scores in 95. Ubiquitous as fuck.</p>

<p>http://en.wikipedia.org/wiki/FICO</p>

<p>FICO score introduced in 89, based on 3 credit files</p>

<ul>
<li>Experian</li>
<li>Equifax</li>
<li>TransUnion</li>
</ul>


<p>(so people don&#8217;t refer to FICO, but rather FICO polls the 3 above?)</p>

<p>http://en.wikipedia.org/wiki/Credit_score_in_the_United_States#mediaviewer/File:Credit-score-chart.svg</p>

<p>Score composed of</p>

<ul>
<li>35% payment history, presence of derogatory info (defaults, bankruptcies)</li>
<li>30% debt burden, including credit card debt to limit ratio (e.g. if
I&#8217;m $100 in credit card debt, the FICO penalty is smaller if I have a
higher limit).</li>
<li>15% length of credit history: average age of accounts, and age of
oldest account (hence a new CC might temporarily lower score since it
decreases average age).</li>
<li>10% types/diversity of credit:</li>
<li>10% recent searches

<ul>
<li>hard inquiries:

<ul>
<li>applications for loans/CCs count against you, but then again these
are coalesced within 30-45 days as a single query if you&#8217;re &#8220;rate
shopping&#8221;</li>
</ul>
</li>
<li>soft inquiries:

<ul>
<li>consumer queries</li>
<li>applying to rent an apartment</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Hard inquiries look bad because maybe you&#8217;re desperate for credit or
maybe they&#8217;re so frequently because past hard inquiries led to your
rejection. Limit to 1-2 times a year to minimize penalty.</p>

<p>When people say triple credit score, they mean they&#8217;ll show you your
Experian, Equifax, and TransUnion scores.</p>

<p>FICO scores are out of 850. 280 is shitty, 700 is &#8220;good&#8221;, 760+ is
excellent.</p>

<h2>Mortgage</h2>

<p>http://en.wikipedia.org/wiki/Mortgage_loan</p>

<p>The distinguishing characteristic is that real or personal property is
put up as collateral for the loan in the case of default.</p>

<p>Real property refers to legally defined property mixed with human
effort, e.g. houses, roads, etc. Usually means &#8220;land and anything
affixed to it&#8221;, vs private property (&#8220;clothing, furniture, money&#8221;).</p>

<h2>Alternative Financial Services</h2>

<p>http://en.wikipedia.org/wiki/Alternative_financial_services</p>

<p>Pawn shops, paycheck advancements.</p>

<p>In impoverished countries, microfinancing also falls under this
category.</p>

<h2>Catfish</h2>

<p>http://www.urbandictionary.com/define.php?term=catfish</p>

<blockquote><p>A catfish is someone who pretends to be someone they&#8217;re not
using Facebook or other social media to create false identities,
particularly to pursue deceptive online romances.</p></blockquote>

<h2>Installment Credit / Fixed Credit</h2>

<p>Installment: type of credit that has fixed no of payments.</p>

<p>Revolving credit: no fixed payments, can withdraw from balance,
transfer, etc, e.g. credit cards.</p>

<h2>Clojure riddle</h2>

<pre><code>(take 1 (map (fn [a] (println a)) (range)))
(take 32 (map (fn [a] (println a)) (range)))
</code></pre>

<p>Both of these print 0 to 31.</p>

<pre><code>(take 32 (map (fn [a] (println a)) (range)))
</code></pre>

<p>This prints 0-63. Why? I think it has something to do with 32 legged
tries something something. Or chunks. Or something.</p>

<h2>IRAs</h2>

<p>Individual Retirement Account</p>

<p>http://www.rothira.com/traditional-ira-vs-roth-ira</p>

<p>Traditional IRA:</p>

<ul>
<li>deposit pre-tax assets</li>
<li>no income limits</li>
<li>started in 1974 in some congressional act</li>
</ul>


<p>Roth IRA:</p>

<ul>
<li>deposit post-tax assets</li>
<li>early withdraw has 10% penalty, with some exceptions for first house,
education expenses, etc</li>
<li>maximum income enforced of 114k-128k before you can&#8217;t contribute</li>
<li>desirable if you think you&#8217;ll be in a higher tax bracket if you&#8217;re
older (relative to IRA where you&#8217;re taxed when you withdraw, so if
you&#8217;re withdrawing at a higher tax bracket then you&#8217;re boned).</li>
<li>largely targets younger, lower income</li>
<li>established in 1997 after proposed in 89 by someone and senator
William Roth</li>
</ul>


<p>In general there&#8217;s penalties for early withdraw until 59.5 years of age
(who decides this shit?).</p>

<p>401k is where an employee deducts from employee&#8217;s paycheck (optionally
matching) and makes a tax-free deposit. Withdraws are taxed though, like
a traditional IRA.</p>

<p>401k different from IRAs in it&#8217;s a plan optionally provided by an
employer. eg A freelancer can&#8217;t take advantage of a 401k, and not all
employees can take advantage of a company&#8217;s 401k.</p>

<p>All of this to encourage people to save and invest and prepare ourselves
for retirement.</p>

<p>Both IRAs and 401ks have the minimum penalty distribution (withdrawals)
age of 59 1/2 years.</p>

<p>IRAs have limits like $5,500 per a given year. This might change from
year to year though.</p>

<h2>Smart Folder</h2>

<p>http://www.cultofmac.com/48911/100-tips-19-what-are-smart-folders/</p>

<p>AMAZINGLY I have no idea what this is.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Xorblasm]]></title>
    <link href="http://machty.github.com/blog/2014/12/10/xorblasm/"/>
    <updated>2014-12-10T11:04:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/12/10/xorblasm</id>
    <content type="html"><![CDATA[<h2>Frameless App</h2>

<p>Chromeless app, nothing more than a webview really, for quick
prototyping of mobile web apps.</p>

<p>https://itunes.apple.com/us/app/frameless-full-screen-web/id933580264?mt=8</p>

<h2>Servo</h2>

<p>Parallel browser project written in Rust for parallel execution, memory
safety, etc. Doesn&#8217;t intend to be a full on user-facing browser, but
intends to have other folk build on top of it.</p>

<p>http://blog.servo.org/</p>

<h2>&#8220;use strict&#8221;; as opposed to sloppy</h2>

<p>Non <code>"use strict";</code> mode is AKA sloppy mode.</p>

<h2>Safari inspector learnings</h2>

<p>There are THREE TABS:</p>

<ul>
<li>Resources (ctrl + 1)</li>
<li>Timelines (ctrl + 2)</li>
<li>Debugger (ctrl + 3)</li>
</ul>


<p>They control the middle view, but when active they can be clicked again
to remove the left panel. THAT IS KINDA FUCKED UI, PEOPLE.</p>

<p>Pressing Escape a bunch of times always yields Console, for which there
is a full on console tab, but in Resources (1) and Debugger (2) you can
open a debugger on the bottom, but on Timelines I guess it doesn&#8217;t make
sense to embed that console. Alt+Command+C opens the full on console.
Functionality on full screen console and embedded console seems the
same.</p>

<p>Filter vs Search resources:</p>

<ul>
<li>filter: filename only</li>
<li>search: searches file contents</li>
</ul>


<h2>TIL <code>sessionStorage</code></h2>

<p>Ridiculous, why is this useful? Why is this fundamentally different than
a global var?</p>

<h2>(Safari) Webarchive</h2>

<p>http://en.wikipedia.org/wiki/Webarchive</p>

<p>Save all resources associated w a document, used by Safari.</p>

<p>You can go to Resources tab and click the down arrow to download a
webarchive.</p>

<h2>Tiled Rendering</h2>

<p>Android and pretty much everyone uses tiled rendering.</p>

<p>http://en.wikipedia.org/wiki/Tiled_rendering#cite_note-1</p>

<p>&#8220;Show composited layer borders&#8221; in chrome will show you all the layers
that get uploaded to the GPU, after which the GPU does compositing. So
you paint w CPU and then composite on the GPU.</p>

<h2>Continuous Repaint</h2>

<p>Repaints don&#8217;t happen all the time, but you still might want to identify
which parts of the page are slow to paint, so you can force the renderer
to repaint all the things all the time by enabling this setting, showing
and removing sections.</p>

<p>https://developer.chrome.com/devtools/docs/rendering-settings</p>

<h2>Chrome Timeline Counters</h2>

<ul>
<li>Listeners: [1:4]</li>
</ul>


<p>means that at the start of listening, there was 1 listener, and at the
end, there were 4.</p>

<h2>Sysdig</h2>

<p>http://www.sysdig.org/</p>

<blockquote><p>Think of it as strace + tcpdump + lsof + awesome sauce.</p></blockquote>

<h2>Gradle daemon and parallel</h2>

<p>Add this to your <code>gradle.properties</code>:</p>

<pre><code>org.gradle.daemon=true
org.gradle.parallel=true
</code></pre>

<p>IT DOES WHAT YOU THINK IT DOES.</p>

<h2>Comme de Garcons</h2>

<p>http://en.wikipedia.org/wiki/Comme_des_Gar%C3%A7ons</p>

<p>Heart with eyes logo.</p>

<h2>Cul-de-sac</h2>

<p>http://en.wikipedia.org/wiki/Cul-de-sac</p>

<p>It just means a fucking dead end. DEAD END.</p>

<p>One of these shits:</p>

<p><img src="http://upload.wikimedia.org/wikipedia/commons/b/b2/Cul-de-Sac_cropped.jpg" alt="cul de sac" /></p>

<h2>Live updates in Cordova</h2>

<ul>
<li>HTTP caching

<ul>
<li>Make sure the WebView is set up to cache</li>
<li>Do we have control over that though? What if webview ejects these
items from cache? Can we prevent that?</li>
<li>Assuming full, non-forgetting HTTP caching is an option:

<ul>
<li>user session GET query returns assets that the user should use</li>
<li>webview downloads them; assets are aggressively cached</li>
</ul>
</li>
<li>But what about</li>
</ul>
</li>
</ul>


<h2>HTMLBars attr interpolation</h2>

<pre><code>&lt;div attr=&gt;
</code></pre>

<p>vs
,[</p>

<pre><code>&lt;div attr=""&gt;
</code></pre>

<p>The rule is that quotes ALWAYS means string interpolation. Even if the
above evals to the same thing, the latter goes through a layer of string
interpolation.</p>

<h2>Broccoli funnel</h2>

<p>subset of a tree, matched by regex.</p>

<p>https://www.npmjs.com/package/broccoli-funnel</p>

<h2>Ember-cli-addons</h2>

<p>Hahah of course, just including an ember-cli addon doesn&#8217;t mean</p>

<h2>No property syntax in ES6 class syntax</h2>

<p>This is intentional. You have to set all your properties in the
<code>constructor()</code>.</p>

<h2>popState, router stuff</h2>

<p>I need to fix this:</p>

<p>https://github.com/emberjs/ember.js/pull/9752</p>

<p>Transitions need to be undoable.</p>

<ul>
<li>pushState transition from &#8216;foo&#8217; -> &#8216;bar&#8217;

<ul>
<li>undo: popState</li>
</ul>
</li>
<li>replaceState transition from &#8216;foo&#8217; -> &#8216;bar&#8217;

<ul>
<li>undo: replaceState(&#8216;bar&#8217;)</li>
</ul>
</li>
</ul>


<p>No such support for back button w HashLocation.</p>

<blockquote><p>Clicking back and then clicking forward, and aborting the transition in the willTransition is still broken. The reason is that when I hit back I now the previous route, but when I hit back and then forward again I don&#8217;t have that info.</p></blockquote>

<p>You don&#8217;t have &#8220;forward&#8221;. Like, the URL you&#8217;re going to?</p>

<h2>onpopstate fires for all changes?</h2>

<p>https://developer.mozilla.org/en-US/docs/Web/Guide/API/DOM/Manipulating_the_browser_history#The_popstate_event</p>

<p>Weird.</p>

<p>http://jsbin.com/coxega/1#foo</p>

<p>Poorly named?</p>

<blockquote><p>Browsers tend to handle the popstate event differently on page load. Chrome (prior to v34) and Safari always emit a popstate event on page load, but Firefox doesn&#8217;t.</p></blockquote>

<h2>.npmignore defaults to .gitignore</h2>

<p>nuff said.</p>

<h2>How do preview a publish?</h2>

<p><code>npm pack</code></p>

<p>Then inspect the tgz.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[808s and Android Heartbreak]]></title>
    <link href="http://machty.github.com/blog/2014/12/08/808s-and-android-heartbreak/"/>
    <updated>2014-12-08T12:21:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/12/08/808s-and-android-heartbreak</id>
    <content type="html"><![CDATA[<h2>TL;DR on Android WebView Form Shit</h2>

<p>Here&#8217;s a list of unsurmountable barriers involved when implementing
forms in a mobile web app in Cordova</p>

<ul>
<li>You <em>cannot</em> focus an input field on a setTimeout

<ul>
<li>It must be tied to a user input event (e.g. a touchstart/end)</li>
<li>It might look focused but it won&#8217;t bring up the keyboard</li>
<li>You can halfass get things working in cordova by using a plugin that
opens the keyboard before you tell the field to focus, but it you
lose all configurability of the keyboard (whether it&#8217;s numpad,
whether autocorrect is enabled, etc).</li>
</ul>
</li>
</ul>


<h2>Android WebView</h2>

<p>https://www.youtube.com/watch?v=HbOtn5VhGZU</p>

<p>Android Browser just uses a WebView (probably not Chrome).</p>

<p>Platform</p>

<ul>
<li>network, disk</li>
<li>system integration, e.g. HTML5 Camera integration</li>
<li>graphics and rendering; this is why differences exists b/w webkit</li>
<li>V8 is used in Android WebView instead of default webkit</li>
</ul>


<p>What is +1 dependent?</p>

<p>Render loop:</p>

<ul>
<li><p>Event -> Paint -> Draw</p></li>
<li><p>Before Honeycomb (2012?)</p>

<ul>
<li>Paint loops are tight, they capture content into a Picture, and
then render the Picture</li>
<li>Picture is Vector representation of page, not just visible area</li>
<li>No need to go back to webkit when

<ul>
<li>scrolling</li>
<li>zooming</li>
</ul>
</li>
</ul>
</li>
<li>Multithreading

<ul>
<li>UI Thread (Picture)</li>
<li>WebCore Thread (webkit; generates a new Picture that UI thread can
use for zooming, scrolling, etc)</li>
</ul>
</li>
<li>Software rendering

<ul>
<li>Slow, risk of taking longer than 16ms</li>
<li>CSS3D not supported in software</li>
</ul>
</li>
<li>Async rendering

<ul>
<li>e.g. flash, composited on top and often lagged behind scroll</li>
</ul>
</li>
<li>Software doesn&#8217;t keep up w increasing size</li>
<li>Tiling via Hardware

<ul>
<li>Scroll event kicks off request for new tiles to render, if they
don&#8217;t arrive in time, no big deal, everything will be fast even if
there&#8217;s a moment where it&#8217;s blank</li>
</ul>
</li>
<li>Texture Generation Thread?

<ul>
<li>the thing that splits things into tiles</li>
</ul>
</li>
<li>w Hardware acceleration:

<ul>
<li>Painting slower, drawing (compositing?) faster!</li>
<li>Precache tiles

<ul>
<li>prefetch surrounding tiles</li>
<li>direction dependent (e.g. prefetch tiles below if you&#8217;re scrolling
down)</li>
</ul>
</li>
<li>low-res tiles when scrolling quickly&#8230; blur sharpens once it&#8217;s had
time to settle and finish painting</li>
<li>memory usage

<ul>
<li>limited number of tiles (device dependent)</li>
<li>tiles are 256x256</li>
<li>check if plain colors&#8230; JellyBean</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>AHH fuck it all it&#8217;s powered in Chromium now. Why bother.</p>

<p>Android Versions</p>

<p>Operating Systems:</p>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Android_Ice_Cream_Sandwich">Android 4: Ice Cream Sandwich</a></li>
</ul>


<h2>Android API levels</h2>

<p>http://developer.android.com/guide/topics/manifest/uses-sdk-element.html#ApiLevels</p>

<p>API Levels are just integers that match up w Platform versions</p>

<pre><code>Platform Version    API Level   VERSION_CODE    Notes
Android 5.0 21  LOLLIPOP    Platform Highlights
Android 4.4W    20  KITKAT_WATCH    KitKat for Wearables Only
Android 4.4 19  KITKAT  Platform Highlights
Android 4.3 18  JELLY_BEAN_MR2  Platform Highlights
Android 4.2, 4.2.2  17  JELLY_BEAN_MR1  Platform Highlights
Android 4.1, 4.1.1  16  JELLY_BEAN  Platform Highlights
Android 4.0.3, 4.0.4    15  ICE_CREAM_SANDWICH_MR1  Platform Highlights
</code></pre>

<h2>Android Activity / Fragment Lifestyle</h2>

<p>https://plus.google.com/+StevePomeroy/posts/HsthxN21Yp1</p>

<p>http://staticfree.info/~steve/complete_android_fragment_lifecycle.png</p>

<p>https://github.com/xxv/android-lifecycle</p>

<h2>Server-push</h2>

<p>http://caniuse.com/#feat=eventsource</p>

<p>Android supports SSE from 4.4 onward (since Nov 2013).</p>

<h2>Android Chromium WebView since 4.4 KitKat (Nov 2013)</h2>

<p>https://www.youtube.com/watch?v=IOY2UNZU9QQ</p>

<p>Android Browser still uses WebView, just that it&#8217;s Chromium now.
Snapshot of Chrome 30.</p>

<p>Chrome vs Chrome WebView</p>

<ul>
<li>Graphics backend</li>
<li>Otherwise invisible to app developer</li>
<li>Chop out Chrome backend, replace w graphics stack compatible w android
graphics.</li>
<li>SurfaceFlinger</li>
<li>Hardware accelerated layers.</li>
</ul>


<p>KitKat webview is way more HTML5 compliant, rather than old WebKit
browser.</p>

<p>Features:</p>

<ul>
<li>IndexedDB (ios only has shitty/buggy support in 8+)

<ul>
<li>async, non-blocking, etc</li>
</ul>
</li>
<li>Websockets</li>
<li>requestAnimationFrame</li>
<li>SVG filters and effects, sepia, convolution blurs</li>
<li>Hardware accelerated

<ul>
<li>everything</li>
<li>default for all new apps is hardware accelerated on</li>
<li>Question: hardware acceleration can be enabled for the app, but also
the webview itself?</li>
</ul>
</li>
</ul>


<p>WebView methods must be run on UI Tthread, use <code>runOnUiThread</code> if you&#8217;re
within a different thread&#8230;</p>

<p>Compositing Thread,</p>

<p>Main UI thread</p>

<p><a href="http://www.ietf.org/rfc/rfc3986.txt">RFC 3986</a> since kitkat all urls
most conform to it.</p>

<p>A CSS pixel corresponds to 1, 2, 4, etc, depending on your viewport.</p>

<p>There must only be one Meta Viewport tag&#8230; only the last one is
actually used.</p>

<p>Shorthand CSS can override others in KitKit+; be specific w CSS
properties unless you really want to override many things.</p>

<h2>uses-sdk</h2>

<p>http://developer.android.com/guide/topics/manifest/uses-sdk-element.html</p>

<p>Always set target API, but you can still support min SDK versions for
fallback.</p>

<h2>lint</h2>

<p>Android <code>lint</code></p>

<pre><code>$ which lint
/Users/machty/adt-bundle-mac-x86_64/sdk/tools/lint
</code></pre>

<p>seems like ADT, the Eclipse plugin suite.</p>

<p>http://developer.android.com/tools/sdk/eclipse-adt.html</p>

<h2>Focusing fields within Android</h2>

<p>Is it actually fucking possible?</p>

<p>https://code.google.com/p/android/issues/detail?id=27438</p>

<p>http://developer.android.com/reference/android/view/inputmethod/InputMethodManager.html#toggleSoftInput(int, int)</p>

<p>WTF is toggleSoftInput vs toggleSoftInputFromWindow vs</p>

<p>TODO: prevent defocusing with</p>

<pre><code>http://developer.android.com/reference/android/view/inputmethod/InputMethodManager.html#SHOW_FORCED
</code></pre>

<p>?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sloggin]]></title>
    <link href="http://machty.github.com/blog/2014/12/01/sloggin/"/>
    <updated>2014-12-01T11:36:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/12/01/sloggin</id>
    <content type="html"><![CDATA[<h2>Gaslighting</h2>

<p>http://en.wikipedia.org/wiki/Gaslighting</p>

<p>Presenting false information to someone until they doubt their own sanity.</p>

<h2>Deductible</h2>

<p>http://en.wikipedia.org/wiki/Deductible</p>

<p>The amount that must be paid by policy owner, out of pocket,
before insurance kicks in.</p>

<blockquote><p>Given the nature of medical treatment, the insured often faces multiple medical expenses spread over several days for a single illness or injury. Due to this reason, health insurance deductibles often tend to be imposed on a term basis (e.g. annually) as opposed to a per-visit threshold.[4] In spite of this, major medical insurance policies may have a per-visit excess which often does not cover the cost of routine visits to a GP, unless it is certified to be a part of a continuous treatment (and the bills can be collated in a single claim).</p></blockquote>

<p>So it&#8217;s annual so that you&#8217;re always paying <em>something</em> out of pocket.
Deductibles are ultimately limited by a Out-of-Pocket Maximum.</p>

<p>http://www.bcbsm.com/index/health-insurance-help/faqs/topics/how-health-insurance-works/deductibles-coinsurance-copays.html</p>

<ul>
<li>If you have $1500, and you get a $2000 medical bill, you have to pay
$1500 of that, no matter what, and then the last $500 might require
paying coinsurance

<ul>
<li>After deductible: insurance kicks in after deductible is paid</li>
<li>No deductible: insurance kicks in immediately without having to pay
a deductible (there could still be coinsurance)</li>
<li>Before deductible: you still have to pay deductible, but</li>
</ul>
</li>
</ul>


<p>Ah ok so you&#8217;re still chipping away at a yearly deductible. After which
you only need to think about coinsurance and copay.</p>

<p>But first, deductible types:</p>

<ul>
<li>Annual: most common, chipping away at deductible throughout the year</li>
<li>Per-episode: min deductible to pay per each type of visit, e.g. each
time you&#8217;re hospitalized</li>
<li>Out of network deductible: what it sounds like; sometimes paying out
of network deductible subtracts from your regular deductible.</li>
<li>Family deductible: multiple members chipping away at deductible</li>
</ul>


<p>Coinsurance: your share of health care service costs, usually a
percentage. e.g. if you&#8217;ve paid your deductible, then you might have to
pay 20% of whatever&#8217;s left over.</p>

<p>Copay: a fixed amount you pay for a given service, each time you use it.
Could be $40 when you buy prescription drugs, $20 every doctor visit,
etc. Copays prevent frequent visits to the same person/service, since
they add up, particular if the service is cheap, whereas coisurance, a
fixed percentage, doesn&#8217;t add up if the service is cheap.</p>

<ul>
<li>Pros

<ul>
<li>You always know the fixed amount (rather than a percentage tied to
total cost)</li>
</ul>
</li>
</ul>


<p>So the difference is just fixed amount vs percentage?</p>

<h2>Honeypot</h2>

<p><a href="http://en.wikipedia.org/wiki/Honeypot_%28computing%29">wiki</a></p>

<p>Hacker lure, like a bear to honey. Pretends to be some network with
useful info, contains nothing, logs attack attempts.</p>

<h2>STEM fields</h2>

<p>http://en.wikipedia.org/wiki/STEM_fields</p>

<p>Science, technology, engineering, math.</p>

<p>http://en.wikipedia.org/wiki/Women_in_STEM_fields</p>

<p>STEM equity: gender equity in STEM fields.</p>

<h2>Who is logged in what are they doing?</h2>

<p><code>w(1)</code>:</p>

<blockquote><p>w &#8211; display who is logged in and what they are doing</p></blockquote>

<pre><code>$ w
USER     TTY      FROM              LOGIN@  IDLE WHAT
machty   console  -                18Nov14 15days -
machty   s000     -                18Nov14 9days tmux a
machty   s029     -                22Nov14     - tmux a
</code></pre>

<h2>rcp, rsh, rcmd</h2>

<p>Non-secure scp, ssh, and rcmd. Uses kind of a crappy auth scheme,
wherein the server, rshd, has its <code>iruserok</code> fn called and can decide
based on IP, etc, whether things are cool.</p>

<p>TLDR use ssh.</p>

<h2>hub pull-request depends on -u upstream</h2>

<p>This used to create pull request from ember to ember</p>

<pre><code>hub pull-request
</code></pre>

<p>I had to do</p>

<pre><code>hub pull-request -h machty:some-crap
</code></pre>

<p>but if I <code>git push -u</code> then it&#8217;ll set the upstream and the original will
work. Hooray. Thanks Robert Jackson.</p>

<h2>bash hashes commands</h2>

<p>Because path resolution is slow, bash will hash the location of a
command. This broke my shit when I upgraded git from the Mac
(/usr/bin/git) to the one from homebrew (/usr/local/bin/git).</p>

<p>Bash has some built-ins <code>hash</code> and <code>type</code>. <code>type</code> tells you how a
command resolves:</p>

<pre><code>$ type git
git is hashed (/usr/local/bin/git)

$ hash
hits    command
   2    /usr/bin/which
   1    /usr/local/bin/node
   2    /Users/machty/.rvm/gems/ruby-2.0.0-p353@global/bin/bundle
   3    /usr/local/bin/git
   1    /Users/machty/.rvm/rubies/ruby-2.0.0-p353/bin/irb
  12    /usr/bin/man
   2    /bin/ls
   2    /usr/bin/rsh
   1    /bin/LS
   1    /usr/bin/w
   1    /usr/bin/dig
   1    /bin/rcp
</code></pre>

<p>And you can invalidate a cached command via <code>hash -r COMMAND</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stankonia]]></title>
    <link href="http://machty.github.com/blog/2014/11/20/stankonia/"/>
    <updated>2014-11-20T16:25:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/20/stankonia</id>
    <content type="html"><![CDATA[<h2>Zonebie</h2>

<p>https://github.com/alindeman/zonebie</p>

<p>Ruby gem to randomly change the current timezone to help catch bugs /
false assumptions in your timezone-touching code.</p>

<p>NOTE: trolled myself because i had a <code>_spec.rb</code> file that didn&#8217;t have
<code>require 'spec_helper'</code> at the top and hence Zonebie magic wasn&#8217;t
happening.</p>

<h2>Hellbanning</h2>

<p>http://en.wikipedia.org/wiki/Hellbanning</p>

<p>Aka, shadowbanning. You don&#8217;t know you&#8217;re banned, but no one sees your
stuff, people stuff responding to you. Used on HN.</p>

<h2>Redis SLAVEOF</h2>

<p>http://redis.io/commands/slaveof</p>

<p>Master-slave replication. A slave has its own port, connects to parent
with SYNC, starts a BGSAVE, shares shit, blah blahblah.</p>

<p>You can even pretend to be a slave with netcat.</p>

<pre><code>nc localhost 9595
SYNC
</code></pre>

<p>And then you&#8217;ll get a stream of all the stuff a redis slave server sees.
If you write to the master you&#8217;ll the slave being sent that same write
request.</p>

<p>Slaves can have slaves. If a master disconnects, slaves become
masters&#8230;?</p>

<p>Slaves are read only:</p>

<pre><code>(error) READONLY You can't write against a read only slave.
</code></pre>

<p>If master disconnects, it&#8217;s still a slave until you tell it to
<code>SLAVE NO ONE</code>, then it severs that connection, preserving the
replicated data, and then the old master might connect to the new master
and invert the master-slave relationship. PRETTY RAD. Or you could also
tell the new master to become a slave, but that&#8217;ll mean it loses all of
its data.</p>

<p>Use slave chains if a master is overloaded w forwarding writes to all of
its slaves. Instead of</p>

<pre><code>- MASTER
  - SLAVE 0
  - SLAVE 1
  - SLAVE 2
  - SLAVE 3
</code></pre>

<p>You could do</p>

<pre><code>- MASTER
  - SLAVE 0
    - SLAVE 1
      - SLAVE 2
        - SLAVE 3
</code></pre>

<h2>Git: tags are a flat hierarchy</h2>

<p>You could release multiple versions of a project from different
branches. There&#8217;s no requirement that all the versions you tag are on
<code>master</code> or <code>release</code> or anything like that. Tags, as opposed to branch
names, are a flattened hiearchy.</p>

<p>A branch is just a pointer.</p>

<p>A tag is just a pointer.</p>

<p>The difference is that if you commit to a branch, the pointer moves. A
tag on the other hand stays pointing to that commit SHA.</p>

<p>A client can make sure a master write made it to slave via checking
UUID&#8217;s, and using <code>INFO</code> command to check sync status, etc.</p>

<h2>Redis Sentinel</h2>

<p>http://redis.io/topics/sentinel</p>

<p>Monitors master/slaves, restarts, notifies, failovers, etc.</p>

<h2>Redis-cli</h2>

<pre><code>-x                 Read last argument from STDIN.

Example:
cat /etc/passwd | redis-cli -x set mypasswd
</code></pre>

<p>So that sets <code>mypasswd</code> to the contents of a file.</p>

<pre><code>$ echo "SOMEVALUE" | redis-cli -x set wat
OK
$ redis-cli get wat
</code></pre>

<h2>Redis: MULTI/EXEC doesn&#8217;t mean pipelining</h2>

<p>Pipelining refers to a redis client queueing commands and then sending
them to a Redis server all at once. The Redis server its has its own
concept of queued commands via MULTI/EXEC, but the concepts are separate;
you could invidually send a bunch of commands including MULTI/EXEC, and
get responses to each, but once you&#8217;re in the MULTI/EXEC block, you
start getting QUEUED as a response.</p>

<h2>Redis: WATCH, DISCARD, MULTI</h2>

<p>You can invalidate your own WATCH pretty immediately:</p>

<pre><code>127.0.0.1:6379&gt; WATCH foo
OK
127.0.0.1:6379&gt; SET foo wat
OK
127.0.0.1:6379&gt; MULTI
OK
127.0.0.1:6379&gt; EXEC
(nil)
</code></pre>

<p>Obviously you can&#8217;t put it in a MULTI</p>

<pre><code>127.0.0.1:6379&gt; MULTI
OK
127.0.0.1:6379&gt; WATCH foo
(error) ERR WATCH inside MULTI is not allowed
</code></pre>

<p>DISCARDS must take place within MULTI block.</p>

<pre><code>127.0.0.1:6379&gt; DISCARD
(error) ERR DISCARD without MULTI
</code></pre>

<h2>Optimistic Locking</h2>

<p>http://en.wikipedia.org/wiki/Optimistic_concurrency_control</p>

<p>Redis implements optimistic locking; it never locks a datatype and
prevents someone from writing; rather, transactions can be aborted and
retried if it&#8217;s detected that someone else wrote to the same data
(detected via watch).</p>

<p>Pessimistic locking would be, say, if a DB row was locked and someone
wanting to write to it was blocked til the lock was given up, which
apparently most databases do.</p>

<p>Postgres (and other relational DBs) have other approaches:</p>

<p>http://blog.2ndquadrant.com/postgresql-anti-patterns-read-modify-write-cycles/</p>

<p>TL;DR Avoid read-modify-write</p>

<h2>No pipelining + latency = multiplied awfulness</h2>

<p>Just like w TCP or anything with roundtrips, latency is the multiplier.
Good thing I wrote this.</p>

<h2>Standard Deviation</h2>

<p>http://en.wikipedia.org/wiki/Standard_deviation#History</p>

<blockquote><p>The term standard deviation was first used[10] in writing by Karl Pearson[11] in 1894, following his use of it in lectures.</p></blockquote>

<p>So there&#8217;s no statistical meaning behind the word &#8220;standard&#8221;. We&#8217;re all
just talking about &#8220;deviation&#8221;, using the most basic/default/standard
way of calculating it.</p>

<h2>Ruby blocks v Python context manager</h2>

<p>One thing I noticed regarding <code>return</code>s:</p>

<pre><code>def foo():
  print(1)
  yield
  print(2)

def bar():
  with foo():
    print(999)
    return "LOL"
</code></pre>

<p>In python this yields</p>

<pre><code>1
999
2
</code></pre>

<p>and returns &#8216;LOL&#8217;. This is interesting because the <code>print(2)</code> is still
hit, which isn&#8217;t what would happen in Ruby.</p>

<pre><code>def foo
  puts 1
  yield
  puts 2
end

def bar
  foo { puts "999"; return "LOL" }
end
</code></pre>

<p>yields</p>

<pre><code>1
999
=&gt; "LOL"
</code></pre>

<p>The <code>2</code> isn&#8217;t printed because in Ruby, the return causes the caller to
return.</p>

<h2>Redis ZSETS sort by key when scores equal</h2>

<p>That&#8217;s all.</p>

<h2>Lua</h2>

<p>http://en.wikipedia.org/wiki/Lua_(programming_language)</p>

<p>Why use Lua:</p>

<ul>
<li>you&#8217;re building something that needs to be scriptable</li>
<li>that shouldn&#8217;t have a heavy footprint from the interpreter</li>
<li>that might be in an embedded system</li>
<li>that is easy to grok</li>
<li>that lots of people use already</li>
</ul>


<p><a href="http://en.wikipedia.org/wiki/Wikipedia:Lua">Wikipedia is Lua-extensible</a></p>

<p>Redis offers Lua scripting now, useful in certain cases over
pipelining/multi-exec, since (I think) it allows logic to be placed on
the Redis server rather than having to do all logic on the application
sides and remembering to do all the WATCH/MULTI/EXEC crap.</p>

<h2>Redis: why lock?</h2>

<p>Because WATCH/retry loops quickly degrade as the number of actors
increase; particularly if the WATCH is coarse (only keys can be WATCH&#8217;d,
not specific items in a hash).</p>

<p>Instead of retrying, a lock might make sense in this case.</p>

<p>Implementing locks is error-prone:</p>

<ul>
<li>a process grabs a lock, but takes too long and the lock times out, and
process is blissfully unaware and keeps doing potentially destructive
things</li>
<li>process crashes, doesn&#8217;t releaes lock, and everyone wastes time
waiting for the timeout</li>
<li>process crashes, other blocked processes try to acquire lock at same
time, think they each got the lock. (This is an issue in general but
more likely to happen if many processes attempting-to-lock are blocked
at the same time)</li>
</ul>


<p>SETNX only writes if not already present. It&#8217;s a test and set. Useful
for locks. You can just spin-lock on a sleep(.001) loop.</p>

<p>Increasing the granularity of lock generally improves perf.</p>

<h2>Dogpiling</h2>

<p>http://en.wikipedia.org/wiki/Cache_stampede</p>

<p>aka cache stampede; under very high loads, multiple processes try to
warm the cache at the same time, and performance takes a megahit.</p>

<p>This book seems to suggest a more general phenomenon of the system
taking a hit if lots of people are trying to acquire a lock. Probably
because they&#8217;re all spin locks, and spin lock take processing time.
Couldn&#8217;t we just implement a semaphore w BLPOP and LPUSH?</p>

<h2>Horrible App Store Deploy BS</h2>

<p>http://stackoverflow.com/a/26511924/914123</p>

<h2>Qualcomm: Mobile Station Modem</h2>

<p>http://en.wikipedia.org/wiki/Qualcomm</p>

<p>MSM: The CPU on Nexus, made by Qualcomm. You see in the android source
code a whole bunch.</p>

<p>What is Qualcomm?</p>

<blockquote><p>Qualcomm Incorporated is an American global semiconductor company that designs and markets wireless telecommunications products and services.</p></blockquote>

<h2>ioctl</h2>

<p>Swiss army knife for special io device files.</p>

<h2>Dalvik</h2>

<p>VM for android.</p>

<h2><code>set -e</code></h2>

<p>Makes it so that the shell terminates after the first unsuccessful
command. You can kill a tmux/terminal pane by just doing</p>

<pre><code>set -e
ls somethingthatdoesnotexist
</code></pre>

<p>Boom.</p>

<h2>Heroku Buildpack</h2>

<p><code>compile</code> gets run and apparently passed the app root path</p>

<pre><code>mkdir -p "$1/bin/server"
cp "bin/nginx-$STACK" "$1/bin/server/nginx"
</code></pre>

<h2>Nginx logging</h2>

<p>Customize via <code>error_log</code> and <code>access_log</code> directives, but keep in mind
it&#8217;s going to expect to use <code>./logs/*.log</code> no matter what before it&#8217;s
had time to read your config file (how else would it log a failure to
parse a config file?).</p>

<h2>Render right from config/routes.rb</h2>

<pre><code>get '/horse_ass', :to =&gt; proc { |env|
                                    [
                                      200,
                                      {"Content-Type" =&gt; 'text/plain'},
                                      ["FUDGLES"]
                                    ]
                                  }
</code></pre>

<p>It&#8217;s just the Rack API (anything that <code>respond_to?(:call)</code>).</p>

<h2>Millinery</h2>

<p>Women&#8217;s hats. A Milliner is a person who sells hats.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis the slack-jawed yokel]]></title>
    <link href="http://machty.github.com/blog/2014/11/17/redis-the-slack-jawed-yokel/"/>
    <updated>2014-11-17T08:02:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/17/redis-the-slack-jawed-yokel</id>
    <content type="html"><![CDATA[<p>Some stores&#8217;ll never page to disk, but then again, some stores&#8217;ll, like
Redis, the slack-jawed yokel.</p>

<h2>Transparent Huge Pages</h2>

<p>https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/s-memory-transhuge.html</p>

<p>Huge pages are a Linux feature whereby pages, normally 4096 bytes,
can be 2MB or 1 GB. Useful for some applications, difficult to
configure, hence Transparent Huge Pages is provided automagically, I
guess. But it&#8217;s not recommended for database workloads&#8230; why not?</p>

<p>Probably Redis has the answer:</p>

<p>http://redis.io/topics/latency</p>

<p>Redis uses forking for</p>

<ul>
<li>generating RDB snapshot</li>
<li>rewriting AOF</li>
</ul>


<p>Forking is slower in certain virtual settings; EC2 old Xen instances
could take more than a second to fork. Newer ones using HVM
(hardware virtual machine) leverages assistive hardware to make this
efficient.</p>

<p>If you have transparent hugepages, they&#8217;ll need to be COW&#8217;d upon fork,
which is expensive. So you can disable them entirely:</p>

<pre><code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
</code></pre>

<h2>Hypervisor / Virtual Machine Monitor</h2>

<p>http://en.wikipedia.org/wiki/Hypervisor</p>

<p>Tis what it sounds like. It&#8217;s the thing that schedules processing time
b/w multiple virtual machines. It&#8217;s the host to the guest virtual
machines.</p>

<h2>Multiple values in sysfs</h2>

<p>http://techoverflow.net/blog/2013/08/01/how-to-check-if-hugepages-are-enabled-in-linux/</p>

<pre><code>$ cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
</code></pre>

<p>I thought this meant the first value (active) was the current value and
the other two were left as documentation, but no, it means madvise is
current value among the other two options. Kinda wacky. Who parses this?</p>

<p>IRC tells me this is a common sysfs pattern.</p>

<pre><code>$ cat /sys/block/xvda/queue/scheduler
[noop]
</code></pre>

<h2>SSR</h2>

<p>Server-side render.</p>

<h2>InfoSec</h2>

<p>Information Security. Usually refers to IT security, but sometimes
physical backups play into it, such as off-site tape backups.</p>

<h2>Which process is using which port?</h2>

<pre><code>lsof -i $PORT
# e.g.
lsof -i :5000
</code></pre>

<p><code>-i</code> just means filter by internet address. The above supplied only a
port, you could also provide a host name.</p>

<h2>Redis Pub Sub</h2>

<p>Not super common because</p>

<ul>
<li>older versions might crash system / OS if clients don&#8217;t read published
messages fast enough, back pressure, bloating outgoing buffer. New
versions disconnect slow readers though.</li>
<li>Unreliable message delivery if there&#8217;s an intermittent disconnect and
reconnect.</li>
</ul>


<h2>Redis Expiration</h2>

<ul>
<li>you can manually DEL or use expiration</li>
<li>expires whole keys, i.e. can&#8217;t expire individual items in a set (hence
more common to use ZSETs with timestamps and manual deletion based on
a range)</li>
</ul>


<h2>Stripe Token Retrieval</h2>

<p>https://stripe.com/docs/api#retrieve_token</p>

<p>This is only interesting since it clarified that a token wraps a card
with details that you can access before you, say, add it to a card.
This is the only way presently to prevent attaching the same card
to a customer twice: retrieve and then add.</p>

<p>But for my purposes I can just add it twice, and check the fingerprint
after word to prevent duplicates on my app&#8217;s Customer obj.</p>

<p>Why doesn&#8217;t Stripe de-dup? https://groups.google.com/a/lists.stripe.com/forum/#!topic/api-discuss/OzmhpQOs_SU</p>

<blockquote><p>We do not check, as many people have their own policies around when
duplicates are acceptable or aren&#8217;t.
However, you can look at the &#8220;fingerprint&#8221; property on cards that you
have saved in order to dedup.</p></blockquote>

<h2>Ruby Enumerators are awesome</h2>

<p>&#8220;Hey how come there&#8217;s <code>each_with_index</code> but not <code>map_with_index</code>?&#8221;</p>

<p>Use enumerators. Lots of the Ruby Enumerables return enumerators if you
don&#8217;t pass a block to them, and they can be chained. For instance you
can create a 1-indexed series via:</p>

<pre><code>10.times.map { |i| i+1 }
</code></pre>

<p>Does this work with lazy enumerators?</p>

<pre><code>(0..Float::INFINITY).lazy.map { |i| i + 1 }.first(10)
</code></pre>

<p>So yeah, just that ranges have no <code>times</code>&#8230; probably more correct to
think of <code>10.times</code> as a range rather than other way around.</p>

<h2>Stripe Checkout is only Web</h2>

<p>There&#8217;s no native ios/android version of Stripe Checkout. It&#8217;s just web.
iOS/Android SDKs offer token exchange utilities and some widgets but
it&#8217;s not the full tailored UI solution that Checkout is.</p>

<h2>Stripe passes through failed CVVs</h2>

<p>https://support.stripe.com/questions/cvc-or-avs-failed-but-payment-succeeded</p>

<p>You can still process a card with the wrong CVV. Though you probably
shouldn&#8217;t.</p>

<p>Test:</p>

<blockquote><p>4000000000000101: cvc_check will fail.</p></blockquote>

<h2>Rx state machine</h2>

<p>https://github.com/logicalguess/rx-state-machine</p>

<p>Basically every transition that occurs maps to a new state machine. You
define a state machine in a classic OO manner but it streamifies it for
you.</p>

<h2>Write amplification w SSDs</h2>

<p>http://en.wikipedia.org/wiki/Write_amplification</p>

<p>When the amount you write to an SSD is amplified by the fact that you
often have to flash much more than you originally intended to write
based on how SSD works. So you wanna be careful about abusing <code>fsync</code>.
Ramdisks might help too.</p>

<h2>MySQL installation n00bness</h2>

<p>There&#8217;s <code>mysql</code> and <code>mysql-server</code>. They&#8217;re separate packages. Derp.
One can connect to a server and issue commands, one actually installs
the database.</p>

<pre><code>/usr/bin/mysqladmin -u root password 'new-password'
</code></pre>

<p><code>mysqld_safe</code> is a script for starting up the daemon (as opposed to
<code>service mysqld start</code>?).</p>

<pre><code>cd /usr ; /usr/bin/mysqld_safe &amp;
</code></pre>

<p>Apparently it&#8217;s helpful because it:</p>

<ul>
<li>restarts the server upon error</li>
<li>runtime logging</li>
</ul>


<p>Apparently MySQL on Linux stores the DB files in</p>

<pre><code>/var/lib/mysql/
</code></pre>

<p>There&#8217;s a secure install script that does a few things:</p>

<pre><code>sudo /usr/bin/mysql_secure_installation
</code></pre>

<ol>
<li>Encourages you to set root password</li>
<li>Require <code>root</code> login must come from <code>localhost</code>; disable remote
logins.</li>
<li>Delete test database</li>
<li>Reload perms table.</li>
</ol>


<h2>PHP FPM</h2>

<p>http://php-fpm.org/</p>

<p>A simple and robust FastCGI Process Manager for PHP</p>

<p>I think this is necessary if you do nginx + php (apache has built in
niceties w php).</p>

<p>I already wrote about this shit and forgot it again:</p>

<blockquote><p>CGI applications are processes spun up by a web server to handle an
incoming request. Unscalable since spinning up processes all the time
takes a toll on the OS, not to mention that there&#8217;s no way to do
resource sharing (DB connection sharing, in-memory caching (because
the process dies at the end of request)).</p>

<p>With FastCGI, there&#8217;s a persisting FastCGI server that owns all of the
CGI programs, and webservers interact with FastCGI via a binary protocol
(over a socket (local) or TCP connection (remote)).</p></blockquote>

<p>So FPM is the thing that stays up and running, and may spawn PHP
instances, but can do the connection sharing.</p>

<h2>WordPress FPM</h2>

<p>http://codingsteps.com/install-php-fpm-nginx-mysql-on-ec2-with-amazon-linux-ami/</p>

<pre><code>location / {
    root   /var/www/html;
    index  index.php index.html index.htm;
}
location ~ \.php$ {
      fastcgi_pass   unix:/var/run/php-fpm/php-fpm.sock;
      fastcgi_index  index.php;
      fastcgi_param  SCRIPT_FILENAME  /usr/share/nginx/
                       html$fastcgi_script_name;
      include        fastcgi_params;
}
</code></pre>

<p>Why location and not server? Isn&#8217;t this gonna be required within
nginx.conf? I guess server means a proxy.</p>

<p>I&#8217;m guessing it&#8217;s expecting this include to be within a server
directive. Which I don&#8217;t want. I want simultaneous blogs, yo.</p>

<p>AH OK figured it out:</p>

<ul>
<li><code>server</code> doesn&#8217;t just mean a proxy; it means nginx will spin up such a
server, bind to the ports, and then proxy or serve the thing itself or
pass to FastCGI, however you&#8217;ve configured it. <code>server</code> must be
within the <code>http</code> context.</li>
<li><code>location</code> must be within <code>server</code> or nested within <code>location</code>
context.</li>
</ul>


<p>So we have</p>

<pre><code>location / { ...
</code></pre>

<p>and</p>

<pre><code>location ~ \.php$ { ... 
</code></pre>

<p>So the first matches root. The second case-insensitive matches anything
ending in .php.</p>

<h2>Nginx FastCGI</h2>

<p>FastCGI is a binary protocol. Nginx implements that protocol. Just in
the same way nginx can match a URL and proxy through to an underlying
server, nginx can match a URL and proxy through to a FastCGI server.</p>

<p>There might be multiple FastCGI-speaking servers that nginx might talk
to. PHP-FPM is an alternative implementation over the default FastCGI.</p>

<p>Nginx won&#8217;t spin up a fastCGI server (in the same way it won&#8217;t spin up a
proxy server that it proxies requests to), but rather expects it to be
already running and answering requests from a unix socket or internet socket.</p>

<p>A fast-cgi process is a process manager. It might spin up 8 worker
instances of, say, php, and reuse these instances efficiently. It might
feature adaptive process spawning, like PHP-FPM does, or it might just
block one request if N+1 requests arrive at the same time. Either way,
it&#8217;s better than:</p>

<ol>
<li>Constant process starting/stopping that plain of CGI entails</li>
<li>Building PHP into apache (<code>mod_php</code>), which means you can&#8217;t restart
PHP (after, say, an upgrade) without restarting Apache. Also, you
lose permissions granularity if it&#8217;s built into Apache, which opens
security holes that could be closed by letting PHP run at a different
uid/group, etc.</li>
</ol>


<p>So what&#8217;s PHP FPM?</p>

<h2>Nginx index causing internal redirect</h2>

<p>http://nginx.org/en/docs/http/ngx_http_index_module.html#index</p>

<p>It should be noted that using an index file causes an internal redirect, and the request can be processed in a different location. For example, with the following configuration:</p>

<p>Interesting. That&#8217;s how you can make a <code>/</code> behave like a PHP and be
processed like a PHP.</p>

<h2>Configure PHP-FPM to create unix domain socket with nginx owner/user</h2>

<p>http://stackoverflow.com/questions/23443398/nginx-error-connect-to-php5-fpm-sock-failed-13-permission-denied</p>

<p>You can configure php-fpm to create a unix domain socket and chown it
to a different user/group. Since nginx workers need to talk to it, it
should be configured to their user/group, which was <code>nginx</code> for me.</p>

<p>Now I&#8217;m getting &#8220;No input file specified.&#8221;</p>

<p>LONG STORY SHORT I was pointing to home/some-user/sites/wordpress and
unless you&#8217;re root, that&#8217;s inaccessible.</p>

<h2>PHP config</h2>

<p>https://www.digitalocean.com/community/tutorials/how-to-install-linux-nginx-mysql-php-lemp-stack-on-ubuntu-12-04</p>

<p>They want me to change <code>cgi.fix_pathinfo</code> to 0, else it will be fuzzy
match a php file for processing, a potential security risk.</p>

<p>Then I have to change PHP FPM to accept requests off a unix domain
socket:</p>

<pre><code>listen = /var/run/php5-fpm.sock
</code></pre>

<p>Makes sense, otherwise it assumes the fast cgi server is localhost 9000.</p>

<p>So&#8230; does nginx spin up a FastCGI server? Is it part of nginx? Is it
just a protocol?</p>

<h2>System V Services</h2>

<p>You don&#8217;t directly start mysqld, you do</p>

<pre><code>service mysqld start
</code></pre>

<h2>CentOS</h2>

<p>http://en.wikipedia.org/wiki/CentOS</p>

<p>Largely a clone of Red Hat Enterprise Linux.</p>

<p>http://unix.stackexchange.com/questions/27323/is-centos-exactly-the-same-as-rhel</p>

<blockquote><p>CentOS is very close to being RHEL without the branding and support. In particular, the library versions are the same, so binaries that work on one will work on the other. The administration tools are the same and configured in similar ways. However, there are a few differences, as the two distributions sometimes apply different minor patches. For example, in this question, it was apparent that RHEL 5 and CentOS 5 apply different rules to identify files under /etc/cron.d.</p>

<p>In other words, at the level of your course, you can treat CentOS and RHEL as interchangeable. But if you needed to look up the precise behavior of a program in a corner of the man page, you may encounter differences.</p></blockquote>

<h2>Alice in Flames</h2>

<p>http://techblog.netflix.com/2014/11/nodejs-in-flames.html</p>

<p>Flame chart things I didn&#8217;t know:</p>

<ul>
<li>X axis isn&#8217;t necessarily passage of time; I think it is in Chrome but
doesn&#8217;t need to be for purposes of flame chart?</li>
<li>Width of a box is aggregate call time;</li>
</ul>


<p>Ehh I guess flame charts do different things? Seems that chrome X axis
is in order. But it doesn&#8217;t need to be, in the same way that the tree
view of calls isn&#8217;t in time order; you care about total elapsed time,
not passage of time.</p>

<h2>Heroku one-off dynos</h2>

<p>https://devcenter.heroku.com/articles/one-off-dynos</p>

<p>TL;dr heroku run console (and any other run) will use a one-off dyno.</p>

<h2>Sampling vs Tracing</h2>

<p>https://www.jetbrains.com/profiler/webhelp/Profiling_Guidelines__Choosing_the_Right_Profiling_Mode.html</p>

<p>Sampling vs Tracing.</p>

<p>TL;DR tracing is more expensive, affects perf, but is more accurate than
sampling.</p>

<h2>Wordpress Admin on SSL</h2>

<p>http://codex.wordpress.org/Administration_Over_SSL</p>

<p>If you&#8217;re reverse-proxying, you have to prevent loops via:</p>

<pre><code>define('FORCE_SSL_ADMIN', true);
if ($_SERVER['HTTP_X_FORWARDED_PROTO'] == 'https')
       $_SERVER['HTTPS']='on';
</code></pre>

<p>Remember that wp-config.php is going to be loaded on every stupid thing.
Right?</p>

<h2>Heroku buildpacks</h2>

<p>Integration-tested via <a href="https://github.com/heroku/hatchet">Hatchet</a>.</p>

<p>https://devcenter.heroku.com/articles/buildpack-api</p>

<ul>
<li>detect: determines whether to apply buildpack to app</li>
<li>compile: apply the transformations</li>
<li>release: provides metadata back to runtime&#8230;?</li>
</ul>


<p>If you want both a node and ruby setup, you can do</p>

<pre><code>https://github.com/ddollar/heroku-buildpack-multi
</code></pre>

<p>and point to the default heroku-provided buildpacks within a .buildpacks
file.</p>

<h2>nginx-buildpack</h2>

<p>https://github.com/ryandotsmith/nginx-buildpack/blob/master/bin/start-nginx</p>

<p>This is a pretty awesome file that uses lots of Linux-y IPC trickery to
get</p>

<p>You must put the following in your Procfile:</p>

<pre><code>web: bin/start-nginx bundle exec unicorn -c config/unicorn.rb
</code></pre>

<p>The start-nginx script spins up processes in the background; nginx
will wait for</p>

<h2>App server</h2>

<p>Unicorn, Puma, Rainbows, zbatery, etc. It&#8217;s the thing that listens to a
socket and runs your application code for you. Rails isn&#8217;t an app
server, but Puma running a rails app is. Rails is an app framework. Puma is
an app server.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis B Hayes]]></title>
    <link href="http://machty.github.com/blog/2014/11/15/redis-b-hayes/"/>
    <updated>2014-11-15T13:16:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/15/redis-b-hayes</id>
    <content type="html"><![CDATA[<h2>Redis&#8217;n</h2>

<p>This and other notes based on Redis in Action.</p>

<p>Questions:</p>

<ul>
<li>is it really brag-worthy to say in-memory-store? It gets persisted to
disk anyway; don&#8217;t other DBs bring as much into memory as possible for
fast lookups?

<ul>
<li>A: other databases are &#8220;primarily on-disk&#8221; but of course yes memory
caching exists.</li>
</ul>
</li>
</ul>


<p>Relation to memcached:</p>

<ul>
<li>Similar efficiency based on in-memory lookups</li>
<li>Redis features 2 persistence strategies (memcached doesn&#8217;t persist I
guess?)</li>
<li>Redis supports strings + 4 other data structures; memcached is strings</li>
</ul>


<p>ZSETs are hash of string keys to floating points, can be queried by
order, ordered by their weight.</p>

<h2>Databases: row insertion (often) fast</h2>

<p>Because no need for a random read + random write; appending to a file
(what most DBs do) is fast.</p>

<h2>ACID</h2>

<p>Set of properties that guarantee reliable database transactions</p>

<ul>
<li>Atomicity - all or nothing</li>
<li>Consistency - transactions bring database from one valid state to another</li>
<li>Isolation - are partially completed transaction visible to others?</li>
<li>Durability - post-transaction, data is committed even in power loss</li>
</ul>


<h2>Fortnight</h2>

<p>Two weeks.</p>

<h2>Heroku database URL</h2>

<pre><code>postgres://username:password@ec2-xx-xx-xx-xx-xx.compute-1.amazonaws.com:5432/d45d81ucgm3205
</code></pre>

<p>It&#8217;s just username + password at some publicly accessible EC2 URL. Your
cherished postgres instances just live on some EC2 farm. What a crock.</p>

<h2>Docker</h2>

<p>Docker images are read-only templates. Use them to generate containers.
Build other shit on top of containers.</p>

<p>Docker has its own IANA port numbers for REST and secure REST API&#8230;
what does this actually mean?</p>

<p>http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=docker#DOCKER</p>

<p>AH OK I have some more ideas:</p>

<p>Boot2Docker is what you use to run Docker on Mac OS X. Why? Because
docker depends on linux kernel specifics. Boot2Docker runs a Virtual Box
instance of some Linux-y thing</p>

<pre><code># within the virtual instance
$ uname
Linux version 3.16.4-tinycore64 (root@39d2c877bc4f) (gcc version 4.7.2 (Debian 4.7.2-5) ) #1 SMP Thu Oct 23 16:14:24 UTC 2014
</code></pre>

<p>So with the Boot2Docker setup, when you run the <code>docker</code> command on a
Mac terminal, it has to jump through some hoops to run the docker
instance on the Linux virtual box:</p>

<pre><code>Mac `docker` command
Proxy through to docker:tcuser@192.168.59.103
Run `docker` command, which talks to 
docker daemon
</code></pre>

<p>So there&#8217;s no docker daemon on OS X; the only persistent docker thing
you&#8217;ll see is Virtual box instances running the docker virtual instance
that the actual docker daemon lives on:</p>

<pre><code> /Applications/VirtualBox.app/Contents/MacOS/VBoxHeadless --comment boot2docker-vm --startvm 798082c7-01d7-4a4f-89fc-1ebf70bc1a0a --vrde config
 /Applications/VirtualBox.app/Contents/MacOS/VBoxNetDHCP --ip-address 192.168.59.99 --lower-ip 192.168.59.103 --mac-address 08:00:27:93:D3:BC --netmask 255.255.255.0 --network HostInterfaceNetworking-vboxnet0 --trunk-name vboxnet0 --trunk-type netadp --upper-ip 192.168.59.254
</code></pre>

<p>You can ssh into the docker VM box. Either w password or reusing the key
they generate for you when you install boot2docker; put this in
~/.ssh/config</p>

<pre><code>Host docker
  HostName 192.168.59.103
  User docker
  IdentityFile "/Users/machty/.ssh/id_boot2docker"
  IdentitiesOnly yes
</code></pre>

<p>(I realized later that there&#8217;s a convenient command for this:
<code>boot2docker ssh</code>&#8230; whoops!)</p>

<p>So with this config in place I&#8217;m guessing that I can either run
<code>docker version</code> or <code>ssh docker docker version</code> and see the same
thing. BOOYA both produce:</p>

<pre><code>Client version: 1.3.1
Client API version: 1.15
Go version (client): go1.3.3
Git commit (client): 4e9bbfa
OS/Arch (client): linux/amd64
Server version: 1.3.1
Server API version: 1.15
Go version (server): go1.3.3
Git commit (server): 4e9bbfa
</code></pre>

<p>So anyway, you can run commands against a docker image. This spins up a
container, runs the command, and stops the container&#8230; does it delete
the container?</p>

<pre><code>docker ps --help
</code></pre>

<p>Nevermind that I&#8217;ll figure it out later. Let&#8217;s figure out how to get a
Redis container running:</p>

<pre><code>https://registry.hub.docker.com/_/redis/
</code></pre>

<p>Use <code>docker build</code> to build from a Dockerfile, which kinda explains how
it ends up getting mounted to the outside world. So this is how you can
just have a docker instance of a thing that you can run commands
against, even in a Mac OS X setting? I know nothing, Jon Snow.</p>

<p>https://docs.docker.com/examples/running_redis_service/</p>

<pre><code>FROM        ubuntu:12.10
RUN         apt-get update &amp;&amp; apt-get install -y redis-server
EXPOSE      6379
ENTRYPOINT  ["/usr/local/bin/my-dumbass-redis-server"]
</code></pre>

<p>So this starts with the ubuntu:12.10 image, installs redis into the
container created from that image, exposes 6379&#8230; to&#8230;? What does this
mean?</p>

<blockquote><p>The EXPOSE instructions informs Docker that the container will listen on the specified network ports at runtime. Docker uses this information to interconnect containers using links (see the Docker User Guide). Note that EXPOSE only works for inter-container links. It doesn&#8217;t make ports accessible from the host. To expose ports to the host, at runtime, use the -p flag.</p></blockquote>

<p>From http://docs.docker.com/reference/builder/</p>

<p>So it&#8217;s exposed if we&#8217;re linking containers but not someone exposed to
the host app.</p>

<p>Ah I tried</p>

<pre><code>docker run -i --entrypoint=/usr/bin/redis-server dcf91
</code></pre>

<p>and then I couldn&#8217;t ctrl-C because of, something, but if i do</p>

<pre><code>docker run -it --entrypoint=/usr/bin/redis-server dcf91
</code></pre>

<p>then the -t option will attach a pseudo TTY and I can do it. This
stuff is so cray cray. Actually I lied ctrl-C doesn&#8217;t work for TODO
reasons, but Ctrl-P + Ctrl-Q detaches you from it? Seems good.</p>

<p>OK but if I do</p>

<pre><code>docker run -it -p 9191:6379 --entrypoint=/usr/bin/redis-server dcf91
</code></pre>

<p>this will run in interactive mode with ctrl-p ctrl-q supporting
detachability, mapping the container 6379 (default redis port) to the
host port 9191, and overriding the entrypoint because I didn&#8217;t know what
I was doing in the Dockerfile. I should have left it at the default
/usr/bin/redis-server because that&#8217;s where apt-get will put it.</p>

<p>ANYWAY the mapping works, but the problem is i have to SSH into the
docker host VirtualBox first to see it. How do I get beyond the
Boot2Docker wall? Wait I know I&#8217;ll just fuckin use some SSH magic.
Tunnels n shit.</p>

<pre><code>ssh docker -L 9191:localhost:9191
</code></pre>

<p>ah but this will open a login shell, which I don&#8217;t need/want for what
I&#8217;m doing:</p>

<pre><code>ssh docker -N -L 9191:localhost:9191
</code></pre>

<p>The -N stands for &#8220;Do not execute a remote command&#8221;</p>

<p>After which point I could just redis-cli but since I&#8217;m brutally low
level I&#8217;ll do</p>

<pre><code>$ nc localhost 9191
SET WAT WOOT
+OK
GET WAT
$4
WOOT
</code></pre>

<p>Booooooya. So cool.</p>

<p>OK gonna be an idiot. SSH all the way. Is that possible? It means being
able to SSH into container&#8230; sounds like you can&#8217;t do that without
going through</p>

<h2>Docker detach</h2>

<p>Docker attaching/detaching is pretty weird. I don&#8217;t know the rationale
behind it but they make it very easy to attach to a box but then not be
able to attach. Basically you have to always pass -it to run and then
can use ctrl-P ctrl-Q to detach.</p>

<p>Or, you can <code>kill -9</code> the attached processed; if you you just do <code>kill</code>,
that sends SIGTERM and that proxies through and closes the shitty
process, but again Ctrl-C doesn&#8217;t use it.</p>

<h2>Docker forwarding for OS X</h2>

<p>This is a great article</p>

<p>http://viget.com/extend/how-to-use-docker-on-os-x-the-missing-guide</p>

<p>Things learned:</p>

<ul>
<li><code>boot2docker ssh</code></li>
<li>Add <code>dockerhost</code> to /etc/hosts</li>
<li>Use <code>nsenter</code>

<ul>
<li><code>sudo nsenter -m -u -n -i -p -t $PID</code>

<ul>
<li><code>-m</code> use mount namespace of target process</li>
<li><code>-u</code> use UTS namespace of target process (UTS stands for time-sharing? legacy unix thing?)</li>
<li><code>-n</code> use network namespace of target process</li>
<li><code>-i</code> IPC</li>
<li><code>-i</code> IPC</li>
<li><code>-p</code> IPC</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>LXC Containers (vs Docker)</h2>

<p>https://linuxcontainers.org/</p>

<p>Better chroot, comparable to docker containers. Ways of containing
processes, resources, etc., dependent on modern linux kernel features,
mostly on process namespacing.</p>

<p>Excellent article comparing to Docker: http://www.flockport.com/lxc-vs-docker/</p>

<p>Things learned:</p>

<ul>
<li>yuno virtualization?

<ul>
<li>because of performance cost</li>
</ul>
</li>
<li>LXC and Docker are fast</li>
<li>Lightweight VMs</li>
<li>Is Docker a lightweight VM?</li>
</ul>


<p>A Docker container runs a single process:</p>

<blockquote><p>Docker restricts the container to a single process only. The default docker baseimage OS template is not designed to support multiple applications, processes or services like init, cron, syslog, ssh etc. As you can imagine this introduces a certain amount of complexity and has huge implications for day to day usage scenarios. Since current architectures, applications and services are designed to operate in normal multi process OS environments you would need to find a Docker way to do things or use tools that support Docker. When it comes to applications for a LAMP container you would need to build 3 containers that consume services from each other, a PHP container, an Apache container and a MySQL container. Can you build all 3 in one container? You can, but there is no way to run php-fpm, apache and mysqld in the same container without a shell script or install a separate process manager like runit or supervisor.</p></blockquote>

<p>http://docs.docker.com/articles/using_supervisord/</p>

<blockquote><p>Traditionally a Docker container runs a single process when it is launched, for example an Apache daemon or a SSH server daemon.</p></blockquote>

<p>This is the entry point. Note that <code>ubuntu</code> has no entry point. Not sure
if it&#8217;s possible to use <code>run</code> with a container that has an entry point,
since the entry point is the process that gets run.</p>

<p>So it should be possible for me to run an ubuntu netcat and portforward
at the same time:</p>

<p>All from Mac:</p>

<pre><code>$ docker run -i -p 9292:9292 --expose=[9292] ubuntu:14.04 nc -l 0.0.0.0
</code></pre>

<p>9292</p>

<p>Separate Mac terminal window:</p>

<pre><code>$ nc dockerhost 9292
</code></pre>

<p>And now these two assholes talk to each other!</p>

<p>Here are all the options I used</p>

<ul>
<li><code>-i</code>: run container interactively with terminal attached; without
this, nc immediately closes once someone connects to it since STDIN is
presumably dev null</li>
<li><code>-p 9292:9292</code>, map docker host port 9292 to container port 9292</li>
<li><code>--expose=[9292]</code> open the firewall since it wasn&#8217;t listed as exposed
in the dockerfile</li>
</ul>


<p>Shit is SO COOL.</p>

<h2>Docker vs Heroku</h2>

<p>Hmm, not even worth comparing.</p>

<h2>Nested SSH tunnels</h2>

<p>http://superuser.com/questions/96489/ssh-tunnel-via-multiple-hops</p>

<p>Map localhost:9998 to host2&#8217;s port 22.</p>

<pre><code>ssh -L 9998:host2:22 -N host1
</code></pre>

<p>Map localhost:9999 to blahbalhbablh you get the picture.</p>

<pre><code>ssh -L 9999:localhost:1234 -N -p 9998 localhost
</code></pre>

<p>Shit is sooooo crazy. I love this stuff.</p>

<h2>ProxyCommand</h2>

<p>From <code>SSH_CONFIG(5)</code>.</p>

<pre><code> ProxyCommand
         Specifies the command to use to connect to the server.  The command string
         extends to the end of the line, and is executed with the user's shell.  In the
         command string, any occurrence of `%h' will be substituted by the host name to
         connect, `%p' by the port, and `%r' by the remote user name.  The command can be
         basically anything, and should read from its standard input and write to its
         standard output.  It should eventually connect an sshd(8) server running on some
         machine, or execute sshd -i somewhere.  Host key management will be done using
         the HostName of the host being connected (defaulting to the name typed by the
         user).  Setting the command to ``none'' disables this option entirely.  Note
         that CheckHostIP is not available for connects with a proxy command.

         This directive is useful in conjunction with nc(1) and its proxy support.  For
         example, the following directive would connect via an HTTP proxy at 192.0.2.0:

            ProxyCommand /usr/bin/nc -X connect -x 192.0.2.0:8080 %h %p
</code></pre>

<p>So this proxies through an already established HTTP Connect proxy at
192.0.2.0:8080. That&#8217;s so awesome.</p>

<p>netcat even brags of this:</p>

<pre><code> Common uses include:

       o   simple TCP proxies
       o   shell-script based HTTP clients and servers
       o   network daemon testing
       o   a SOCKS or HTTP ProxyCommand for ssh(1)
       o   and much, much more
</code></pre>

<h2>get.docker.com</h2>

<pre><code>curl https://get.docker.com
</code></pre>

<p>It returns a bootstrapping shell script for setting up docker.</p>

<p>You can <code>#include</code> it when booting an EC2 instance. Pretty cool.</p>

<h2>Basic Authentication</h2>

<p>If I spin up a stupid netcat server</p>

<pre><code>nc -l localhost 9191
</code></pre>

<p>and then query it from Chrome</p>

<pre><code>http://user:password@localhost:9191
</code></pre>

<p>Here&#8217;s what I see:</p>

<pre><code>GET / HTTP/1.1
Host: localhost:9191
Connection: keep-alive
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 Safari/537.36
Accept-Encoding: gzip,deflate,sdch
Accept-Language: en-US,en;q=0.8
Cookie: blahblah
</code></pre>

<p>No reference to <code>user</code> or <code>password</code>. Which means that this information
isn&#8217;t sent up front unless the server requests basic authentication by
first sending back a 401 with the <code>WWW-Authenticate</code> header, after which
point the user and password will be sent.</p>

<p>And because it&#8217;s sent as a header (user:pass in base64) it&#8217;s encrypted
if sent over TLS. But it&#8217;ll be plaintext in your address bar :).</p>

<h2>SSH randomart</h2>

<p>You see it when you create a key pair. Why? Because it&#8217;s just an easy
ass visual way to compare keys rather than some Base64 shit.</p>

<p>You can see the randomart for an existing key by printing the
fingerprint in verbose mode:</p>

<pre><code>$ ssh-keygen -lv -f ~/.ssh/id_boot2docker.pub
2048 b2:3e:e4:d3:c1:9d:1b:75:46:0b:53:aa:18:6b:c7:c6  machty@machty.home (RSA)
+--[ RSA 2048]----+
|             ..  |
|            o..  |
|        .   .+ . |
|         * .. +  |
|      ..S.Eo o   |
|      .+oo+      |
|     o.. . o     |
|     .+ . .      |
|      .o         |
+-----------------+
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shananananananananeeees]]></title>
    <link href="http://machty.github.com/blog/2014/11/11/shananananananananeeees/"/>
    <updated>2014-11-11T06:43:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/11/shananananananananeeees</id>
    <content type="html"><![CDATA[<h2>Linux hierachy layout</h2>

<p><code>man hier</code> answers these questions.</p>

<p>Basically, <code>/usr</code> is a secondary hierarchy for libs/bins/other things
that aren&#8217;t strictly required for single-user mode (e.g. root).</p>

<p><code>/usr/local</code> is tertiary. Things you compile yourself might belong
there. Typically implies lower level of permissions. Rationale for
separting from <code>/usr</code> is that <code>/usr</code> might be some read-only thing
mounted and shared across machines, ready to be swapped out and upgraded
at any time, but <code>/usr/local</code> is crap that you can fuck wit.</p>

<p><code>brew</code> expects to install in the tertiary <code>/usr/local</code> directory:
<code>/usr/local/Cellar</code>. So you can have multiple versions of executables
installed via brew, but only one wins on the command line by way of
symlinks within <code>/usr/local/bin</code> pointing to specific executables in
<code>/usr/local/Cellar/projectname/1.23/bleh</code>. Symlinks to the rescue.</p>

<pre><code>$ sudo brew install wat
Error: Cowardly refusing to `sudo brew install`
You can use brew with sudo, but only if the brew executable is owned by root.
However, this is both not recommended and completely unsupported so do so at
your own risk.
</code></pre>

<p>I guess this is nice since it prevents all the ugliness of installing
shared executables at root privileges when they&#8217;re not needed.</p>

<h2>Raptor</h2>

<p>Ruby server. Apparently fast.</p>

<p>http://www.rubyraptor.org/how-we-made-raptor-up-to-4x-faster-than-unicorn-and-up-to-2x-faster-than-puma-torquebox/#zero_copy</p>

<ul>
<li>Uses nginx HTTP parser

<ul>
<li>due to battle-tested reliability</li>
<li>could have used PicoHTTPParser, but not much community adoption
though it claims being faster than nginc</li>
<li>could have used Mongrel&#8217;s Ragel HTTP parser, but lots of
Ruby-specific</li>
</ul>
</li>
<li>Comes w reverse proxy buffer, such as what nginx has but hyper
optimized to typical ruby raptor workflows</li>
<li>Multi-process</li>
<li>Sounds like multi-threadedness will be a paid solution that comes
later.</li>
</ul>


<p>The C++ component of Raptor is the server that consists of:</p>

<ul>
<li>Buffering reverse proxy</li>
<li>HTTP parser</li>
<li>HTTP server</li>
</ul>


<p>Apparently these are all part of the same thing.</p>

<h2>Puma Lopez mode</h2>

<p>Puma Ruby server comes with a Lopez mode named after <code>@brianmario</code> who
suggested it. It&#8217;s a tcp-only (no http) version of puma. To that guy&#8217;s
knowledge Puma is the only threaded/pre-forking Ruby server that offers
such a mode.</p>

<h2>nginx Reverse proxy buffer</h2>

<p>http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering</p>

<p>I think it&#8217;s on by default, but you can disable it, configure buffer
sizes, or enable with <code>X-Accel-Buffer</code>.</p>

<h2>nginx in general</h2>

<ul>
<li>master process

<ul>
<li>manages worker process, reloads config files, etc</li>
</ul>
</li>
<li>worker processes

<ul>
<li>process requests</li>
</ul>
</li>
</ul>


<p>conf files are directives, either one liners or blocks.</p>

<ul>
<li>Block directives

<ul>
<li>same structure as normal directives</li>
<li>but have braces</li>
<li>if braces allow directives inside of them, it&#8217;s called a context</li>
</ul>
</li>
</ul>


<p>Contexts:</p>

<ul>
<li>main

<ul>
<li>events</li>
<li>http

<ul>
<li>server

<ul>
<li>location</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>Catch up or move along</h2>

<p>http://unlearningeconomics.wordpress.com/2012/04/03/the-keenkrugman-debate-a-summary/</p>

<h2>30s write delay</h2>

<p>http://oldblog.antirez.com/post/redis-persistence-demystified.html</p>

<p>http://stackoverflow.com/questions/13650134/after-how-many-seconds-are-file-system-write-buffers-typically-flushed</p>

<p>http://brad.livejournal.com/2116715.html</p>

<p>Modified kernel buffers (of files on disk) wait up to 30s to be flushed
to disk. You can &#8220;force&#8221; it was fsync. But then your disks might lie to
you about what actually was persisted (verifiable via a test involving
pulling the power cord).</p>

<h2>Redis persistence</h2>

<p>http://oldblog.antirez.com/post/redis-persistence-demystified.html</p>

<p>It&#8217;s in-memory key-value, so does it ever get saved to disk?</p>

<p>Why yes it does, via:</p>

<ul>
<li>snapshotting; configure min writes since last sync, or a timeout, and
it&#8217;ll persist a snapshot to a .rdb file. Half completed transactions
(via MULTI/EXEC) don&#8217;t show up of course.</li>
<li>append-only AOF files

<ul>
<li>get rewritten based on memory contents if file grows too large</li>
<li>possible for an empty redis db (written and then deleted keys) has a
large AOF file.</li>
</ul>
</li>
</ul>


<p>Both can be enabled; it&#8217;s nice to have rdb files that you can back up.</p>

<blockquote><p>AOF rewrites are generated only using sequential I/O operations, so the whole dump process is efficient even with rotational disks (no random I/O is performed). This is also true for RDB snapshots generation. The complete lack of Random I/O accesses is a rare feature among databases, and is possible mostly because Redis serves read operations from memory, so data on disk does not need to be organized for a random access pattern, but just for a sequential loading on restart.</p></blockquote>

<p>Interesting, so usually a database that stores to disk would need to
organize data for efficient random access, but in Redis-land, everything
is loaded into memory.</p>

<p>This confused me:</p>

<blockquote><p>One of the additional benefits of RDB is the fact for a given database size, the number of I/Os on the system is bound, whatever the activity on the database is. This is a property that most traditional database systems (and the Redis other persistence, the AOF) do not have.</p></blockquote>

<p>In other words, AOFs can be large even for empty databases (due to
deletions).</p>

<p>appendfsync:</p>

<ul>
<li>appendfsync no

<ul>
<li>syncs at kernel whim (30 s on linux)</li>
</ul>
</li>
<li>appendfsync everysec

<ul>
<li>average 1 sec, at most 2 delay before buffers sent to kernel and sync&#8217;d</li>
</ul>
</li>
<li>appendfsync always

<ul>
<li>sync before each client ack</li>
<li>slowest</li>
</ul>
</li>
</ul>


<p>Default is <code>appendfsync everysec</code>, which is pretty good durability
without murdering speed.</p>

<blockquote><p>What Redis implements when appendfsync is set to always is usually called group commit. This means that instead of using an fsync call for every write operation performed, Redis is able to group this commits in a single write+fsync operation performed before sending the request to the group of clients that issued a write operation during the latest event loop iteration.</p></blockquote>

<p>Hmm that&#8217;s interesting&#8230; Redis has an event loop that can answer
multiple clients in a single iteration?</p>

<p>http://pauladamsmith.com/articles/redis-under-the-hood.html#event-loop</p>

<p>Ah, it uses epoll and the like; multiple sockets and events can have
arrived in one go, so it loops through all of those, does the necessary
reads, etc.</p>

<pre><code>/* Include the best multiplexing layer supported by this system.
 * The following should be ordered by performances, descending. */
#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
</code></pre>

<h2>UTF-8</h2>

<p>http://en.wikipedia.org/wiki/UTF-8</p>

<p>I can&#8217;t believe I never sat down and read this shit.</p>

<ul>
<li>backwards compat w ASCII since ascii only used the 7 bits (signed
char) to determine character.</li>
<li>81% of webpages use this encoding</li>
<li>ASCII is valid UTF-8</li>
<li>UTF-8 is variable length; the 8th bit determines length</li>
<li>there are invalid byte sequences (that you have to look out for when
reading files / raw shit)</li>
</ul>


<h2>ISO/IEC 8859</h2>

<p>http://en.wikipedia.org/wiki/ISO/IEC_8859</p>

<ul>
<li>single byte</li>
<li>all ascii is ISO</li>
<li>Seems like standard alphabet is preserved, but other 8 bit range stuff
differs.</li>
</ul>


<h2>Ruby string encoding</h2>

<p>http://stackoverflow.com/questions/20521371/set-utf-8-as-default-for-ruby-1-9-3</p>

<ul>
<li>Ruby 1.8 and below didn&#8217;t knew the concept of string encodings at all. Strings were more or less byte arrays.</li>
<li>Ruby 1.9: default string encoding is US_ASCII everywhere.</li>
<li><p>Ruby 2.0 and above: default string encoding is UTF-8.</p>

<p>  $ ruby -e &#8220;puts &#8221;.encoding&#8221;
  UTF-8</p></li>
</ul>


<h2>hiredis</h2>

<p>https://github.com/redis/hiredis</p>

<p>Presumably stands for &#8220;high(ish) level redis lib&#8221;.</p>

<p>The Ruby gem can optionally use this as a driver but it comes at the
expense of portability (JRuby can&#8217;t use this driver). But by default
Ruby just uses Ruby sockets to talk to redis.</p>

<h2>Public wifi</h2>

<p>Is there any security difference between a password-less public wifi and
one in which literally everyone knows the password?</p>

<h2>/private on os X</h2>

<p>http://unix.stackexchange.com/questions/63555/what-is-darwins-private-directory-for</p>

<p>fun fact: <code>/etc</code> is a symlink for <code>/private/etc</code> on OS X. Wacky.</p>

<h2>WEP, WPA, WPA2</h2>

<p>http://www.howtogeek.com/167783/htg-explains-the-difference-between-wep-wpa-and-wpa2-wireless-encryption-and-why-it-matters/</p>

<ul>
<li>WEP (Wired Equivalent Privacy)

<ul>
<li>oldest</li>
<li>WEP 128 most common even though there&#8217;s 256</li>
<li>major security vulnerabilities based on RC4 stream cipher cracking</li>
<li>on busy network, cracking could happen within a minute; if network
is slow, attacker can send fake packets and get replies that it can
use to crack over time.</li>
<li>passive attacks: you have to collect information. Gather shit.</li>
<li>Shamir (from RSA fame) was one of the crackahs.</li>
</ul>
</li>
<li>WPA

<ul>
<li>PSK (pre-shared key) is most common</li>
<li>256 min (over 64 and 128 WEP garbage)</li>
<li>message integrity checks (detects some MITM)</li>
<li>TKIP (temporal key integrity protocol) is predecessor to AES</li>
<li>too tied to WEP (meant for firmware progressive upgrades) and hence
prone to some WEP vulnerabilities, hence:</li>
</ul>
</li>
<li>WPA2

<ul>
<li>AES</li>
<li>CCMP (replacement for TKIP? but with TKIP fallback)</li>
</ul>
</li>
</ul>


<p>Just disable a thing called WPS and you&#8217;ll be good.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mr Noah]]></title>
    <link href="http://machty.github.com/blog/2014/11/08/mr-noah/"/>
    <updated>2014-11-08T08:29:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/08/mr-noah</id>
    <content type="html"><![CDATA[<h2>Root/apex/base/DNS and things finally clicking</h2>

<p>They the same thing.</p>

<p>http://stackoverflow.com/a/16041655/914123</p>

<p>Also I like</p>

<pre><code>(Note: root, base, apex domains are all the same thing. Using interchangeably for google-foo.)
</code></pre>

<p>CNAME:</p>

<ul>
<li>Canonical Name record: Aliases another name</li>
<li>Use case: define CNAME for <code>ftp</code>, <code>www</code>, etc to point to the
<code>example.com</code> A record so that only the A record&#8217;s IP needs to
change</li>
<li>Can point to any ol domain, not just within <code>example.com</code>. Heroku
and other cloud services use this to have a CNAME pointing to
a domain name under the control of a dynamic name server that
can dish out different A name records (hence different IPs)</li>
<li>Can&#8217;t define CNAMEs for apex domains (e.g. <code>example.com</code>).</li>
<li>Can&#8217;t be shared with other records for that name, e.g. MX. CNAME wins
and fucks over the others, I think.</li>
</ul>


<p>A record:</p>

<ul>
<li>Points to an IP address.</li>
<li>Terminates DNS lookups</li>
</ul>


<p>ALIAS/ANAME:</p>

<p>http://blog.dnsimple.com/2011/11/introducing-alias-record/</p>

<p>http://support.dnsimple.com/articles/differences-between-a-cname-alias-url/</p>

<blockquote><p>Before going further into the details, it’s important to know that A
and CNAME records are standard DNS records, whilst ALIAS and URL
records are custom DNS records provided by DNSimple. Both of them
are translated internally into A records to ensure
compatibility with the DNS protocol.</p></blockquote>

<p>Aliases can coexist with other records at that level (so someone asking
for MX gets MX if defined for that name rather than resolving
elsewhere).</p>

<p>Ok, so DNS hosts just returns CNAMEs and A records (and others too), and
DNS hosts like DNSimple and DNS Made Easy can provide custom record
types that ultimately translate to A records. Makes sense.</p>

<p>So what about URL?</p>

<blockquote><p>This type of record uses an HTTP redirect to redirect visitors from a domain to a web site.</p></blockquote>

<p>So the A record returned from a URL record on DNSimple is going to point
to one of DNSimple&#8217;s server IPs. I set one up for
snaggletooth.alexmatchneer.com => http://www.example.com.</p>

<pre><code>$ curl -H "Host: snaggletooth.alexmatchneer.com" 50.31.209.254
&lt;a href="http://www.example.com"&gt;Moved Permanently&lt;/a&gt;
</code></pre>

<p>I also added a URL record for ugly.alexmatchneer.com to point to some
random s3 image and discovered that browsers in fact follow redirects
for images, hence this works:</p>

<pre><code>&lt;img src="http://ugly.alexmatchneer.com"/&gt;
</code></pre>

<p><img src="http://ugly.alexmatchneer.com" alt="" /></p>

<p>DNSimple is also nice enough to append the path to whatever its
forwarding, so <code>ugly.alexmatchneer.com/wat</code> forwards to the destination
specified in the URL record + <code>/wat</code>.</p>

<h2><code>dig</code> recursion</h2>

<p>Either you tell your DNS server to recurse for you, or you do it
yourself.</p>

<p>Name server does it for you (all these options just get rid of crufty
shit):</p>

<pre><code>dig +noall +answer +additional +recurse alexmatchneer.com
alexmatchneer.com.      3544    IN      A       23.235.39.133
</code></pre>

<p>You do it yourself:</p>

<pre><code>dig +noall +answer +additional +norecurse alexmatchneer.com
FWDR-68.FWDR-237.FWDR-161.FWDR-12. 3600 IN A    68.237.161.12
FWDR-71.FWDR-243.FWDR-0.FWDR-12. 3600 IN A      71.243.0.12
</code></pre>

<p>FWDR is a FiOS thing. These are from the additional section. So I guess
it means I should call them. But at what point do I go through
Verizon&#8217;s DNS? I thought I used like 4.2.2.2?</p>

<pre><code>cat /etc/resolve.conf

#
# Mac OS X Notice
#
# This file is not used by the host name and address resolution
# or the DNS query routing mechanisms used by most processes on
# this Mac OS X system.
#
# This file is automatically generated.
#
domain home
nameserver 192.168.1.1
</code></pre>

<p>I guess DNS is decided when I connect to a router. Ahhh I guess routers
perform DNS? Ok ok ok what if I tell <code>dig</code> which name server to query?</p>

<pre><code>$ dig +noall +answer +additional +norecurse @4.2.2.2 alexmatchneer.com
i.gtld-servers.net.     109080  IN      A       192.43.172.30
k.gtld-servers.net.     109080  IN      A       192.52.178.30
m.gtld-servers.net.     166299  IN      A       192.55.83.30
h.gtld-servers.net.     114566  IN      A       192.54.112.30
b.gtld-servers.net.     133590  IN      A       192.33.14.30
b.gtld-servers.net.     118573  IN      AAAA    2001:503:231d::2:30
a.gtld-servers.net.     159638  IN      A       192.5.6.30
a.gtld-servers.net.     113094  IN      AAAA    2001:503:a83e::2:30
e.gtld-servers.net.     113091  IN      A       192.12.94.30
f.gtld-servers.net.     166299  IN      A       192.35.51.30
j.gtld-servers.net.     166299  IN      A       192.48.79.30
g.gtld-servers.net.     109080  IN      A       192.42.93.30
d.gtld-servers.net.     101447  IN      A       192.31.80.30
l.gtld-servers.net.     136989  IN      A       192.41.162.30
</code></pre>

<p>WORD ok top level domains, makes sense. I bet if I let it recurse for me
it&#8217;ll gimme what I want:</p>

<pre><code>$ dig +noall +answer +additional +recurse @4.2.2.2 alexmatchneer.com
alexmatchneer.com.      3600    IN      A       23.235.46.133
</code></pre>

<p>Word. And if I use my router&#8217;s IP:</p>

<pre><code>dig +noall +answer +additional +norecurse @192.168.1.1 alexmatchneer.com
FWDR-68.FWDR-237.FWDR-161.FWDR-12. 3600 IN A    68.237.161.12
FWDR-71.FWDR-243.FWDR-0.FWDR-12. 3600 IN A      71.243.0.12
</code></pre>

<p>It refers me to some Verizon name server shit. Which is why if I type in
some nonsense domain name, I get redirected to some shitty Verizon
search page. Regardless of whether I&#8217;m in Chrome or in curl:</p>

<pre><code>$ curl oinasiodasd.asdasiodasod.asdoi
&lt;!DOCTYPE ... blah blah http://searchassist.verizon.com/
</code></pre>

<p>So what if I enable VPN? Prediction: my VPN provider will be making
queries on my behalf, presumably not behind some Verizon name server
shit.</p>

<pre><code>$ curl oinasiodasd.asdasiodasod.asdoi
curl: (6) Could not resolve host: oinasiodasd.asdasiodasod.asdoi
</code></pre>

<p>Basically (annoying caching hangover aside) dig will skip the FiOS
forwarding/recursing if I&#8217;m on VPN. All this makes sense. Perfect
sense. COMPLICATED THOUGH JESUS.</p>

<h2>VPNs and private network IPs</h2>

<p>ICANN set aside numbers like 192.168&#8230; and 10&#8230; for private networks.
VPN doesn&#8217;t interfere with that shit because it&#8217;s within that range.
Derp.</p>

<p>Ahhhh that does though that I could still use my router as a DNS, no?</p>

<p>VPN enabled:</p>

<pre><code>$ dig .
;; SERVER: 8.8.4.4#53(8.8.4.4)
</code></pre>

<p>VPN disabled:</p>

<pre><code>$ dig .
;; SERVER: 192.168.1.1#53(192.168.1.1)
</code></pre>

<p>And even w VPN enabled I could still query my Verizon router&#8217;s DNS</p>

<pre><code>$ dig +norecurse @192.168.1.1 .
FWDR-68.FWDR-237.FWDR-161.FWDR-12. 3600 IN A    68.237.161.12
FWDR-71.FWDR-243.FWDR-0.FWDR-12. 3600 IN A      71.243.0.12
</code></pre>

<p>So who the hell decides where I query from?</p>

<h2>DHCP</h2>

<p>Dynamic Host Configuration Protocol.</p>

<p>When you connect to a network, this tells you all sorts of useful
defaults:</p>

<blockquote><p>The DHCP server manages a pool of IP addresses and information about client configuration parameters such as default gateway, domain name, the name servers, and time servers.</p></blockquote>

<p>When you connect to a network, the DHCP broadcasting stuff happens and
you wind up with an IP, bingo bango bongo. When you connect to a network
but can&#8217;t establish an IP, it&#8217;s probably because DHCP hasn&#8217;t finished
yet.</p>

<p>But this is where 192.168.1.1 as a name server comes from; the Verizon
router will use DHCP to tell you to use it. Other routers might do other
things. When I tether to my phone it gives a different DNS.</p>

<h2>TXT Records</h2>

<pre><code>$ dig +short borflex.alexmatchneer.com TXT
"Another dumb thing"
"I am a big dumb ridiculous idiot!"
</code></pre>

<h2>Nested subdomains</h2>

<p>For the <code>alexmatchneer.com</code> domain, I added a CNAME for
<code>e.x.c.alexmatchneer.com</code> to point to expresscheckoutapp.com
and now it just works to nav to http://e.x.c.alexmatchneer.com</p>

<h2>Route 53</h2>

<p>Heyyyy that&#8217;s the port that DNS servers use.</p>

<h2>Rubydns</h2>

<p>https://github.com/ioquatix/rubydns</p>

<p>Pretty cool. You can make your own DNS server. I got mine to tell me I
was an idiot:</p>

<pre><code>$ dig +short @54.165.102.18 barflonkula TXT
"You are a big idiot"
</code></pre>

<h2>SOA Records</h2>

<h2>Pointilism</h2>

<p>Painting with dots. Like that Ferris Bueller painting, or some of Van
Gogh&#8217;s self portraits.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jaily Dournal]]></title>
    <link href="http://machty.github.com/blog/2014/11/06/jaily-dournal/"/>
    <updated>2014-11-06T16:00:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/06/jaily-dournal</id>
    <content type="html"><![CDATA[<h2><code>rails c --sandbox</code></h2>

<p>Rolls back changes on exit.</p>

<pre><code>(0.4ms)  SAVEPOINT active_record_1
(0.2ms)  ROLLBACK TO SAVEPOINT active_record_1
</code></pre>

<p>Wraps in a transaction which is why you see the above rather than BEGIN
statements</p>

<h2>Temporarily change $stdout</h2>

<p>I wanted to get pretty-print output in Ruby. Solution, using <code>rails c</code>:</p>

<pre><code>f = File.open('tmp', 'w')
stdout_old = $stdout
$stdout = f
pp hash
$stdout = $stdout_old # could also do File.open(1)
</code></pre>

<h2>Review: difference between pipes and sockets</h2>

<ul>
<li>pipes came first in early 70s</li>
<li>pipes are always stream oriented; sockets can also be datagram
oriented.</li>
<li>pipes are unidirectional (and require two pipes for back and forth).
sockets are bi-directional.</li>
</ul>


<h2>FAT doesn&#8217;t support hardlinking</h2>

<p>I was going to try ember-cli-ramdisk mounting on file systems other than
HFS+, hoping that one FS would support more efficient reuse of freed
blocks, which might minimize paging on the grounds that if ramdisks
blocks are being reused, then additional ramdisk file allocations won&#8217;t
occur, hence memory lookups won&#8217;t occur, hence paging can&#8217;t happen.</p>

<p>Anyway, got initial builds working in FAT32, but incremental rebuilds
didn&#8217;t work due to hard-linking:</p>

<p>https://github.com/rlivsey/broccoli-concat/blob/master/index.js#L86</p>

<p>I thought we got rid of those, but then again the above use case is
fine, since hardlinks are only used concat output and not root files.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OSS Cards Against Humanity]]></title>
    <link href="http://machty.github.com/blog/2014/11/05/oss-cards-against-humanity/"/>
    <updated>2014-11-05T12:40:00-05:00</updated>
    <id>http://machty.github.com/blog/2014/11/05/oss-cards-against-humanity</id>
    <content type="html"><![CDATA[<p>Why not.</p>

<h2>Black Cards</h2>

<ul>
<li>&#8220;GamerGate: it&#8217;s actually about <code>__________</code>&#8221;</li>
<li>&#8220;Ember.js: a framework for creating <code>_________</code>&#8221;</li>
</ul>


<h2>White Cards</h2>

<ul>
<li>Ethics in video game journalism</li>
<li><h1>GamerGate</h1></li>
<li>Thought Leadership</li>
<li>Brogrammers</li>
<li>Cracking the Nut</li>
<li>Destroy All Software</li>
<li>Ember.js</li>
<li>AngularJS</li>
<li>React</li>
<li>Two-way data-binding</li>
<li>Unidirectional Data Flow</li>
<li>Handlebars templates</li>
<li>Functional reactive programming</li>
<li>Hacker News</li>
<li>Stability without Stagnation</li>
<li>Ambitious Web Applications</li>
<li>Thirsty Randos</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troll toll to get into this Journal]]></title>
    <link href="http://machty.github.com/blog/2014/10/31/troll-toll-to-get-into-this-journal/"/>
    <updated>2014-10-31T06:45:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/31/troll-toll-to-get-into-this-journal</id>
    <content type="html"><![CDATA[<h2>HDIutil</h2>

<p>Trying to make some ramdisks on OS X</p>

<pre><code>hdiutil attach -nomount ram://8388608
</code></pre>

<p><code>hdiutil</code> attach will normally try to attach and mount a file system.</p>

<ul>
<li>attach: map a hardware device to some <code>/dev/wat</code> device file</li>
<li>mount: try to read the device, discover its file system, and mount it
as a directory so that you can actually cd into it, access files, etc</li>
</ul>


<p>So <code>-nomount</code> prevents the second step and just generates a dev file
mapped to ram.</p>

<pre><code>diskutil erasevolume HFS+ 'RAM Disk' /dev/wat
</code></pre>

<h2>What is a volume?</h2>

<p>Seems like a higher-level abstraction over disks and partitions.</p>

<p>http://tldp.org/HOWTO/LVM-HOWTO/whatisvolman.html</p>

<ul>
<li>Move things around more easily</li>
<li>give things better names than <code>/dev/sda</code> / sdb etc</li>
</ul>


<p>Basically let&#8217;s you overlay logical volumes on top of physical drives.
Makes it easy to move space around between various drives, make things
more sane.</p>

<pre><code>    hda1   hdc1      (PV:s on partitions or whole disks)                        
       \   /                                                                    
        \ /                                                                     
       diskvg        (VG)                                                       
       /  |  \                                                                  
      /   |   \                                                                 
  usrlv rootlv varlv (LV:s)
    |      |     |                                                              
 ext2  reiserfs  xfs (filesystems)                                        
</code></pre>

<p>Use LVM2 in Linux land.</p>

<h2>tcpdump</h2>

<p>Ahhh got it to work</p>

<pre><code>$ sudo tcpdump -t 'host machty.com'
$ ping machty.com
IP 192.168.109.90 &gt; pages.github.com: ICMP echo request, id 24129, seq 0, length 64
IP pages.github.com &gt; 192.168.109.90: ICMP echo reply, id 24129, seq 0, length 64
IP 192.168.109.90 &gt; pages.github.com: ICMP echo request, id 24129, seq 1, length 64
IP pages.github.com &gt; 192.168.109.90: ICMP echo reply, id 24129, seq 1, length 64
IP 192.168.109.90 &gt; pages.github.com: ICMP echo request, id 24129, seq 2, length 64
IP pages.github.com &gt; 192.168.109.90: ICMP echo reply, id 24129, seq 2, length 64
</code></pre>

<h2>UDP can broad/multicast</h2>

<p>And TCP cannot?</p>

<h2>Sequenced-packet sockets</h2>

<p>Combined aspects of UDP/TCP.</p>

<ul>
<li>Connection oriented</li>
<li>Datagram oriented (message boundaries preserved)</li>
<li>Reliable</li>
</ul>


<p><code>SOCK_SEQPACKET</code> type, available in UNIX domain. TCP/UDP do not support
it, but SCTP does.</p>

<p>Stream Control Transmission Protocol. Allows stream multiplexing,
preserves message boundaries unlike TCP.</p>

<pre><code>socket(AF_INET, SOCK_STREAM, IPPROTO_SCTP);
</code></pre>

<p>So, TCP isn&#8217;t the only internet-domain stream protocol; SCTP is too.
It&#8217;s a stream of messages. Rather than bytes.</p>

<p>UDP + congestion controller = DCCP, Datagram Congestion Control
Protocol.</p>

<h2>Alternative IO</h2>

<ul>
<li><code>select</code>/<code>poll</code> to watch multiple fds for availability;

<ul>
<li>benefit over alternatives is that you don&#8217;t have to manually poll w non-blocking
reads.</li>
<li>doesn&#8217;t scale well w hundreds+ of FDs</li>
</ul>
</li>
<li>signal-driven IO; kernel notifies when IO ready, process can do other
still til its ready for IO; benefit over select/poll is that it
doesn&#8217;t block</li>
<li>epoll

<ul>
<li>application can monitor many file descriptors</li>
<li>linux specific but BSD has kqueue, and there are others</li>
<li>avoids complexities w signal programming</li>
</ul>
</li>
</ul>


<p>There&#8217;s also POSIX async IO (AIO); perform IO but don&#8217;t block, get
notified later when it goes through. Linux has thread implementation but
might have it on kernel now, who knows.</p>

<p>Since <code>epoll</code> is Linux specific, use a lib that provides a portal
evented layer to you process: use epoll if present, else fall back to
select/pool; <code>libevent</code> is one such lib.</p>

<ul>
<li>libevent

<ul>
<li>libev: high perf event loop based on libevent but without bugs

<ul>
<li>only ran on Unix, so node (which originally use libev) needed a
solution</li>
</ul>
</li>
<li>libuv: abstraction around libev or IOCP (Windows-based IO Completion
Port)

<ul>
<li>used in node</li>
<li>used in rust</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Two types of readiness notifications:</p>

<ul>
<li>level-triggered: fd is actually ready, now, for IO

<ul>
<li>poll/select &amp; epoll</li>
<li>Allows you to repeat the poll operation after a small read; no need
to read everything in a buffer at once</li>
</ul>
</li>
<li>edge-triggered: IO has occurred since last poll (but it&#8217;s possible
someone else already took care of it)

<ul>
<li>signals &amp; epoll</li>
<li>Try and read as much as possible because you have no way of knowing
whether there&#8217;s more data (without a non-blocking fail), usually by
non-blocking reads in a loop until EAGAIN or EWOULDBLOCK.</li>
</ul>
</li>
</ul>


<p>Note that epoll can do both. These constraints impact program design.</p>

<p>Non-blocking IO + edge-triggered notifications is common because there
are so many cases where blocking IO will screw you; better to non-block
in a lop and check if data is actually there.</p>

<p><code>poll</code> is like <code>select</code> but instead of grouping by operation you just
provide a list of FDs objects and say which type of IO you&#8217;re interested
in per object.</p>

<p>Readiness means an operation will not block. It doesn&#8217;t mean data will
transfer as it could mean EOF or error. The only guarantee is
non-blockage.</p>

<p>Downsides w poll/select when large number of FDs</p>

<ul>
<li>Need to initialize large structures to pass to kernel</li>
<li>Kernel needs to loop through all file descriptors</li>
<li>Program must inspect all returned FDs</li>
</ul>


<h2>Stripe</h2>

<p>You can do a one time charge, or you can save card to a customer.</p>

<p>You can create a Card in their API, but you must specify a customer or
recipient.</p>

<p>An OAuth access token acts like a secret API key:</p>

<blockquote><p>To swap this for an access_token, which acts like a secret API key&#8230;</p></blockquote>

<p>https://stripe.com/docs/connect/getting-started</p>

<p>So this error: https://support.stripe.com/questions/connect-publishable-key-error-with-shared-customers</p>

<p>Background: every Stripe API request sends in your app&#8217;s secret key for
authentication.</p>

<pre><code>Stripe.api_key = ENV['YOUR_DUMBASS_STRIPE_SECRET_KEY']
</code></pre>

<p>This makes it send automatically, but you can also override per API
request by providing a second parameter.</p>

<pre><code>charge = Stripe::Charge.create({
  ...
}, some_api_key)
</code></pre>

<p>In this case, the issue was that my implicitly provided app api key was
being provided to the API request to make a charge using a token
generated by the connected account.</p>

<p>I don&#8217;t really know why this is a problem; seems like something you
should be able to do, right? But actually you have to provide that
second parameter as the access token from when the account connected to
your app.</p>

<p>Trickay.</p>

<h2>Who&#8217;s using ports in OS X?</h2>

<pre><code>sudo lsof -nP -iTCP -sTCP:LISTEN
</code></pre>

<p>Figured I&#8217;d be able to use netstat for this, should probably look into
why I can&#8217;t.</p>

<h2>Self pipes</h2>

<p>Because <code>pselect</code> isn&#8217;t widely supported, you can use the self pipe
trick to get rid of race conditions surrounding signal handlers and
select. Recall the common race condition:</p>

<ul>
<li>Install signal handler that sets global flag to true</li>
<li>Run an IO loop that checks this global flag in order to decide whether
to perform some action.</li>
</ul>


<p>But if a signal arrives between these steps then you might start looping
without checking the flag, etc.</p>

<p>So you can self-pipe to get around this:</p>

<ol>
<li>Create a unix domain pipe, non-blocking on both ends to prevent any
sort of queueing/blocking behavior.</li>
<li>Within a signal handler, write a single byte into the pipe; <code>write</code>
is async signal safe, so we&#8217;re cool.</li>
<li>Include the pipe in <code>select</code>, always check if self-pipe is in
<code>readfs</code></li>
</ol>


<p>Unicorn uses a self pipe.</p>

<pre><code># We use SELF_PIPE differently in the master and worker processes:
#
# * The master process never closes or reinitializes this once
# initialized.  Signal handlers in the master process will write to
# it to wake up the master from IO.select in exactly the same manner
# djb describes in http://cr.yp.to/docs/selfpipe.html
#
# * The workers immediately close the pipe they inherit.  See the
# Unicorn::Worker class for the pipe workers use.
</code></pre>

<blockquote><p>Richard Stevens&#8217;s 1992 book &#8220;Advanced programming in the UNIX environment&#8221; says that you can&#8217;t safely mix select() or poll() with SIGCHLD (or other signals). The SIGCHLD might go off while select() is starting, too early to interrupt it, too late to change its timeout.</p></blockquote>

<p>This just means race conditions; anyway, Unicorn doesn&#8217;t mix select with
other FDs; rather it uses it as parent-child process communication; the
master will sleep/block on a select of the self pipe, and only the self
pipe, and will awaken, clear the pipe, and continue onward.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Crowley Girdle]]></title>
    <link href="http://machty.github.com/blog/2014/10/29/crowley-girdle/"/>
    <updated>2014-10-29T23:46:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/29/crowley-girdle</id>
    <content type="html"><![CDATA[<h2>Partial reads/writes for sockets?</h2>

<p>Why/how?</p>

<ul>
<li>reads: fewer bytes in the socket buffer than requested</li>
<li>writes: insufficient buffer space to transfer all requested bytes and

<ul>
<li>interrupt by signal handler (not just a signal, but a
custom-specified signal handler)</li>
<li>NONBLOCK enabled and only some of the bytes could be transferred</li>
</ul>
</li>
</ul>


<p>So it seems that <code>write</code>s just put data into the buffer, rather than
guarantee some transmission. How do you block to make sure all data was
received by the server?</p>

<ul>
<li>UDP: you can&#8217;t, since delivery is unreliable and there&#8217;s no concept of
ack</li>
<li>TCP: you <em>could</em> I guess, but at that point you&#8217;d probably want some
kind of application layer ack</li>
</ul>


<p>Blocking reads/writes only (and immediately) return 0 bytes if EOF.
Non-blocking would fire EAGAIN (or WOULDBLOCK for synack).</p>

<h2>What interrupts a blocking call?</h2>

<p>Signal handlers. And only signal handlers.</p>

<p>This article does a great job explaining why EINTR fires:
http://250bpm.com/blog:12</p>

<p>Basically, if you CTRL-C (or any other interrupt), you can&#8217;t just have
the kernel start you off back on the same syscall that was blocking, or
else you&#8217;ll have no opportunity to respond to a handled interrupt, e.g.
setting a flag in a signal handler that tells the world it&#8217;s quitting.</p>

<p>Also note that yielding the CPU (by way of a blocking sys call) is <em>not</em>
considered an &#8220;interrupt&#8221;; durr, if that were the case, then every
syscall would self-interrupt. This sounds obvious now but this shit is
complicated!</p>

<h2>Shutdown</h2>

<p><code>close</code> closes both sides of socket, shutdown only does half.</p>

<ul>
<li>shutdown read: future reads become eof. Called on UNIX domain socket,
EPIPE and SIGPIPE will happen if the peer continues to write.</li>
<li>shutdown write: Future writes yield SIG/EPIPE. Starts signalling EOF
to reading peer.</li>
</ul>


<p>Shutdown operates on the file description, closing all other descriptors
pointing to the same description. <code>shutdown</code> doesn&#8217;t touch the
descriptor; you still need to close it.</p>

<h2>recv and send</h2>

<p>Basically socket-specific <code>read</code> and <code>write</code>.</p>

<p>Basically allows granularity of read behavior; you can non-block a
read/write rather than performing an ioctl first.</p>

<ul>
<li>DONTWAIT; fire EAGAIN if nothing there (only differs</li>
</ul>


<h2>Giant Ruby IO impedance mismatch</h2>

<p>This has been plaguing me since the beginning of time, particularly as I
tried to apply examples to Ruby code: Ruby&#8217;s default IO methods don&#8217;t
map to the same-named syscalls.</p>

<p>Ruby -> C/syscall</p>

<ul>
<li>IO#read -> fread(n): uses internal C buffer, doesn&#8217;t return less than n
unless EOF</li>
<li>IO#readp -> read(n): no buffers used, only read what&#8217;s there</li>
<li>IO#read_nonblock -> read with <code>NON_BLOCK</code> set</li>
</ul>


<p><code>read_nonblock</code> is like <code>read_partial</code> with nonblock.</p>

<p>SO MORAL OF THE STORY if you&#8217;re reading C code with <code>read()</code> and want
the equivalent Ruby, use <code>readpartial</code>.</p>

<h2>sendfile</h2>

<p>Takes the shortcut between disk file and peer socket, rather than having
to buffer in user space first. Whatever fd you pass to it must be
<code>mmap</code>-able, which usually just means regular files on disk.</p>

<p>Not-unlike the Apache x-sendfile optimization (or nginx x-accel) wherein
rather than having the application logic handle the slow
buffering/serving of a file, let nginx or apache do it for you.
Restrictions obviously apply but whatevs.</p>

<h2>shutdown rd w TCP</h2>

<p>Calling <code>shutdown()</code> on the read side of a TCP socket doesn&#8217;t make a ton
of sense since there&#8217;s no good way to essentially signal an EPIPE on the
sending peer. So don&#8217;t do it, it&#8217;s unreliable.</p>

<h2><code>TIME_WAIT</code></h2>

<p>This is the period of time after an active close that&#8217;s been acked by
the peer; you chill out in this state for a little bit to make sure that
any re-transmitted duplicate packets are received and dropped.</p>

<p>This period of time can be like, 30 seconds to 2 minutes depending on
the implementation. Cray cray. I guess this is why you can&#8217;t bind to a
port immediately after killing the process that was using it.</p>

<p><code>TIME_WAIT</code> is useful for:</p>

<ul>
<li>reliable connection termination</li>
<li>allowing enough time for duplicated packets from an old connection to
not confuse a new connection</li>
</ul>


<p>It allows enough time to send acks and fins and what not.</p>

<p>Noobs try and disable it, as if they don&#8217;t need it, but they probably
do. <code>TIME_WAIT</code> ensures reliability of future connections. You can use
<code>SO_REUSEADDR</code> though, coming soon.</p>

<h2>netstat</h2>

<p>Tells you inet and unix socket information. Including send and receive
queue bytes. I just got Arq so that has a full sendQ for obvious
reasons.</p>

<p>It&#8217;s probably rare that both Send-Q and Recv-Q would be large; it means
a program is sending lots of shit (and the server can&#8217;t transmit it fast
enough) and the program isn&#8217;t reading lots of things out of the buffer.</p>

<p><code>*:*</code> means bound.</p>

<p>Get stats via <code>-s</code>. Stats for tcp <code>-sp tcp</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gnomons all up on this journal]]></title>
    <link href="http://machty.github.com/blog/2014/10/17/gnomons-all-up-on-this-journal/"/>
    <updated>2014-10-17T17:52:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/17/gnomons-all-up-on-this-journal</id>
    <content type="html"><![CDATA[<h2>timerfd</h2>

<p>Just like any other api that inverts signal-handling API into something
file handle-y, Linux has such an API for expiring timers, which means
you can use these file handles w functions like <code>select</code>, <code>poll</code>, and
<code>epoll</code>.</p>

<h2>CLOEXEC reminder</h2>

<p>I&#8217;ve already kinda learned this but I FORGET. The CLOEXEC flag is
something that appears in <code>open</code> and other APIs that create a file
handle; it means that if <code>exec</code> is ever called on a process with a
CLOEXEC handle, the handle will close rather than leak to a process that
can&#8217;t even really access that file handle.</p>

<h2>spawn = fork + exec</h2>

<p>Allows for greater degree of flexibility in what happens between those
two steps. Node only has spawn because it&#8217;s trying to be cross platform.
There&#8217;s no fork. There&#8217;s no spoon.</p>

<p><code>posix_spawn</code> exists to more directly spawn a process when <code>fork</code> is not
supported.</p>

<h2>Embedded System</h2>

<p>http://en.wikipedia.org/wiki/Embedded_system</p>

<p>Usually means you&#8217;re running software on some piece of hardware with a
dedicated purpose, limited resources, and real-time requirements. It&#8217;s a
computer system embedded into some larger mechanical whole.</p>

<h2>fork</h2>

<p>No guarantee of where parent or child is scheduled to run so don&#8217;t run
into conditions.</p>

<p><code>fork</code>ing can be wasteful especially if immediately <code>exec</code>ing, cept for
the fact that</p>

<ul>
<li>program text is marked read only and often shares the same page
mapping as parent</li>
<li>copy-on-write</li>
</ul>


<p>You can execute code in a single-purpose forked child process without
changing the resource/memory footprint of the parent. Useful if the code
in question is unspeakably/unstoppably leaky or prone to memory fragmentation.</p>

<p>Mmm mmm mmm using pipes.</p>

<pre><code>r,w = IO.pipe

if fork
  w.close
  answer = r.read
  Process.wait
  puts $?.exitstatus
  puts answer
else
  r.close
  w.write("SOME BULLSHIT")
  exit(123)
end
</code></pre>

<p>Useful for when the single byte/octet return code (<code>$?.exitstatus</code>)
won&#8217;t cut it. Insane Posse Clown.</p>

<p>BSD used to have a shit wasteful fork back in the day, then added vfork,
and then everyone else made efficient COW <code>fork</code>s, but still most people
provide an implementation of vfork which:</p>

<ul>
<li>performs zero duplication, even of VM pages (which all forks due, even
if no writes have happened yet under a COW system). Parent memory
shared until <code>exec</code> or <code>_exit</code>.</li>
<li>Parent execution suspended til then</li>
</ul>


<p>This is risky as fuck because:</p>

<ul>
<li>simply returning from a function will impact parent process&#8217;s memory.
It&#8217;s almost like a step-through longjmp, cept, no this totally breaks.
It&#8217;s not like the parent will pick up from wherever the child process
leaves off; the process counters are different between processes, so
if you return from a function in a child, you&#8217;ll fuck with the stack
frame, probably cause some SEGVs, and bite the big one.</li>
</ul>


<p>You can on the other handle futz with file handles in this time
(apparently these are duplicated? just not VM pages?)</p>

<pre><code>vfork -- spawn new process in a virtual memory efficient way
</code></pre>

<p><code>_exit</code> must be used because <code>exit</code> would flush stdio and thus fuck the
parent.</p>

<p>But don&#8217;t use vfork. It sucks. It&#8217;s obsolete. It&#8217;s not even in SUSv4.
It&#8217;s garbage shit-kicker nonsense. <code>fork</code> is pretty much just as fast
when COW exists.</p>

<p>You can actually control whether parent or child runs first via
<code>proc/sys/kernel/sched_child_runs_first</code> (or the sysctl equiv).</p>

<p>The argument for parent-first is that the TLB cache is warm with parent
stuff, so memory lookups are faster yadda yadaa I can&#8217;t imagine this
actually makes a difference unless you&#8217;re forking like a motherforker.</p>

<p>Hehehehe:</p>

<pre><code>3.times { fork }
puts "wat"
</code></pre>

<p>Results in</p>

<pre><code>wat
wat
wat
wat
wat
wat
wat
wat
</code></pre>

<p><code>atexit</code> handlers are shared w <code>fork</code>.</p>

<h2>Process rehash, new learnings</h2>

<p>Even if a parent kills a child, it has to wait or else it remains a
zombie.</p>

<p>There&#8217;s no such thing as a zombie orphan; well actually there are
transient zombie orphans; <code>init</code> will adopt them and immediately <code>wait</code>
on them, and then they&#8217;ll be collected.</p>

<p>Prediction: it gets messy to call <code>wait</code> in a SIGCHLD handler since
signals might coalesce; two SIGCHLDs might coalesce into one, and the
foolish programmer might only wait once. I bet if I looked at unicorn
I&#8217;d find a loop.</p>

<h2>KGIO</h2>

<p>http://bogomips.org/kgio/</p>

<p>Ruby gem w C extensions for Kinder, Gentler IO.</p>

<ul>
<li>Avoids expensive EAGAIN / EINPROGRESS exceptions

<ul>
<li>EAGAIN: non-blocking IO when there&#8217;s no data, so try <em>again</em> later</li>
<li>EINPROGRESS: (less common), similar to EAGAIN but distinguished for
things like <code>connect</code> in mid 3-way handshake</li>
</ul>
</li>
<li>other niceties</li>
</ul>


<p>Used in Unicorn and related Rainbows!. Can&#8217;t use in JRuby obviously but
then again no forking server model is supported in JRuby.</p>

<h2>JRuby Servers</h2>

<p>https://github.com/jruby/jruby/wiki/Servers</p>

<p>Can&#8217;t use unicorn but you can use things like Puma, or Trinidad, which
wraps rack/rails within a Tomcat container. So many things I do not
know!</p>

<h2>Isomorphic Code</h2>

<p>Can run on server or client. Not sure of the roots, but sounds like it
started in 2011, at least as way of describing JS code that runs between
both? React-router is &#8220;isomorphic&#8221;.</p>

<h2>ELF format</h2>

<p>Executable and Linking Format.</p>

<h2>Built-ins</h2>

<p>Sometimes no forking occurs because it&#8217;s quick and efficient, or because
there are desirable side effects that should make it into the current
shell process. Like <code>cd</code> (can&#8217;t change the cwd of the current process
if you&#8217;re forking). Or <code>export</code>.</p>

<h2>Why keep fd&#8217;s open on <code>exec</code>?</h2>

<p>If you&#8217;re exec&#8217;ing, shouldn&#8217;t you lose access to file handles? Not
necessarily; stream redirection via the shell is a good example where
you preserve what 0 1 2 point to.</p>

<h2>Auto-reap zombie child processes</h2>

<p>If you set the signal mask to ignore <code>SIG_CHLD</code>, potential zombie child
processes will be automatically reaped. So no need to <code>wait</code> in that
case.</p>

<p>Also, it&#8217;s implementation-defined (according to SUS) whether <code>exec</code>
resets the <code>SIG_CHLD</code> disposition after an exec (Linux preserves,
Solaris resets to default, etc).</p>

<h2><code>exec</code> clears sig handlers</h2>

<p>Of course, since sig handlers live in the text and text is replaced, so
must handled signals be reset. Everything else about signals is
preserved (cept that <code>SIG_CHLD</code> ignores may not
get reset to <code>DFL</code> depending on the implementation), but since most
processes expect to start with a clean slate, if you didn&#8217;t write the
process, you should reset everything yourself before an exec.</p>

<h2><code>system</code></h2>

<p>Runs a shell command, with all the processing/substitutions of the
command string that you would expect.</p>

<p>Inefficient; fork+execs twice, one for the shell, and one for the
process you intend to run.</p>

<p>set-user-id and set-group-id programs should never use <code>system</code>
since it opens the door to setting weird ENV vars to get unintended
programs to run, and they&#8217;ll inherit the effective user id. The
canonical example is setting IFS to <code>a</code>, and then <code>shar</code> runs and is
interpreted as <code>sh r</code>, and <code>r</code> is some script/executable that is no
running with admin privileges to do whatever it wants. Evil shit!</p>

<p>Note that modern shells reset IFS now as a rule.</p>

<h2><code>clone</code></h2>

<p>A Linux-specific sister of <code>fork</code>; it creates a new process, but starts
at the beginning of a provided function called w a provided arg. The
child process terminates if this function returns (or the exit variants
are called).</p>

<ul>
<li>you can specify termination signal (rather than always CHLD)</li>
<li>flags let you meticulously control a variety of info about what is
actually shared with the new child process: file handles, signal
dispositions, etc.</li>
</ul>


<p>So basically it&#8217;s threads&#8230; but separate processes? Is there a name for
these kinds of threads? LinuxThreads I guess.</p>

<p>Threads/processes are just kernel scheduling entities with differing
degrees of shared attributes. Processes are just, like, organizational
distinctions. Schedule wise, the scheduler doesn&#8217;t care whether it&#8217;s a
thread or a process. Blueskying of course.</p>

<h2>Mode 7</h2>

<p>http://en.wikipedia.org/wiki/Mode_7</p>

<p>The special background layer mode that enabled perspective-y effects, as
well as zooming and other cool things. F-zero relied on it, the map in
Link to Past relied on it. Any zoomy crazy shit that wasn&#8217;t a sprite
relied on it.</p>

<h2>Dyson Sphere</h2>

<p>Shell around the sun.</p>

<p>http://www.islandone.org/LEOBiblio/SETI1.HTM</p>

<p>Other lifeforms smashed apart their planets to build a dyson sphere to
capture energy, support population growth.</p>

<p>We could knock apart Jupiter, use its mass to create a sphere around sun
with a diameter twice that of Earth&#8217;s orbit. The shell would be 10 feet
thick, and make&#8230; something&#8230; habitable?</p>

<h2>Fermi Paradox</h2>

<p>http://waitbutwhy.com/2014/05/fermi-paradox.html</p>

<p>http://en.wikipedia.org/wiki/Fermi_paradox</p>

<p>The apparent contradiction between the high probability of alien life
and the fact that no one has contacted us yet.</p>

<p>Explanation 1: They don&#8217;t exist</p>

<p>Possibly due to a Great Filter; at some point in development, something
wipes them out, or there&#8217;s some point of evolution that&#8217;s reaaaally
really hard to get beyond that filters out people who made it beyond the
previous step.</p>

<p>So where does that leave <em>us</em>?</p>

<p>We&#8217;re</p>

<ul>
<li><p>rare</p>

<ul>
<li>other rare survivors might be reaaaally far away</li>
<li>the filter is behind us</li>
</ul>
</li>
<li><p>first: no one else to communicate with yet</p></li>
<li>fucked: we&#8217;re about to hit our wall like everyone else has</li>
</ul>


<p>Ah I love the potentialy reason for why intelligent life wouldn&#8217;t
broadcast data: there are potential harmful civilizations that would
destroy anyone they could find, and most advanced civilizations are
smart enough not to give away their locations to these fuckers. That&#8217;s
why we only have SETI today, and not METI (messaging to ETs), which most
scientists agree is a profoundly unwise idea.</p>

<p>Eesh, or there&#8217;s only one superpredator species, like humans on Earth,
that would wipe out another species before it got too intelligent.
Nipping the universe in the bud. Fuuuck that.</p>

<p>Or we&#8217;re not using the right technologies to communicate, or even if we
did, other minds might work much faster or slower than ours; it could
take years for them to say hello. Hahaha.</p>

<p>Or we are receiving communications but the gov is hiding it.</p>

<p>Or other civilizations see us, but we&#8217;re a look-don&#8217;t-touch zoo.</p>

<p>Or the higher civilizations are here, but we couldn&#8217;t begin to
understand them, they&#8217;re just so advanced.</p>

<p>Or we&#8217;re just so fundamentally incorrect about reality.</p>

<p>Types of civilization</p>

<ul>
<li>I: ability to use all the energy on the planet (Sagan says we&#8217;re a
0.7; just under the legal limit)</li>
<li>II: ability to harness all power of a star (e.g. Dyson sphere)</li>
<li>III: ability to harness all power of a whole galaxy</li>
</ul>


<p>http://waitbutwhy.com/ seems pretty awesome in general. Three thumbs up.</p>

<h2>fork v vfork</h2>

<p>I&#8217;ve already written about this but followup: vfork is faster, but
considering how slow the followup <code>exec</code> is, this is probably
neglibibilgigible.</p>

<h2><code>time</code> and forking</h2>

<p>Are child process CPU times included?</p>

<pre><code>N = 20000000

def slow_thing
  inc = N / 5
  N.times do |i|
    puts "." if i % inc == 0
  end
end

do_fork = true
if do_fork
  fork do
    slow_thing
  end

  Process.wait
else
  slow_thing
end
</code></pre>

<p>With forking:</p>

<pre><code>real    0m1.565s
user    0m1.544s
sys     0m0.013s
</code></pre>

<p>Without forking:</p>

<pre><code>real    0m1.480s
user    0m1.464s
sys     0m0.011s
</code></pre>

<p>Seems that yes they are (there&#8217;s no difference with the above).
Actually, quick aside: it seems that <code>time</code> is a bash builtin. Not sure
why, but if I use <code>/usr/bin/time</code> as well, not much changes</p>

<pre><code>1.42 real         1.41 user         0.00 sys
1.49 real         1.47 user         0.01 sys
</code></pre>

<p>Ah, <code>time</code> will add together all forked processes. And apparently if you
have two active processes, you might wind up with a CPU time greater
than real time. LET US FIND THE FUCK OUT.</p>

<pre><code>N = 20000000

def slow_thing
  inc = N / 5
  N.times do |i|
    puts "." if i % inc == 0
  end
end

num_forks = 2

num_forks.times do
  fork do
    slow_thing
  end
end

num_forks.times { Process.wait }
</code></pre>

<p>BOO YA</p>

<pre><code>real    0m1.636s
user    0m3.199s
sys     0m0.021s
</code></pre>

<p>How awesome is it when things start clickin.</p>

<p>So how does time actually do it?</p>

<h2>curl + tar</h2>

<pre><code>curl http://mirror.anl.gov/pub/gnu/time/time-1.7.tar.gz | tar -x
</code></pre>

<p>I guess you&#8217;re also supposed to use <code>-z</code> as well, but in my manpages:</p>

<pre><code> -z      (c mode only) Compress the resulting archive with gzip(1).  In extract
         or list modes, this option is ignored.  _Note that, unlike other tar
         implementations, this implementation recognizes gzip compression auto-
         matically when reading archives._
</code></pre>

<p>That&#8217;s so nice of them. And ridiculous that it wouldn&#8217;t always work that
way. Maybe not.</p>

<h2>Threads</h2>

<p>Every process has at least one thread.</p>

<p>Concurrency via processes has some limitations:</p>

<ul>
<li>nothing is shared so communication is limited to IPC, pipes and what
not</li>
<li>expensive due to page table duplication and file descriptor table dups
and all the things that happen during <code>fork</code></li>
</ul>


<p>Most process attributes are shared between threads. Here&#8217;s some stuff
that&#8217;s not:</p>

<ul>
<li>thread ID</li>
<li>signal mask</li>
<li>thread-specific data</li>
<li>alternate signal stack</li>
<li><code>errno</code></li>
<li>floating-point env</li>
<li>scheduling priority/policy</li>
<li>CPU affinity</li>
<li>capabilities</li>
<li>stack</li>
</ul>


<p>Of course you could share stack vars between threads but this runs the
risk of data invalidation once a function returns etc etc etc.</p>

<p>How does errno work if it&#8217;s a variable? Trick question, it&#8217;s a macro
that evals to an lvalue (so you can still manually set it yourself).</p>

<p><code>join</code> is like <code>wait</code>, it seems to free up that thread, and you wouldn&#8217;t
want to call it twice, lest terrible things happen. You must detach or
join. (Seems like the equiv in processland for detach is to set
<code>SIG_CHLD</code> to ignore).</p>

<p>Unlike processes, threads are created as peers. If A spawns B spawns C,
then A can join C. They&#8217;re all siblings in the same process cult.
There&#8217;s no concept of generic <code>Process.wait</code> to join with any ol thread.
This makes sense since some lib function could spawn a thread and you
might clobber its operation and steal its return value.</p>

<p>Once you detach a thread, you can&#8217;t join it. Again keep in mind that
threads can&#8217;t escape the fact that they live in a process; if the
process dies, so do the threads.</p>

<p>Threads can&#8217;t use the full virtual memory address space; they have to
share with all the other threads. Probably not a big deal unless you&#8217;re
doing insane shit, but still.</p>

<p>Mutexes in Linux are implemented in user spaces using futexes (fast user
space mutexes). They only cause system calls when multiple threads lock.</p>

<h2>Pitchblende</h2>

<p>Old school word for Uraninite. Basically ore ready for uranium
processing.</p>

<h2>Javascryptonomicomicon</h2>

<p>http://matasano.com/articles/javascript-cryptography/</p>

<h2>ffs ssl</h2>

<p>http://wingolog.org/archives/2014/10/17/ffs-ssl</p>

<blockquote><p>WTF their price is 49 dollars for a stupid certificate? Your domain name was only 10 dollars, and domain name resolution is an actual ongoing service, unlike certificate issuance that just happens one time.</p></blockquote>

<p>Hmm clearly I need to better understand how this all works&#8230; being a
CA, root or no, definitely requires constant service</p>

<p>Wow this is sneaky: http://thejh.net/misc/website-terminal-copy-paste</p>

<p>Never copy and paste secure stuff from a webpage. I&#8217;m so sad.</p>

<p>Lol &#8220;Well now I have to live with this confidence-inspiring dialog,
because I left off the organization&#8221;</p>

<p>TIL Organization is the name you choose that you want to pop up when
inspecting certifications.</p>

<p>Wow, googlebot still doesn&#8217;t use TLS 1.2.</p>

<h2>Condition variables</h2>

<p>Didn&#8217;t I write about this already? So forgetful!</p>

<pre><code>Loop
  Lock around shared structure
    If shared structure "empty"/unusable, wait on condition variable


(other thread):
Lock around shared structure
  Put data into it
  Signal Condition variable
</code></pre>

<p>If you didn&#8217;t do this, you&#8217;d have some wasteful CPU-hog loop that
constantly locked and unlocked the mutex to check if the shared
structure had data ready.</p>

<ul>
<li>Always re-check the condvar predicate; some other thread might have
invalidated it in the meantime.</li>
<li>Also, a looseness of predicates lends to some flexibility; a producer
doesn&#8217;t need to know the exactly logic that consumer cond var depends
on; producer can just say &#8220;hey interested people, this shared
structure changed, so chiggity check yourselves&#8221;</li>
<li>Some implementations, particularly multi-core, might wake up a cond
var for fucks sake (explicitly allowed by SUSv3</li>
</ul>


<h2>Reentrant revisited</h2>

<p>We already knew thread-safe functions aren&#8217;t necessarily reentrant (e.g.
a signal handler that locks a mutex might be reentered and deadlock the
same mutex on the same thread). A reentrant function is one that doesn&#8217;t
access globals, mutexes included (what if a shared structure, possibly a
mutex is passed on to the function? Deadlock could still occur&#8230; I
guess the answer is that a function might be itself reentrant, but a
caller might not be reentrant so the chain of calls it makes with
whatever global data it&#8217;s using isn&#8217;t going to be reentrant).</p>

<p>Not every function can be reentrant. malloc can&#8217;t be: it <em>must</em> access
global data. A <code>_r</code> suffix implies reentrancy.</p>

<h2><code>pthread_once</code></h2>

<p>The pthreads library provides facilities for ensuring that some kind of
initialization (that occurs in the function you provide it) only happens
once.</p>

<h2>Thread-specific data</h2>

<p>I guess it&#8217;s a thread-local storage, but TLS is an overlay API that&#8217;s
friendlier to program in. In C at least you use the <code>__thread</code> keyword
when declaring a variable and voila you get everything for freeee.</p>

<h2>Thread cancellation</h2>

<p>Threads can be cancelled at cancellation points, SUS-specified
lib/syscall functions, or if you&#8217;re a moron (or you have a 0.001% use
case) you can set cancelability to be async, which means it might
interrupt at any ol machine instruction.</p>

<h2>ncurses</h2>

<p>New curses. Used for writing terminal apps. Cross-terminal. Optimizes
refreshes for our remotely connected friends.</p>

<h2>Thread stacks</h2>

<p>Main thread has mucho size, thread stacks are smaller but configurable
in size.</p>

<h2>Signals and threads</h2>

<p>Horrible combination, avoid when possible. Reason being: signals existed
long before threads and never expected to integrate with them.</p>

<ul>
<li>signal dispositions are process wide (you can&#8217;t have one thread ignore
a signal and another use a specific handler, etc)</li>
<li>signals might be directed to thread or process. Thread directed if

<ul>
<li>signal is result of hardware instruction (SEGV, etc&#8230; same
reason that signals are sometimes synchronous)</li>
<li>SIGPIPE</li>
<li><code>pthread_kill</code> or <code>pthread_sigqueue</code></li>
</ul>
</li>
<li>all others are process wide</li>
<li>kernel selects arbitrary thread to handle signal (as opposed to
multiple threads handling a single sig)</li>
</ul>


<p>Shit. What the fuck is a difference between signal disposition and
signal mask?</p>

<p>Disposition: per-process structure that controls whether a signal
is a) is ignored, b) runs a custom handler, or c) invokes the default
action.</p>

<p>Mask: data structure that knows when signals are temporarily blocked,
the idea being that they should be later unblocked and the signal will
fire.</p>

<p>OK so dispositions are process-wide, but masks are thread-local; so as
one time slice begins, the kernel will look up the mask for that thread
and decide whether a pending signal should fire upon it, and if not,
wait til later. You might have some threads that block all signal
handlers.</p>

<p>Ahhhhhhh SO FUCKING FORGETFUL. Didn&#8217;t I just cover this a few days ago?
Basically I tried writign a ruby program to set the mask, but you don&#8217;t
have access to the mask in Ruby because Ruby uses it internally and is
trying to layer a nice signal API on top of it. I was trying to set up
something where a main thread had a custom handler but all the threads
it spawned blocked SIGINT so that it must be handled by the main thread.</p>

<p>But if I weren&#8217;t too lazy to write the test in C:</p>

<blockquote><p>By manipulating the per-thread signal masks, an application can control which thread(s) may handle a signal that is directed to the whole process.</p></blockquote>

<p>I mostly just wanna know how the kernel selects the thread to wake up.
Does it just loop through each of the sleeping threads and find the one
whose signal mask will allow it?</p>

<p>None of the pthread lib has async-signal-safe functions, so what&#8217;s
recommended:</p>

<ul>
<li>main thread blocks everything, all spawned threads inherit signal mask</li>
<li>single thread deblocks and is responsible for dispatching signals.</li>
</ul>


<p>Reminder: difference b/w reentrant and async-signal-safe:
async-signal-safe is a function you can call in a signal handler because
it as either a) reentrant or b) not interruptible by a signal handler.</p>

<p>So why &#8220;async&#8221;-signal-safe? There are sync signals and async signals
depending on whether hardware issued the signal, whether it&#8217;s a signal
sent to yourself, etc. I think the answer is that async-signal-safe is a
concept that exists even outside of the context of a currently running
signal handler. In other words, if you know <code>foo</code> is NOT async signal
safe, it means <code>foo</code> might be interrupted by an async signal, and if
itself were called, hell might break loose.</p>

<p>So that&#8217;s a useful concept on its own, but when applied to what you&#8217;re
allowed to call within a signal handler, the rule is &#8220;don&#8217;t call non
async-signal-safe functions&#8221; because, within a handler or no, you might
be interrupted by <em>another</em> signal handler and fuck yourself.</p>

<p>So you should always write signal handlers that are themselves reentrant
and only call async-signal-safe functions.</p>

<p>Remember, these async-signal-safe functions might not be reentrant on
their own, but because they block signal handlers from interrupting
them, they are async-signal-safe.</p>

<p><code>exec</code> wipes out all other threads entirely. Gone. No termination /
cancel handlers called.</p>

<p><code>fork</code> kills all other threads than the caller, but mutexes and
conditional variables live on in whatever state. (What about
<code>thread_t</code>s? Probably persist but point to dead threads?). Memory leaks
can happen since the killed threads don&#8217;t have an opportunity to clean
up after themselves.</p>

<p>LOVE.</p>

<p>But you can defined atfork handlers. Otherwise you probably shouldn&#8217;t
fork unless you&#8217;re also <code>exec</code>-ing.</p>

<h2>NPTL</h2>

<p>Native POSIX Threading Lib</p>

<p>The software feature in linux kernel that enabled POSIX-compliant
threads. Red Hat 9 added it first, either as a kernel module or patch to
the OSS kernel.</p>

<h2>Social Engineering</h2>

<pre><code>http://en.wikipedia.org/wiki/Social_engineering_(security)
</code></pre>

<p>It just means sneakily manipulating people into giving away their shit.
Phishing and what not.</p>

<h2>M:N</h2>

<p>aka number of threads map to number of Kernel Scheduling Entities
(processes, threads, etc, schedulable executable things).</p>

<h3>M:1</h3>

<p>Multiple threads to a single KSE. User-level threads aka green threads.
Kernel doesn&#8217;t know about these threads.  Fast context switching, but</p>

<ul>
<li>other slow syscalls block all other threads, e.g. <code>read</code>, since
control is passed from user-spacing threading lib to kernel</li>
<li>kernel can&#8217;t schedule, which means no multi-processor green threads.</li>
</ul>


<p>I&#8217;m guessing the way scheduling worked was to set interval timers and
pre-empty threads. Seems correct.</p>

<h3>1:1</h3>

<p>Kernel-level threads. Pthreads.</p>

<p>Thread creation, context switching, etc, are slower since it requires a
syscall, but it means the kernel can efficiently schedule, put them on
separate cores.</p>

<h3>M:N</h3>

<p>Some kernel, some green.</p>

<p>Keep in mind that this whole <code>M:N</code> nomenclature also applies to
processes vs threads. err, does it? Processes and green threads? I could
be bullsharting right now.</p>

<p>M:N is complex, perhaps not worth it. It was originally the suggested
implementation for NPTL.</p>

<h2>Process Group / Session</h2>

<ul>
<li>Process group

<ul>
<li>Process group leader

<ul>
<li>Process that creates the group, whose PID becomes the process group
ID</li>
</ul>
</li>
<li>New process inherits its parent&#8217;s pgid</li>
<li>Lifetime: from process group creation to when last process leaves
group id</li>
<li>premature question: can a new process take the id of a now-dead
process that used to be pg leader?</li>
<li>processes can leave by terminating or joining another group</li>
<li>pg leader can leave before others, no biggie.</li>
</ul>
</li>
<li>Session

<ul>
<li>collection of PGs</li>
<li>Session leader

<ul>
<li>Process that creates the session, whose PID becomes the session ID</li>
</ul>
</li>
<li>New process inherits its parent&#8217;s pgid</li>
<li>all processes in session share terminal</li>
</ul>
</li>
</ul>


<p>SIGHUP is sent to PG leader when terminal disconnects.</p>

<p>Login:</p>

<ul>
<li>Shell becomes session leader AND process group leader

<ul>
<li>every command it runs, it makes it process group leader (but shell
remains session leader)</li>
<li>any command forks have same process group id (the command&#8217;s pid),
and session id is shell.</li>
<li>shell can create many process groups (commands) and can foreground
one and the rest are background</li>
</ul>
</li>
</ul>


<p>You can only change process group of yourself or your children, else
ESRCH. You can only move a process between groups within the same
session. You can&#8217;t change the process group of the session leader.
You can&#8217;t child the process group of a child if it&#8217;s already called
<code>exec</code> (you might confuse the poor child); I guess this means shells
will fork and then the parent will call setpgid on the child&#8230;? Why
doesn&#8217;t the child just do it itself?</p>

<p>Answer (and I love this book for this shit: go buy the Linux Programming
Interface right now): the parent process needs to be able to send job
control signals to the newly created child (to the new process group
ID it&#8217;ll have), and the child needs to set the new pgid before it
<code>exec</code>s or else all is lost, so how do you guarantee that the new process
group on the child has been set before either the parent or child
proceed? Answer: do it on both and ignore errors on the parent.</p>

<p><code>setsid</code> establishes a process as group and session leader and
disconnects it from any controlling terminal. You can&#8217;t <code>setsid</code> if
you&#8217;re process group leader (EPERM), so you need to <code>fork</code> first to get around
that. This restriction is in place because any other process group
children will still have a pgid that points to this same process group
leader process even if it has a new session id; the only way to ensure
the numbers don&#8217;t point to the same place is to distribute new numbers,
e.g. new process ids, e.g by forking.</p>

<pre><code>if fork
  Process.wait
  puts "done"
else
  # if you hadn't forked, setsid would fail because
  # this process would be PG leader
  Process.setsid
  puts gets
end
</code></pre>

<p>The above code doesn&#8217;t error out like I thought it would&#8230; it still has
access to the terminal, but, it, umm, couldn&#8217;t open the terminal if it
wanted to?</p>

<p>When session leader opens a controlling terminal, it becomes the
controlling process for a terminal. They are LINKED. If a process
has a controlling terminal, it can open special file <code>/dev/tty</code>.</p>

<p>Ah ok so to make the above snippet break, I should have actually done</p>

<pre><code>fork do
  Process.setsid
  File.open('/dev/tty')
end
Process.wait
</code></pre>

<p>which yields</p>

<pre><code>setsid.rb:3:in `initialize': Device not configured - /dev/tty (Errno::ENXIO)
</code></pre>

<p>You open <code>/dev/tty</code> if the shell (or someone else) already redirected
your output (by <code>dup2</code>ing 0 1 2 file descriptors) and you want to get
back in touch with your <code>tty</code>. Call your <code>tty</code>. She&#8217;s your controlling
terminal.</p>

<p>AH HA this is how you can take a program with redirected input and get,
say, the users password from the terminal. Fuckin badass. Let&#8217;s try it.</p>

<pre><code>$ echo "wat" | ruby getpass.rb
here's some stuff from stdin
wat
now type something in: borflex
You wrote borflex
</code></pre>

<p>So many cool shits!</p>

<p>Note that you can also open <code>/dev/tty</code> without it becoming the
controlling terminal.</p>

<p>TTY captures special characters, converts into signals sent to members
of the foreground process group.</p>

<p>This demonstrates how a signal is delivered to literally everyone in the
foreground process group (5 &#8220;omg&#8221;s are printed).</p>

<pre><code>N = 5
N.times do
  fork do
    trap(:INT) do
      puts "omg\n"
      exit
    end
    sleep
  end
end

trap(:INT) do
  puts "main\n"
end

N.times do
  Process.wait
end
</code></pre>

<p>It&#8217;s possible for no one to be the foreground process, but rare, and not
when there&#8217;s a shell that&#8217;s monitoring for foreground process to quit
(via wait or some other signal handle).</p>

<p><code>tcgetpgrp</code> gets the foreground process group of a provided terminal fd.
<code>bash</code> and other shells make use of this when passing the terminal
around to child processes.</p>

<h2>SIGHUP</h2>

<p>This is sent when terminal connection is severed (also sends SIGCONT to
make sure the process is alive). Happens when:</p>

<ul>
<li>terminal driver detects disconnect of one form or another</li>
<li>the last pseudoterminal file handle has been closed (i don&#8217;t
understand this)</li>
</ul>


<p>You can ignore / handle SIGHUP but future reads yield EOF. (What about
output? Same thang?).</p>

<p>SIGHUP is often used to tell a daemon process to reload a config file.
Why SIGHUP? Because daemons have no controlling terminal so the single
is otherwise useless; why not put it to work in some useful way? nginx
and others do this.</p>

<p>When terminal disconnects, the controlling process (the shell) gets a
SIGHUP. Shells will terminate, but before doing that they&#8217;ll send a
SIGHUP to each of the process groups.</p>

<p><code>nohup</code> just forks, sets disposition to ignore HUP, and then <code>exec</code>s.
Easy peasy.</p>

<p>Note that shells won&#8217;t send SIGHUP to any process groups it didn&#8217;t
create.</p>

<pre><code>echo $$
</code></pre>

<p>This is like the thing that trolled me before w <code>ruby -e</code>&#8230; <code>$$</code> in a
shell command is the shell&#8217;s pid.</p>

<p>Background tasks don&#8217;t get sighup&#8217;d when you exit a terminal. SIGHUP
only gets sent to foreground processes. That seems cray cray.</p>

<h2>$$ in Ruby</h2>

<p>Everywhere I look it says it is the id of the running process. As in,
you know, the pid. But if I do</p>

<pre><code>ruby -e "puts Process.pid; puts $$"
</code></pre>

<p>I get two different numbers.</p>

<p>You know why?</p>

<p>Because, of course, <code>$$</code> means something to shells! Before the command
even runs, <code>$$</code> will be substituted with the pid of the shell! If I
change double quotes to single quotes, I get the same numbers, of
course!</p>

<pre><code>ruby -e 'puts Process.pid; puts $$'
</code></pre>

<p>This is why I started going crazy and thinking that the internet was
just wrong and that <code>$$</code> means the session id. Wrong!</p>

<h2>terminfo / termcap</h2>

<p>These are packages that some programs like vi and less make use of for
gracefully switching in and out of screen-controlling modes; vi and less
take over the screen but when you quit, you see the terminal lines from
before you ran them. This uses terminfo, which is a terminal database
which provides facilities such as that.</p>

<h2>sigtstp</h2>

<p>You can catch a suspend ^Z via sigtstp and then signal sigstop to
actually stop the process, but any parent processes listening in will
think you were stopped by sigstop rather than sigtstp, so you can do the
more complicated thing of setting disposition to default, unblocking,
raising tstp (so that it suspends this time), and then resetting the
mask upon reentry.</p>

<h2><code>init</code> and <code>wait</code></h2>

<p>I originally thought that <code>init</code> would <code>wait</code> on every orphan it
inherited, but of course, <code>wait</code> is blocking if nothing&#8217;s actually
terminated, so I think <code>init</code> actually checks the status of the
process and <code>wait</code>s only if it&#8217;s terminated. But if it inherits a
stopped process group, it won&#8217;t call wait. So you might have stopped
process groups that live on forever and no one ever terminates them.</p>

<p>Hence, upon orphaning, the kernel sends SIGHUP + SIGCONT to any
orphaning process groups that have at least one stopped process, and
these signals are sent to everyone. Of course, at some later
post-orphaning time, these orphaned processes might be stopped and would
need <em>something</em> to come along and resume or terminate them.</p>

<h2>nice values</h2>

<p>You can give processes (specific pids, groups&#8217;, users&#8217;) varying nice
values from -20 to 19, give or take. Lower means higher priority, high
means lower.</p>

<h2>The Complexity Balloon and other yehudaisms</h2>

<blockquote><p>&#8220;you can only squeeze the complexity ballooon&#8221;
- Yehuda Katz</p></blockquote>

<p>On XMLHttpRequest capitalization:</p>

<pre><code>btw: the rule for XHR is:
First acronym all caps
subsequent acronyms title-case
old Java rule
</code></pre>

<h2>Security book recommendations</h2>

<p>The art of software security assessment</p>

<p>The grey hat hackers handbook</p>

<h2>Scheduling</h2>

<p>CPUs have their own run queues, with items with different priorities.
Processes can be scheduled with RR or FIFO real-time policy.</p>

<p><code>RR</code> is round robin. Equal priority processes get a time slice and then
get thrown in back of queue at the end. They can be pre-empted by higher
order shits. How?</p>

<ul>
<li>higher order process blocked on syscall becomes unblocked (io
available, etc)</li>
<li>another process raised to higher level</li>
<li>current process raised to lower value than some other process</li>
</ul>


<p><code>RR</code> similar to standard RR <code>SCHED_OTHER</code>, difference being in the
strictness of the weighting algorithm. <code>SCHED_OTHER</code> uses nice values,
but a lower nice value than another isn&#8217;t a strict determiner of
scheduling order, but rather a weighted suggestion, where as <code>RR</code> is
brutal deterministic.</p>

<p>In <code>FIFO</code> there&#8217;s no timeslice. You have it til you&#8217;re preempted or give
up control.</p>

<p>You can lock up a system with <code>RR</code> or <code>FIFO</code>. To prevent:</p>

<ul>
<li>set low CPU limit, which causes default-terminating <code>SIGXCPU</code> to fire.</li>
<li>use <code>alarm()</code></li>
<li>use high priority watchdog process to watch others, adjust their
priorities.</li>
</ul>


<p>CPU Affinity just means the tendency for a process/thread/kernel
scheduling unit to run on the same CPU; switching CPUs involves a slow
context switch, cache invalidation, etc.</p>

<h2>Daemonizing</h2>

<ul>
<li>long-running, often started at boot</li>
<li>has no controlling terminal (hence never receives INT, TSTP, HUP,
etc); its excluded from job-control signals</li>
</ul>


<p>some examples</p>

<ul>
<li>cron</li>
<li>sshd: the ssh server that&#8217;s open for remote logins.</li>
<li>httpd: http server

<ul>
<li>ah so TCP isn&#8217;t a daemon, it&#8217;s a kernel-level IO protocol, but httpd
is a server. It&#8217;s apache, duerp.</li>
</ul>
</li>
<li>inetd: superserver daemon&#8230;? sounds like this doesn&#8217;t exist on mac.</li>
</ul>


<p>Some daemons run as kernel threads, w names bracketed.</p>

<p>To daemonize:</p>

<ul>
<li>fork: child process lives, master terminates.

<ul>
<li>returns control to terminal</li>
<li>child process lives on, and is no longer process group leader (it
inherits parent pgid which doesn&#8217;t match its new pid, so it can&#8217;t be
leader). this is needed for setsid (remember that setsid can&#8217;t be
called with process group leader, because other children processes
might be in that process group with a pgid pointing to a process
that lives in a different session, but process group members must
all live within the same session as a rule)</li>
</ul>
</li>
<li>setsid

<ul>
<li>become session leader

<ul>
<li>frees you from connected terminal</li>
<li>allows you to connect to another terminal (but&#8230; see below)</li>
</ul>
</li>
</ul>
</li>
<li>fork again

<ul>
<li>this prevents any future terminals you connect to from becoming
controlling terminal (TODO: why/when would a daemon connect to a
terminal?)</li>
<li>you don&#8217;t have to do this if you don&#8217;t open another terminal, or you
open terminals with <code>O_NOCTTY</code></li>
</ul>
</li>
<li>Clear umask (remember that umasks negate certain permissions on create
files&#8230; right? TODO: review this shit)</li>
<li>Change cwd to <code>/</code> so that in case the process was started with a cwd
on another file system that doesn&#8217;t contain <code>/</code>, that fs can be
unmounted

<ul>
<li>remember you can&#8217;t unmount &#8220;busy&#8221; FSs

<ul>
<li>files are open on it</li>
<li>processes have CWDs on it (how does it know? loop through the
processes?)</li>
</ul>
</li>
</ul>
</li>
<li>Close all file descriptors, 0, 1, 2, for similar busy FD unmount
reasons, also because you might get some TTOUs if you try and read
from stdin.</li>
<li><code>dup2</code> them to point to <code>/dev/null</code> so that lib functions and other
things don&#8217;t unexpectedly file, or you don&#8217;t open another file in
their place but some lib function things they&#8217;re std IO and then write
shit to them

<ul>
<li>reading dev null yields EOF. PRETTY EFFIN HANDY</li>
</ul>
</li>
</ul>


<p>Daemon shutdown consists of SIGTERM and a 5-second-layer SIGKILL if it
doesn&#8217;t clean up and shut down in that time (all processes concurrently
have 5 seconds to shutdown, not 5 seconds of CPU each; all SIGTERMS send
out at same time).</p>

<p>Memory leaks and fd leaks are a bigger deal for daemons since they&#8217;re so
long-lived.</p>

<p>Care must be taken to prevent multiple identical daemons from running;
Pidfiles help.</p>

<p>Logfiles can&#8217;t grow forever, and manually deleting a file if the daemon
doesn&#8217;t let go of it won&#8217;t prevent the resources from being freed until
the last handle is closed to the file. <code>logrotate()</code> can be used to aid
in this.</p>

<p>HUP is unused since daemons don&#8217;t have controlling terminals so people
classically use it to re-initialize or reload conf files.</p>

<p>Btw you can open other people&#8217;s TTYs. And it means you can write to
their screens.</p>

<p><code>syslog</code> and <code>syslogd</code> is a useful utility to global logging; write to
syslog unix domain datagram socket and blah dee blah dee blah it&#8217;ll
funnel out to wherever based on a shared conf FILE!</p>

<h2>Security</h2>

<p>Privilege via</p>

<ul>
<li>root started it</li>
<li>set-user-id or set-group-id + owned by root</li>
</ul>


<p>Guidelines:</p>

<ul>
<li>don&#8217;t use set-user/group-id unless you really need it; better to
use a child process

<ul>
<li>or if you need it, don&#8217;t give it root</li>
</ul>
</li>
<li>e.g. a process currently with set-user-id that allows unprivileged
users to write to a file they don&#8217;t have access to: much better to
set-group-id to a group specific to this process, and set group of
that file to that group. group. group. group.</li>
<li>if you&#8217;re set-user-id, only operate in that mode when you need it.</li>
<li>real id is whoever started the program</li>
<li>basically, shit is really hard and you should refer to this book every
time you need a refresher because this stuff megasucks.</li>
<li>don&#8217;t exec shells or other interpreters when you have privileges. Way
way way too open to abuse.</li>
<li>close file handlers; they&#8217;re just integers, not hard to reuse. recall
CLOEXEC</li>
</ul>


<h2>Clear out your memory</h2>

<p>Why? Because it might be paged out to a swap area that privileged
programs can read. So that seems pretty iScare. If that&#8217;s visible, why
not fuckin, fuckin, fuckin memory, man, why can&#8217;t you like open another
process and look at its memory maaaan, even without it having been paged
first.</p>

<p>Actually you can if you&#8217;re not careful. You can create special device
files that have direct access to RAM. That&#8217;s so fucking crazy. So even
<code>chroot</code> can&#8217;t even save you if you have a set-user-id-root program.
Crazy crazy crazy.</p>

<p>Also if you have a leftover open file handle to <code>/</code> then you can
<code>fchdir</code> into it and <code>chroot(".")</code> chroot back into that shiot.</p>

<h2>Use capabilities</h2>

<p>UNIX privileges are all-or-nothing; Linux adds the notion of
capabilities. Use em. They&#8217;re granular n shit.</p>

<h2>Use virtual kernels</h2>

<p>Totally isolated. They have no concept of the raw kernel. Can&#8217;t access
it. Can&#8217;t do shiot.</p>

<p>BSD <code>jail()</code> addresses lots of this stuff. SO MANY FUCKING LOOP HOLES OH
MY GOD.</p>

<h2>Time of check, time of use</h2>

<p><code>access()</code> lets you query access of user to file. If you&#8217;re
set-id-to-root and you check if some unprivileged user has access to
something, maybe a symlink, and then in between that check and whatever
you end up destructively doing, the symlink is swapped to elsewhere,
then you might clobber some shittles. Malicious user could go nutso and
fire off a bunch of <code>SIGSTOP</code>s and change runtime environment to fuck
with a privileged program.</p>

<h2>Bounds checking</h2>

<p>strcpy babeh</p>

<p>Linux now has stack address randomization which randomizes the location
of the stack in an 8 MB range in VM. Seems cool. That&#8217;s why if you print
the pointer of a var int main in a test C program, it&#8217;s different each
run. This feels similar to stretches in bcrypt and any other security
measure that makes certain operations slower to counteract brute force
attackers.</p>

<h2>DoS denial of service</h2>

<p>Local variants include fork bombing.</p>

<p>Remote DoS more common, combat w:</p>

<ul>
<li>load throttling, dropping requests when overloaded</li>
<li>timeouts</li>
<li>(throttled) logging of overloads</li>
<li>don&#8217;t crash from unexpected loads</li>
<li>avoid algorithmic complexity attacks wherein a structure known to
handle a particular series of input might get fucked and consume lots
of resources. Not insecure, necessarily, but might fuck the fuck.</li>
</ul>


<h2>Shared libs</h2>

<p>Compile w <code>-g</code> nowadays since RAM and disk are cheap. Apparently this
info doesn&#8217;t affect performance?</p>

<p>A shared lib is a <code>.a</code> file composed of <code>.o</code> files, constructed with
<code>ar</code>, as in &#8220;library archives&#8221;.</p>

<pre><code>$ ar tv /usr/lib/liby.a
rw-r--r--       0/0            40 Aug 24 21:45 2013 __.SYMDEF SORTED
rw-r--r--       0/0           920 Aug 24 21:45 2013 main.o
rw-r--r--       0/0           912 Aug 24 21:45 2013 yyerror.o
</code></pre>

<p>Variants of <code>gcc</code> and static libs</p>

<pre><code># generate wat.o
gcc -c wat.c

# generate a.out with wat.o lib
gcc lol.c wat.o otherlibarchive.a

# search standard lib directories (like /usr/lib)
# looks for /usr/lib/libwoot.a
gcc -c wat.c -lwoot

# Search a non-standard directory
gcc -c wat.c -lwoot -L/borflex/snaggletooth
</code></pre>

<p>All of the .a archive and .o object files is added to executable size.
Seems bad. Also when the processes are run, VM is increased by that
much, even though it&#8217;s all redundant read-only text shit.</p>

<p>Ah. Static libs. vs Shared libs. Shared libs are fuckin, fuckin,
dynamic, maaaan.</p>

<p>Ah note that static and global variables obviously aren&#8217;t shared between
shared libs; each process gets a copy.</p>

<p>Pros:</p>

<ul>
<li>Large shared libs between small processes mean quicker process startup
time (though the first time the shared lib is loaded into VM is
obviously slow)</li>
<li>With some limitations, you can swap out libs without relinking</li>
</ul>


<p>Cons:</p>

<ul>
<li>complexity</li>
<li>shared libs must use position-independent code

<ul>
<li>PIC is a way of compiling shared libs so that the location of
functions, statics, globals, string constants, etc, can vary, and I
don&#8217;t understand it.</li>
</ul>
</li>
</ul>


<p>Modern shared lib linking format is ELF: Executable and Linking Format.</p>

<p><code>.so</code> is shared object, shared lib, as opposed to just <code>.o</code>.</p>

<p><code>ar</code> lets you add/remove <code>.o</code>s from library archives, but not so with
<code>.so</code>s.</p>

<p>You can use <code>nm</code> (name list util, i.e. symbol table util) to list
symbols.</p>

<p>Interesting, so given the following <code>wat.c</code>:</p>

<pre><code>int WAT;
</code></pre>

<p>I see <code>_WAT</code> appear in the symbol table:</p>

<pre><code>$ gcc -c wat.c &amp;&amp; nm wat.o
0000000000000004 C _WAT
</code></pre>

<p>Note that it wouldn&#8217;t appear if I&#8217;d made it <code>static</code> or <code>extern</code>. Shit
is cool.</p>

<p>So PIC adds a global offset table to help the dynamic linking describe
where shit is.</p>

<p>Hmm what if I use <code>g++</code> on a cpp file?</p>

<pre><code>$ g++ -c wat.cpp &amp;&amp; nm wat.o
0000000000000000 S _WAT
</code></pre>

<p>What&#8217;s the difference b/w S and C? C apparently means &#8220;common&#8221; symbol,
and S means a generic &#8220;none of the above&#8221; in the <code>man nm</code>.</p>

<p>You can take a dynamic/shared lib and compile statically though, if you
want it:</p>

<ul>
<li>less complexity</li>
<li>safer against .so upgrades</li>
</ul>


<p>So what&#8217;s the difference between a .so and a .dylib? Is that a mac
thing?</p>

<h2>Dynamically loaded libraries</h2>

<p>Deferred loading whenever some plugin is loaded. As in your code does
it. Seems pretty rad. Use <code>dlopen</code>. Search a function by name, invoke
it.</p>

<p>You can use env var <code>LD_DEBUG</code> to print out some shit.</p>

<h2>IPC</h2>

<ul>
<li>byte streams: pipes, fifos, datagram sockets</li>
<li>message: message queues; can&#8217;t read them part way, all or nothing</li>
<li>pseudoterminals</li>
</ul>


<p>Different from shared mem:</p>

<ul>
<li>reads are destructive for byte streams (but yes in some cases you can
seek, multicast/broadcast)</li>
</ul>


<p>MQ (POSIX or SystemV) messages have priorities, and deliver in different
order.</p>

<p>Ah now it&#8217;s coming back to me:</p>

<ul>
<li>UNIX domain sockets

<ul>
<li>ruby: <code>Socket.pair</code></li>
<li>datagram (no need to worry about byte streams/delimiters)</li>
</ul>
</li>
<li>Pipe

<ul>
<li>ruby: <code>IO.pipe</code></li>
<li>stream</li>
<li>flow control handled by kernel</li>
</ul>
</li>
</ul>


<h2>Pipes and FIFOs</h2>

<p>Pipes are old as SHIT. Oldest method of IPC, since 3rd edition of UNIX
in early 1970s.</p>

<p>THAT&#8217;s what that dude meant. Pipes vs Sockets. We use pipes, not
sockets.</p>

<p>Pipes are constrained to related processes, FIFOs can talk between all
processes.</p>

<p>Pipes are unidirectional.</p>

<p>Can&#8217;t reorder data, can&#8217;t random access w lseek.</p>

<p>Writes up to <code>PIPE_BUF</code> are atomic, Linux has <code>PIPE_BUF</code> at 4096. If
larger, kernel might break into pieces. This means that if you have two
processes writing more than <code>PIPE_BUF</code>, the final stream might be
interleaved with both messages (both processes will still be blocked on
<code>write</code> until everything is flushed out).</p>

<p>Partial writes can occur if a write larger than <code>PIPE_BUF</code> is
interrupted by a signal handler, and call comes back with the number of
bytes written. Pretty cool. Makes sense.</p>

<p>Writes will block if kernel buffer is full.</p>

<p>Pipes only work b/w related processes, which mean they must have a
common fork ancestor. Apparently there&#8217;s another way to pass file
descriptors. But I guess it&#8217;d need translating? Because there are
descriptors and open file descriptions, and the descriptor ints are
different between shits.</p>

<p>SIGPIPE is ignored by default, but when you write to broken pipe you get
SIGPIPE and EPIPE. Such pipe.</p>

<p><code>popen</code> runs a shell command, letting you establish a single pipe,
either r or w.</p>

<p>FIFOs are like pipes except that they&#8217;re special files in th file
system. Opening a read-only FIFO blocks until someone writes, and vice
versa.</p>

<p><code>tee(1)</code> writes to stdio AND some other file, which could be a fifo. So
you can split a stream and send it to other things. Pretty cool.</p>

<p>UNRELATED: <code>bash -c "sleep 5"</code> execs sleep 5; it doesn&#8217;t create a bash
process and then fork again, it just straight up execs. Whereas when
you&#8217;re running interactive bash, all commands are forked and then
exec&#8217;d.</p>

<p>Trick with IPC w servers is that there needs to be some known file name,
which opens the door to exploitation if you&#8217;re not careful.
FIFOs (and pipes) are byte streams (rather than unix domain datagram
sockets) so you have to</p>

<ol>
<li>Use some EOM delimiter</li>
<li>Message length header</li>
<li>Fixed message size</li>
</ol>


<p>and on top of all of that everything needs to be less than the pipe
buffer size constant or else kernel might interleave messages.</p>

<p>Pipes provide synchronization by e.g. opening a pipe, forking, having
the parent blocked on a <code>read()</code> until child processes have all done
their shit and closed their end of the pipe.</p>

<p><code>popen</code> has the same considerations as <code>system</code>.</p>

<h2>System V</h2>

<p>http://en.wikipedia.org/wiki/UNIX_System_V</p>

<p>One of first commercial versions of UNIX, from 1983. Today&#8217;s descendants
are AIX, Solaris, and HP-UX.</p>

<p>System V IPC</p>

<ul>
<li>MQs

<ul>
<li>delivered in order, but each message has type, so can be selected
out of order</li>
</ul>
</li>
<li>Semaphores

<ul>
<li>kernel forbids it from going below 0, sync technique</li>
</ul>
</li>
<li>Shared mem</li>
</ul>


<p>w System V IPC you create objects (like files but unlike files). Objects
have ids, but unlike fd&#8217;s, these are system wide.</p>

<p>Semaphores don&#8217;t error when you subtract below zero; rather the kernel
will block if you try and subtract, and it&#8217;ll come back alive when
someone increments.</p>

<h2>Considered Harmful</h2>

<p>http://en.wikipedia.org/wiki/Considered_harmful</p>

<p>Popular phrase originating from &#8220;Go To Statement Considered Harmful&#8221;, a
Djikstra thing.</p>

<h2>mmap</h2>

<p>Memory mapping can either use</p>

<ul>
<li>real files</li>
<li>anonymous zero-d out files</li>
</ul>


<p>Both are shareable between processes. By:</p>

<ul>
<li>mmapping to the same region of the same file</li>
<li>forking with a previously established handle to a</li>
</ul>


<p>Mapping to same file can be configured:</p>

<ul>
<li>private: writes don&#8217;t go through to file, so process changes to the
mapping are isolated from each other. Implemented by copy-on-write.</li>
<li>shared: durp</li>
</ul>


<h2>dev zero</h2>

<p><code>/dev/zero</code> gives you null bytes. Forever!</p>

<h2>dev urandom</h2>

<p>Write seeds, read values.</p>

<p>This isn&#8217;t perfect but kinda works:</p>

<pre><code>ruby -e 'puts File.open("/dev/urandom").gets(4).unpack("l")'
</code></pre>

<p>Extra credit if you can figure out what that ain&#8217;t right</p>

<h2>Get off the aaaaaaair I&#8217;m in the STEREO</h2>

<p><code>mmap</code> can overcommit swap space due to lazy committal. Useful for
implementing things like sparse arrays (a large array might only access
bits and pieces, and if separate by a page.</p>

<pre><code>$ getconf PAGESIZE
4096
</code></pre>

<p>Daatz cool. So if you have a bajillion thingeroo and it&#8217;s allocated on a
shit, then woooop there it is.</p>

<p>Overcommitals can ran the system out of memory at which point the kernel
starts killin&#8217; shit. The kernel code that does this is the OOM
Out-of-Memory killer.</p>

<p>OOM kills with <code>SIGKILL</code>, you can look at your score with a special file
in procfs.</p>

<h2>dscacheutil</h2>

<p>Directory Service Cache Utility. Supercedes <code>lookupd</code>.</p>

<h2>VM operations</h2>

<ul>
<li>protect: set read/write protections on a VM page

<ul>
<li>applies to mmapped files, etc</li>
</ul>
</li>
<li>mlock and mlockall: keep memory in RAM; useful since an attack vector
would be to consume lots of RAM, forcing some processes to write to
disk, and then reading from the swap space.

<ul>
<li>then again, suspend mode in laptops/desktops copied ram to disk, so
you&#8217;re effed.</li>
</ul>
</li>
</ul>


<h2>traceroute UDP vs ICMP</h2>

<p>This doesn&#8217;t work</p>

<pre><code>traceroute machty.com
</code></pre>

<p>This does</p>

<pre><code>traceroute -I machty.com
</code></pre>

<p>because it uses ICMP ECHO rather than a UDP packet (which I guess gets
dropped somewhere along the way? tis a Rackspace github pages HTTP
server so UDP wouldn&#8217;t make sense anyway), and ICMP ECHOs I guess are
considered more sensible and friendly?</p>

<p>I realized this was the issue since <code>ping machty.com</code>, which uses ICMP
ECHO, works just fine.</p>

<h2>Power of Attorney</h2>

<p>Written authorized permission for one person to act on behalf of
another, even if the other disagrees. Relevant in the legal sense, as
well as health care decisions up to and including terminating care and
life support.</p>

<h2>Darwin, Mach, OS X</h2>

<p><a href="http://en.wikipedia.org/wiki/Darwin_(operating_system">Darwin wiki</a>)</p>

<p>Darwin is an an open source operating system released by Apple in 2000</p>

<ul>
<li>Composed of NeXSTEP, BSD, other freeware</li>
<li>Largely compatible with POSIX but not certified.</li>
<li>SUSv3-compatible</li>
<li>Kernel is XNU

<ul>
<li>&#8220;X is Not Unix&#8221; - stupidest naming scheme ever</li>
<li>Hybrid kernel

<ul>
<li>Combo of micro and monolithic: http://en.wikipedia.org/wiki/Hybrid_kernel</li>
<li>Micro kernels are often &lt; 10,000 lines of code, do the bare
minimum, put more things into user space (or at least a higher
privilege ring)</li>
<li>Linus thinks hybrid is just monolithic and that it&#8217;s all just
marketing</li>
</ul>
</li>
<li>Combo of Mach (microkernel) + aspects of BSD (monolithic kernel)</li>
</ul>
</li>
<li>Closed source Cocoa et al frameworks are missing, so it can&#8217;t run mac
applications</li>
</ul>


<p>http://en.wikipedia.org/wiki/Microkernel</p>

<p>So I guess OS X is Darwin + Cocoa and a bunch of other layers.</p>

<h2>POSIX IPC</h2>

<ul>
<li>Simular to System V</li>
<li>API closer to classic UNIX everything-is-a-file abstraction</li>
<li>Simpler API in general</li>
</ul>


<h2>Semaphores: portable?</h2>

<p>Does anyone use operating system semaphores in open source? I mostly see
sockets and forking and what not for IPC in server code; does anyone
use semaphores?</p>

<p>Answer: yes, libuv does (the async IO lib that Node uses). I&#8217;m sure
plenty of other people do too.</p>

<h2>Node copies in all dependencies</h2>

<p>The Node repo doesn&#8217;t use submodules or anything like that; literally
the entire codebase of a dependency, whether v8 or libuv, is copied into
the <code>/deps</code> folder.</p>

<h2>WATCHLISTS</h2>

<p>Something to Chromium devs use that mark certain portions of code as of
interest to some reviewer, presumably to prevent undesirables from
getting into the system.</p>

<p>https://github.com/joyent/node/blob/master/deps/v8/WATCHLISTS</p>

<h2>File locking</h2>

<p>Used for synchronization. Could use semaphores but the kernel already
has file locking so why not use that for files?</p>

<p>stdio lib might cause issues with buffers and locks, so either follow
some rules about immediately flushing, or just skip stdio and use <code>read</code>
and <code>write</code>, or disable the buffering.</p>

<p>Locks apply to open file descriptions (in the shared open file table in
the kernel), not descriptors, so if you dup the descriptor and
explicitly unlock it, it applies to all duplicates (it doesn&#8217;t maintain
a count or anything).</p>

<p>You can only hold a lock associated with an fd. So if all fds are
closed, the lock is unlocked.</p>

<p>If you fork a lock, then it only takes one unlock (parent or child) to
release the lock. Wacky shiznittletons. But this lends to the pattern of
&#8220;parent establishes lock, forks, and closes file descriptor so that only
the child has the lock and open fd&#8221;.</p>

<p>Locks are preserved across exec unless close-on-exec on the fd and the
fd was the last one associated with the description.</p>

<p><code>flock()</code> downsides:</p>

<ul>
<li>only whole files lockable, hamper concurrency if processes would be
otherwise able to write to the same file in separate sections</li>
<li>can only place advisory locks.

<ul>
<li>advisory locks mean the kernel doesn&#8217;t actually help prevent
reads/writes; processes could ignore advisory locks and perform IO
anyway. Mandatory locks on the other hand&#8230;</li>
</ul>
</li>
</ul>


<h2>Record locking with fcntl</h2>

<p><code>fcntl</code> allows record-locking: locking anywhere from a byte to whole
file. &#8220;record-locking&#8221; might be the wrong word since files are just byte
streams with no concept of &#8220;records&#8221; beyond what their consuming
processes decide.</p>

<p><code>fcntl</code> just stands for &#8220;file control&#8221;. I get it confused with <code>ioctl</code>.
<code>fcntl</code> is control over descriptors in a process. Basically <code>fcntl</code>
behaves almost like a kernel sys call in that it&#8217;s just setting an
integer of the command you want to perform followed by varying numbers
of arguments based on the command. So you can do arbtrary, extensible
stuff that might not already be neatly contained within some other
single purpose wrapper fn.</p>

<p>SUSv3 requires record locking for regular files and permits record
locking for other file types even if it doesn&#8217;t really make sense.</p>

<p>Write locks are exclusive (to both read and write locks). Read locks
are not (they can be shared). Nothing new here.</p>

<p><code>fcntl</code> locking works by passing <code>flock</code> structure. You can lock
relative to current file pointer.</p>

<p><code>BADF</code> if you try and read/write lock a file on a file not open for
reading/writing.</p>

<p><code>len</code> of 0 means lock from whence onward (even if bytes are added to the
file).</p>

<ul>
<li>Unlocking always immediately succeeds.</li>
<li>different sections can be locked with different types</li>
<li>locks can be split by e.g. placing a write lock in the middle of a
larger range of read locks (3 locked regions are produced)</li>
</ul>


<p>The kernel prevents deadlocks between processes. (Similar rules apply
with thread mutexes). Chooses one of the processes and unblocks fcntl
with errno <code>EDEADLK</code>. Even circular deadlocks are detected. Pretty cool.</p>

<p>fcntl locking semantics:</p>

<ul>
<li>locks not inherited cross fork()</li>
<li>locks are inherited</li>
<li>threads share record locks</li>
<li>Record locks are pid+inodenum, so, weirdly, if you close an fd, all of
its locks are released, even if they came from other fds. Kind of
crazy? (This is architectural weak sauce; should have been file
handler rather than inode, but it&#8217;s now standardized).</li>
</ul>


<h2>Mandatory Locking</h2>

<p>File system must be mounted with mandatory locking enabled. And it must
be enabled on a per-file basis by enabling set-group-id and disabling
group execute permission. This yields a capital S in <code>ls -l</code> if you&#8217;ve
done it right.</p>

<ul>
<li>most file systems support it, cept for things like VFAT which don&#8217;t
have the required permission bits</li>
<li>locked file can still be deleted</li>
<li>privileges processes can&#8217;t override a mandatory lock. A malicious user
holding on to a well known public file is a form of DOS attack.</li>
<li>mandatory locks has performance hit against IO</li>
</ul>


<p>Probably best avoided?</p>

<p>tmpfs <code>/proc/locks</code> is useful.</p>

<h2>Pidfile</h2>

<p>Create a file and place write lock on it. Any other instances will try
and do the same thing, fail, and terminate under the assumption that a
running process already has a lock on it. Alternate approaches for
networked apps use bound ports to make this same decision.</p>

<p>Often such things live in <code>/var/run</code>. Conventional to write pid to file
and use extension <code>.pid</code>.</p>

<p>You can CLOEXEC a pidfile since some servers reinitialize themselves by
<code>exec</code>ing themselves. Seems crazy?</p>

<h2>Sockets</h2>

<ul>
<li>exist in communication domain

<ul>
<li>determines how socket is identified (form of &#8220;address&#8221;)</li>
<li>range of communication (intra-computer, internet, etc)</li>
</ul>
</li>
<li>Multiple domains of sockets

<ul>
<li>UNIX domain: <code>AF_UNIX</code>. Communication b/w processes on same host
(all within the kernel)</li>
<li>IP (v4 and v6)</li>
</ul>
</li>
</ul>


<p><code>PF_UNIX</code> vs <code>AF_UNIX</code> constants: Protocol Family vs Address Family:
often synonomous, but different due to intent to support protocols
having multiple families of addresses, but this hasn&#8217;t happened.</p>

<p>Every socket impl can be stream or datagram oriented.</p>

<p>Stream socket
- &#8220;reliable&#8221;&#8230;? UNIX Domain streaming sockets? Curious to know what the overlap is with TCP.
- bi-directional, as if a pair of pipes had been established.</p>

<h3>socket()</h3>

<p>Create a socket with</p>

<ul>
<li>domain: UNIX, IPv4, IPv6</li>
<li>type: STREAM vs DATAGRAM</li>
<li>protocol (usually 0 unless you&#8217;re doing funky things with raw sockets</li>
</ul>


<h3>bind()</h3>

<ul>
<li>Binds to an address / port</li>
<li>is actually <em>optional</em>: if you don&#8217;t call <code>bind()</code> you&#8217;ll use an
ephemeral port chosen by the kernel. Though this means you&#8217;d need to
use some other mechanism for publishing.</li>
</ul>


<h3>stream sockets</h3>

<ul>
<li>socket() creates</li>
<li>server

<ul>
<li>bind()s to a specific port/address</li>
<li>listen()s, signalling the kernel that it&#8217;s ready to accept</li>
<li>accept()s to actually process a connection</li>
</ul>
</li>
<li>clients connects()</li>
<li>both can send(), recv(), write(), read(), etc</li>
</ul>


<p>Stream sockets can be active or passive</p>

<ul>
<li>active: can be used in a connect() call

<ul>
<li>usually clients</li>
</ul>
</li>
<li>passive: has been marked to <code>listen()</code> for connections

<ul>
<li>usually servers</li>
</ul>
</li>
</ul>


<h3>listen(int sockfd, int backlog)</h3>

<p>Tell the kernel to start listening for connections. <code>backlog</code> is the
number of connections you&#8217;re willing to have in a pending state before
<code>accept()</code> is called.</p>

<h3>accept()</h3>

<p>Blocks til connection arrives.</p>

<ul>
<li>creates a <em>new</em> socket, the one that you can actually perform IO on
that makes it to the peer socket.

<ul>
<li>so why are they both called sockets? one that&#8217;s used for a
establishing connections, and another that&#8217;s used within an
established connection?</li>
</ul>
</li>
</ul>


<h3>connect()</h3>

<ul>
<li>if it fails, close socket, create a new one, try again</li>
</ul>


<h3>Stream socket IO</h3>

<p>Sockets are bi-directional, which means kernel must internally maintain
two buffers, one for each direction.</p>

<p>Writing into already-closed UNIX domain socket yields SIGPIPE/EPIPE.</p>

<p>For UNIX-domain sockets, if a <code>close</code> fails to propagate or some other
shit goes down (eg a crash), then we have no way of knowing; should
build some sort of ACK into the system (or just use TCP?).</p>

<p>(then again with TCP, what&#8217;s the API for knowing that a specific write
failed? Usually you write and then move on, right? Or do all write&#8217;s
block? Seems weird brah)</p>

<h3>Datagram flow</h3>

<ul>
<li>create socket w socket()</li>
<li>bind() to known address</li>
<li>sendto() to send a datagram</li>
<li>recvfrom() to recv (the from implies that the source address will be
included in the thing)</li>
<li>close() when socket no longer needed</li>
</ul>


<p><code>recvfrom</code> takes a length, returns a single datagram, silently truncating
if less than <code>length</code>. <code>recvmsg</code> is a variant that sets <code>MSG_TRUNC</code> if
truncation occurred.</p>

<h3>Datagram <code>connect()</code></h3>

<p>Seemingly at oods with the known properties of datagram sockets, you can
actually use <code>connect</code> w dgram socks to yield a &#8220;connected datagram
socket&#8221;. This does a few things:</p>

<ul>
<li>allows you to use <code>write</code> and <code>read</code> (rather than having to pass the
address in to <code>sendto</code> and <code>recvfrom</code> all the live long day)</li>
<li>only messages from the socket peer can be <code>read</code></li>
</ul>


<h2>UNIX Domain Sockets</h2>

<p>For bidirectionality you always have to create a pair.</p>

<p>You can create a UNIX domain socket via other means by providing a file
name. You must have permissions to create this thing, and it&#8217;s a special
file of <code>stat</code> time <code>IFSOCK</code></p>

<p>Let&#8217;s do some ruby</p>

<pre><code>require 'socket'
socket = Socket.new(:UNIX, :DGRAM)

# recall that pack_sockaddr_in refers to internet, `un` refers to UNIX
# address becomes a null-packed string. The Ruby socket lib is
# actually pretty low level when it comes to this shit.
address = Socket.pack_sockaddr_un('./wat.unix.socket') 

# this will actually create a special file in the directory
socket.bind(address)
</code></pre>

<p>Verify it&#8217;s a special file (lf -F puts <code>/</code> at the end of directories and
<code>=</code> at the end of sockets). Recall that this requires directory search
permissions since it needs to read the contents of the inode rather than
just the name of the files.</p>

<pre><code>$ ls -F ./wat.unix.socket
./wat.unix.socket=
</code></pre>

<p>Note the starting &#8220;s&#8221;:</p>

<pre><code>$ ls -l ./wat.unix.socket
srwxr-xr-x  1 machty  staff  0 Oct 26 17:57 ./wat.unix.socket
</code></pre>

<p>Also you can&#8217;t open a socket like a normal file:</p>

<pre><code>f = File.open('./wat.unix.socket')
Errno::EOPNOTSUPP: Operation not supported on socket - ./wat.unix.socket
</code></pre>

<p>Note that if you tried to bind to this socket again, you&#8217;d get
EADDRINUSE. This is interesting since you always see this error in the
context of IP address/port collisions, but in this case it&#8217;s a file
pathname; that&#8217;s because address is a generic concept, the specifics of
which are dependent on which socket domain you&#8217;re using</p>

<ul>
<li>IPv4/IPv6 addresses are IP addresses</li>
<li>UNIX is pathnames</li>
</ul>


<p>Both are packed into that address struct so that sys calls can
use/expect the same struct blob.</p>

<p>STREAM sockets have active and passive. <code>passive</code> has had <code>listen()</code>
called on it.</p>

<p>I still am unclear about whether sockets are bi-directional. I think
they are. But I don&#8217;t understand how to stop blocking on <code>read()</code>. Like,
does it block until the buffer is full? I&#8217;ll figure it out later.</p>

<p>Datagrams are supposed to be unreliable, possibly delivered twice, etc,
but UNIX domain datagrams are reliable since it&#8217;s just the kernel
delivering shit (it&#8217;s only difficult to guarantee reliability over a
network). So UNIX domain datagrams are delivered in order and not
duplicated.</p>

<p>Linux allows large datagrams, but some UNIX buffers might only be 2048,
so if you&#8217;re going for portable, go for small.</p>

<p>AHHH fucking dummy, I was using the wrong ruby method. <code>IO#read</code> reads
until you&#8217;ve filled a buffer (or EOF). What you want is <code>read_nonblock</code>.
&#8220;But why do you still need to pass a max len&#8230; this is ruby it can
handle whatever&#8221; I don&#8217;t know, probably because it ultimately translates
to a syscall which needs the length? Yes I think I&#8217;m right about this
after consulting <code>io.c</code> in MRI.</p>

<h2>Socket pairs</h2>

<p><code>socketpair()</code> or <code>Socket.pair</code> returns a pair of connected sockets.
Communication is bi-directional. Benefits over manually creating this
pair include not having to have a public address. Useful for forky IPC.
Very similar to pipe approach to fork IPC cept that pipes are
unidirectional.</p>

<pre><code>require 'socket.rb'
a,b = Socket.pair(:UNIX, :DGRAM)

a.write("bullshit\n")
puts b.read_nonblock(100)
b.write("horseass\n")
puts a.read_nonblock(100)
</code></pre>

<p>Note that this example works also if we used <code>:STREAM</code>.</p>

<h2>Internet domain sockets</h2>

<p>Unlike UNIX domain datagram, internet domain datagrams (implemented on
UDP) are</p>

<ul>
<li>Unreliable, might be delivered twice or not at all.</li>
<li>Sending doesn&#8217;t block if client buffer full, just dropped silently.</li>
</ul>


<p>Network byte order is big-endian.</p>

<p>Marshalling is tricky, dealing with endianness and what not. Converting
to text is often the easiest way to make sense of things, can debug w
<code>telnet</code>, etc.</p>

<p>Clients generally don&#8217;t <code>bind()</code> when making connections / sending
datagrams, in which case the kernel provides an ephemeral port.</p>

<h2>DNS</h2>

<p>Types of requests:</p>

<ul>
<li>recursive: server handles entire task of resolution, including talking
with other DNS servers. I believe though that once you&#8217;ve made the
request, the server can make an iterative approach</li>
<li>iterative: ask <code>.</code> for <code>com.</code>, <code>com.</code> NS for <code>machty.com.</code>,
<code>machty.com.</code> NS for <code>www.machty.com.</code>.</li>
</ul>


<p>There&#8217;s a local DNS server (<code>named</code> for Linux) that you can ask for
names, and it&#8217;ll do iterative. You ask it for recursive and it does
iterative.</p>

<p>Root name servers found via <code>dig . NS</code>, e.g. &#8220;gimme all the Name Server
entries for the root domain <code>.</code>.</p>

<p><code>/etc/resolv.conf</code> configures the DNS resolver, defines how partial
domain names are resolved (they get concatted with the <code>domain</code> entry in
<code>resolve.conf</code>.</p>

<pre><code># assuming `domain home`
ssh machty # machty.home
ssh machty.home
</code></pre>

<p>Both work. But this is just a local DNS nicety, not the distributed DNS
whitchajigger.</p>

<h2>TLDs</h2>

<p>First layer of nodes immediately under the root <code>.</code> node. Two types:</p>

<ul>
<li>generic: com, edu, net, org:</li>
<li>country: co, de:</li>
</ul>


<h2>ports</h2>

<p><code>/etc/services</code> maintains known port names.</p>

<h2>getsockname</h2>

<p>This is useful when you&#8217;re using ephemeral ports (listen without
bind).</p>

<pre><code>require 'socket'
s = Socket.net(:INET, :STREAM)
s.listen(5)
s.puts s.local_address.ip_port
</code></pre>

<p><code>local_address</code> is a wrapper around <code>s.getsockname</code>, which is some null
padded garbage. For UNIX domain sockets it&#8217;ll contain the file pathname.</p>

<p>This is the difference between sockets bound at 127.0.0.1:45454 and
0.0.0.0:45455, which shows that both the local ip and port are
represented in getsockname.</p>

<pre><code>=&gt; "\x10\x02\xB1\x8E\x7F\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00"
=&gt; "\x10\x02\xB1\x8F\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"
</code></pre>

<h2>UDP ephemeral port server</h2>

<p>So, <code>listen()</code> on a STREAM server will implicitly perform an ephemeral
port bind if you haven&#8217;t explicitly called <code>bind()</code> already; is it
possible to make an ephemeral UDP server, wherein the port is
ephemerally decided by the kernel? Well, yes, but you have to call bind;
the trick is to call it with port 0 to signify the kernel should pick
one for you. Why would you do this? I don&#8217;t know; probably if you wanted
some anonymous service that no attacker could guess the port of ahead of
time? But you&#8217;d still need to advertise your port somehow.</p>

<h2>UNIX v internet domain sockets</h2>

<ul>
<li>UNIX faster on some implementations</li>
<li>UNIX DGRAM delivery is reliable</li>
<li>UNIX lets you use file permissions for authentication</li>
<li>UNIX lets you pass open file descriptors when forking, etc</li>
</ul>


<p>But often internet sockets are the way to go since they work locally and
remotely.</p>

<h2>Load-balancing with DNS</h2>

<p>If you provide multiple IPs to the same DNS record, you get round-robin
resolution, babeh.</p>

<h2>inetd; inet daemon</h2>

<p>If you have infrequently used daemons that would hog a process table
entry and resources, why not give their sockets to inetd and inetd will
spin up your servers when they need them.</p>

<ul>
<li>number of processes reduced</li>
<li>inetd does the server-y stuff that its clients would otherwise have to
write.</li>
</ul>


<p>aka internet superserver.</p>

<h2>Who responds to ICMP messages?</h2>

<p>Just routers/gateways. Not apps, unless you&#8217;re opening raw sockets n shit.</p>

<p>For instance you can&#8217;t spin up a rails server on localhost:3000 and
expect <code>ping localhost:3000</code> to work, because it&#8217;s only listening to
TCP/UDP sockets. AHHHH I am saying so much ignorance: here&#8217;s the truth:</p>

<p>ICMP has no concept of port. Port is TCP/UDP concept. ALWAYS REMEMBER
THAT. ICMP is IP-only. Whatever responds to / notices an incoming ICMP
request must only have an IP address, and not expect to use a port in
any way. So gateways/hosts/routers will respond to ICMP. You can ping
your router. You can ping gateways. You can traceroute all the way to a
website. And you can ping localhost, because loopback is a gateway.</p>

<p>So how can you write a program that listens for ICMP requests? You need
a raw socket, which requires sudo:</p>

<pre><code>require 'socket'
require 'base64'

rsock = Socket.new(:INET, :RAW)

loop do
  s = rsock.recv(1024)
  enc = Base64.encode64(s)
  puts enc
end
</code></pre>

<p>Again, no concept of port; there&#8217;s no <code>bind</code> here nor does the kernel
assign an ephemeral port; it&#8217;s all raw sockets babeh.</p>

<p>From <code>ping(8)</code>:</p>

<pre><code>The ping utility uses the ICMP protocol's mandatory ECHO_REQUEST datagram to
elicit an ICMP ECHO_RESPONSE from a host or gateway.
</code></pre>

<p>So <code>ping</code> pings gateways/hosts, not servers.</p>

<h2>ECONNREFUSED</h2>

<p>This is caused by ICMP, if it decides it wants to send anything. Shit
gets dropped.</p>

<h2>MTU: Maximum Transmission Unit</h2>

<p>This is what causes IP packets to be sliced and diced.</p>

<p>http://en.wikipedia.org/wiki/Maximum_transmission_unit</p>

<p>The min MTU between two endpoints is the path MTU. The minimum allowable
MTU is well-defined, I forget what it is, but I think there&#8217;s one
guaranteed by IP.</p>

<p>You can find the path MTU (per TCP/UDP RFC) by setting do not fragment
flag and seeing if ICMP sends back a failure due to datagram size. That
said, lots of gateways silently drop ICMP to prevent DoS:</p>

<pre><code>http://en.wikipedia.org/wiki/Black_hole_(networking)
</code></pre>

<p>People might black hole to prevent pings; you can still send well formed
TCP/UDP requests but it won&#8217;t respond to ICMP <code>ECHO_REQUEST</code>s or
anything like that.</p>

<h2>Packet-switching</h2>

<blockquote><p>Packet switching is a digital networking communications method that groups all transmitted data – regardless of content, type, or structure – into suitably sized blocks, called packets.</p></blockquote>

<p>Switching I believe is how two endpoints of a physical link decide how
to send data between each other; they could either do circuit switching
and have a dedicated connection until the connection was over,
or you can packet switch, whereby you
break the communication into small packets that send all at once (and IP
might slice them to fit the link&#8217;s MTU).</p>

<h2>C Standard Library</h2>

<p>I was confused for a moment about where <code>write()</code> and <code>read()</code> and these
low level syscall wrappers lived&#8230; were they considered part of the C
standard library? Or were they in some bare-bones thing in some other
category of code that operating systems provided?</p>

<p>Naw, it&#8217;s all C Standard Library. It&#8217;s just that (I think) <code>write</code> and
<code>read</code> are so low level as to not be considered part of <code>stdio</code>, the IO
subset of the C standard lib.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faily Burnhole]]></title>
    <link href="http://machty.github.com/blog/2014/10/10/faily-burnhole/"/>
    <updated>2014-10-10T10:24:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/10/faily-burnhole</id>
    <content type="html"><![CDATA[<h2>(postgres) Transactions</h2>

<p>http://www.postgresql.org/docs/8.3/static/tutorial-transactions.html</p>

<ul>
<li>group sequence of SQL into atomic (all-or-nothing) operations</li>
<li>useful for preventing invalid state of a crash occurs in the middle of
a sequence of queries</li>
<li>also, transactions are isolated from each other; one in-progress
transaction won&#8217;t see the partially complete transaction process of
another</li>
<li>postgres implicitly wraps every statement in a transaction block if
you haven&#8217;t.</li>
<li>transaction is <code>BEGIN</code> followed by <code>COMMIT</code> or <code>ROLLBACK</code>.</li>
<li>savepoints allow for finer granularity:

<ul>
<li><code>BEGIN</code></li>
<li><code>UPDATE ...</code></li>
<li><code>SAVEPOINT wat</code></li>
<li><code>UPDATE ...</code></li>
<li><code>ROLLBACK TO wat</code></li>
<li><code>UPDATE ...</code></li>
<li><code>COMMIT</code></li>
</ul>
</li>
</ul>


<p>So how can I use this for nested world environments&#8230;</p>

<h2>Sharing raised error with <code>ensure</code> block</h2>

<p>Saw a Ruby thing I hadn&#8217;t seen before at
https://github.com/rails/rails/blob/master/activerecord/lib/active_record/connection_adapters/abstract/transaction.rb#L186-L205</p>

<p>Basically, a <code>rescue</code> must precede an optional <code>ensure</code> block, but whats
cool is that the error caught be the rescue block is available to the
<code>ensure</code> block. Stripped down, the basic construct is</p>

<pre><code>def wat
  do_something_that_might_fail
  rescue Exception =&gt; e
    # do some error handling
  ensure
    # do generic stuff
    if e
      # continue to do some error handling
    end
end
</code></pre>

<h2>ramdisk + ember-cli</h2>

<p>tmpfs is a form of ramdisk. ramdisk is just a general purpose term for
when a virtual file system is provided but the data just lives on memory
rather than hard disk.</p>

<p>Some questions I need to answer:</p>

<ul>
<li>are there different rules about paging when you create a ramdisk? Does
it prevent its own memory from paging to disk any more than other
processes&#8217; virtual memory?</li>
<li>can you control paging in general? (other than going out of your way
to regularly read memory)</li>
</ul>


<p>My ember-cli builds for a decently complex project are about 2s, which
is waaaay better than the 20s they were a month ago. But I can get my 2s
down to 1s my mounting a ramdisk and symlinking my tmp folder to it:</p>

<pre><code>diskutil erasevolume HFS+ 'RAM Disk' `hdiutil attach -nomount ram://8388608`
rm -rf tmp
mkdir /Volumes/RAM\ Disk/tmp
ln -s /Volumes/RAM\ Disk/tmp tmp
</code></pre>

<h2>Photoshop and tmpfs</h2>

<p>http://www.tekrevue.com/tip/how-to-create-a-4gbs-ram-disk-in-mac-os-x/#comment-904649071</p>

<p>This comment speaks of the diminishing returns of using a ramdisk as
scratch space for Photoshop now that Photoshop is 64bit.</p>

<p>If your system is 32bit, it means running processes can only access
virtual mem addresses from 0 to 2<sup>32-1</sup> (0xFFFFFFFF or 4294967293).
On 64 bit systems, that max address is doubled to 0xFFFFFFFFFFFFFFFF, or
1.84 * 10<sup>19.</sup> If you had more than 4gig of RAM on a 32bit system, each
process would still be limited to a max of 4gig memory usage simply due
to the fact that it can&#8217;t reference memory addresses higher than 4gig.
But Photoshop often needs more than 4gig, so what&#8217;s the solution?
Scratch disks.</p>

<p>Scratch disks are like virtual memory implemented in user space. If you
need to store more temporary data than you can put into memory, just
throw it on scratch disk. The problem is that scratch disks</p>

<p>I could totally be bullshitting right now. I could be wrong. But seems
right? Less wrong?</p>

<h2>ln</h2>

<p>I never remember the argument order for <code>ln</code>.</p>

<p>It is FUCKING LEFTWARD. The thing you create on the right points to the
thing on the left.</p>

<pre><code>ln EXISTING_THING LINK

&lt;-------------
</code></pre>

<p>The thing that confused me was that i kept thinking <code>-s</code> was an option
that accepted an argument. It is not! It&#8217;s just an argless option. <code>ln</code>
always has the format of new thing on right points to left. NEVER
FORGOT.</p>

<h2>Dockyard&#8217;s fixtory</h2>

<p>https://github.com/dockyard/fixtory</p>

<p>Convenient way to populate groups of fixtures, maintain references to
the created instances, etc.</p>

<h2>Execute/search permission</h2>

<p>http://content.hccfl.edu/pollock/AUnix1/FilePermissions.htm</p>

<p>Directories have different levels of &#8220;readability&#8221; based on their read /
execute flags. A file record within a directory has a name and an inode.</p>

<p>You can read names of files in a directory (<code>ls</code>) if you have read
permissions on that directory, and permissions denied if not.</p>

<pre><code>$ ls wat
lol
$ chmod -r wat
$ ls wat
ls: wat: Permission denied
</code></pre>

<p>If you want to access the inode in any way, like to read file
attributes (permissions et al), or to read its contents, you&#8217;ll need
execute (search) permissions on the directory.</p>

<pre><code>$ chmod +r wat
$ chmod -x wat
$ ls wat
lol
$ ls -l wat
ls: wat: Permission denied
$ ls -i wat
ls: wat: Permission denied
$ cat wat/lol
cat: wat/lol: Permission denied
$ chmod +x wat
$ ls wat
lol
</code></pre>

<p>Then of course you need file read permissions to access the contents.</p>

<p>So if you&#8217;re using absolute file paths, every directory along the way
needs execute (search) permissions. But in the unusual case that you&#8217;re
able to <code>cd</code> into a directory (thus setting the process&#8217;s current
working directory, something the kernel tracks to control where relative
file lookups occur from) before a parent directory&#8217;s read permissions
are revoked, you&#8217;re grandfathered in if you&#8217;re using relative file perms
from that point on, unless of course the path you provide steps out and
in again.</p>

<pre><code>$ chmod -x ..
$ cat ../inner/woot
../inner/woot: Permission denied
$ chmod +x ..
$ cat ../inner/woot
lol
</code></pre>

<p>Basically, there&#8217;s no concept of deeper and deeper folders, just
following pointers (<code>..</code> and <code>.</code> included), and pointers have permissions
that must be adhered to in order to traverse them.</p>

<h2>dev id and inode</h2>

<p>Device id + inode uniquely identify a file across all file systems.</p>

<p>If the file itself is a special device file, the inode for that file
contains the major/minor id of the device.</p>

<h2><code>utimes(2)</code> for manually changing access/modify timestamps</h2>

<p>You can change the last modified time and last accessed time of a file.
One use case is how <code>tar</code> and <code>unzip</code> preserve the original timestamps
of archived files when they are unarchived.</p>

<p><code>ctime</code> is changed to now when you run <code>utimes</code>.</p>

<h2>group ID of newly created files</h2>

<p>This is governed by 1. how you&#8217;ve configured the file system and 2.
whether set-group-id is set on parent directory.</p>

<p>I think this is one of the rare cases where set-group-id is meaningfully
used on files rather than directories.</p>

<h2>File deletion doesn&#8217;t require file permissions</h2>

<p>You can delete a file without having any permissions on it, since all
you&#8217;re doing is modifying a parent directory.</p>

<h2>Why not have slashes for directories by default?</h2>

<pre><code>$ ls
inner   lol
$ ls -F
inner/  lol
</code></pre>

<p>Wouldn&#8217;t that slash on <code>inner/</code> be convenient to always display?
Why isn&#8217;t that a default?</p>

<p>Answer: because knowing whether a file is a normal file or a directory
requires execute/search permissions on the directory that file lives in.
You can list file/directory names with next to no permissions, but once
you start looking at inode information (such as whether a file is a
normal file or a directory), you&#8217;ll need search/execute perms, and the
default behavior of <code>ls</code> is much less likely to get permission denied
errors.</p>

<p>So thinketh I. Could be wrong.</p>

<h2>Accessing files w/o read perms</h2>

<p>Even if a directory has no read permissions, if it has execute
permissions, then files inside can still be accessed, but their names
must be known ahead of time since you won&#8217;t be able to list the file
names in that directory.</p>

<p>Seems crazy, but useful for when you have a public directory and don&#8217;t
want to list all the contents.</p>

<p>I guess this also means it&#8217;s not possible to hide a single
file/directory within a directory that has read permissions; i.e. if you
can list one file in a directory, you can list them all.</p>

<p>Neat tidbit: there is (was?) a process accounting flag ASU that&#8217;s set if
the process made use of superuser privileges, and the kernel will only
check superuser permissions last in case it can&#8217;t get privileges any
other way than so that it can avoid unnecessarily setting ASU.</p>

<p>It&#8217;s possible to wind up in a situation where an owner has less access
than a group s/he belongs to, due to the fact that the kernel
permissions algorithm will stop once it finds a matching user or group;
it won&#8217;t try to find read (etc) permissions in all matching permission
bits, it&#8217;ll just check the first set of bits it matches.</p>

<p>ACLs (Access Control Lists) allow for per-user and per-group file permissions.</p>

<h2>Sticky bit</h2>

<p>Used to be a signal for a process to stay in swap, now it&#8217;s a signal
set on directories that, when set, prevents non owners and non write
accessors from being able to delete files they don&#8217;t own.</p>

<h2>umask</h2>

<p>A mask, attached to a process, that negates certain permissions.</p>

<h2>EA: extended attrs</h2>

<p>User EAs can only be applied to files and folders, because</p>

<ul>
<li>symlinks have no meaningful permissions, which means anyone in the
world could clutter symlink w EAs if it were possible</li>
</ul>


<h2>executable permissions</h2>

<p>If no one has executable permissions on a file, then not even <code>sudo</code> can
get it to run as such.</p>

<pre><code>$ chmod -x exe
$ ./exe
-bash: ./exe: Permission denied
$ sudo ./exe
sudo: ./exe: command not found
$ chmod o+x ./exe
$ sudo ./exe
wat
</code></pre>

<p>I guess another way of thinking about it is that a file isn&#8217;t an
executable if no one has permissions to it.</p>

<h2>Processes, cwd, directories, etc</h2>

<p>Kernel tracks two attributes on running processes:</p>

<ul>
<li>root: the point from which absolute paths are looked up. Most
processes have <code>/</code></li>
<li>cwd: the directory from which relative paths are looked up</li>
</ul>


<p>(you can use <code>chroot</code> to change the root directory for a process)</p>

<p>A directory is different from a file in that 1) it is specially marked
as a directory in its inode attrs (remember this is why <code>ls -F</code>
requires execute permissions on the parent directory), and 2)
its contents are a table of filenames (read perms) and inodes
(execute/search perms).</p>

<p>You can&#8217;t use <code>read</code> on a directory, you must use other system calls,
hence <code>cat: wat: Is a directory</code>. Basically, the kernel restricts you to
certain syscalls to modify the directory; you can&#8217;t <code>read</code> / <code>write</code> it
the way you would a normal file. The error that gets thrown is <code>EISDIR</code>.</p>

<h2>inode table</h2>

<p>A filesystem has a root inode table that inode entries in directories
will refer to. The root inode table has the following features:</p>

<ul>
<li>index 0 is unused since a 0 is how directory inodes signal that an
entry is unused</li>
<li>index 1 tracks bad blocks on the disk</li>
<li><p>index 2 is <code>/</code></p>

<p>  $ ls -di /
  2 /</p></li>
</ul>


<p>Woot woot! This stuff is so awesome.</p>

<p>So it&#8217;s in this shared inode table that perms are stored. So if you hard
link, any changes to the permissions of one link affects the other:</p>

<pre><code>permstest :: touch shit1 &amp;&amp; ln shit1 shit2
permstest :: ls -l shit*
-rw-r--r--  2 machty  staff  0 Oct 14 10:40 shit1
-rw-r--r--  2 machty  staff  0 Oct 14 10:40 shit2
permstest :: chmod -rwx shit1
permstest :: ls -l shit*
----------  2 machty  staff  0 Oct 14 10:40 shit1
----------  2 machty  staff  0 Oct 14 10:40 shit2
</code></pre>

<p>TODO: Is this the same w ACLs permissions?</p>

<p>Hardlinks not supported by MS VFAT, but yes to NTFS.</p>

<p>inodes don&#8217;t have filenames&#8230; that&#8217;s only in the directory&#8217;s table;
remember that gzip and gunzip (and some other utilities) share the same
inode, and depending on <code>argv[0]</code>, do different things:</p>

<pre><code>$ ls -i `which gunzip` `which gzip`
343041 /usr/bin/gunzip  343041 /usr/bin/gzip
</code></pre>

<p>The <code>rm</code> algorithm is simple: decrement the inode counter, and if 0,
then deallocate. Reference counting, bitchez.</p>

<p>Based on the above layout, you can&#8217;t (easily) get the filename of an
open file, because multiple files might point to that inode, not to
mention that even if you wanted to loop through the file system&#8217;s inode
table, you still wouldn&#8217;t have the file name, just a matching inode
number.</p>

<p>Hardlinks can&#8217;t be created cross-file-system (since they&#8217;re just inode
numbers. symlinks can handle this though).</p>

<p>Hardlinks can&#8217;t (cept Mac due to Time Machine shit) link to directories
due to circular dependencies (e.g. a nested folder hardlinking to an
ancestor folder).</p>

<p>Symlinks can be displayed with trailing <code>@</code> via <code>ls -F</code>. They don&#8217;t bump
the inode counter. They can be created to point to nothing, or can have
their target files deleted (dangling). Like all the other extra data
displayed by <code>ls -F</code>, search perms are needed on the parent directory.
Symlinks have a symlink file type stored in the inode.</p>

<p>Tools are smart enough to avoid cirular dependencies when symlinks link
to directories, because they actually can know they&#8217;re dealing with a
symlink; a multi-hardlink doesn&#8217;t look any different than anything else.</p>

<pre><code>$ ln -s ./circle ./circle
$ ./circle
-bash: ./circle: Too many levels of symbolic links
</code></pre>

<p>For long ass symlinks (> 60 bytes), a data block (where file data is
normally stored) is used to store the target link. But lots of file
systems (<code>ext2</code>+ et al) have an optimization where if the target is less
than 60 bytes, the target is written right into the inode structure
where the data block pointers are usually kept. I tried reproducing this
with a test case of a bunch of <code>stat</code> syscalls on a small-target symlink
and a 60+ byte symlink and it didn&#8217;t make a difference, but that could
just be due to disk cache on behalf of the kernel; the kernel kept the
inode data in RAM so it was just as fast, but if the cache was cold then
symlink lookups should be a lot faster.</p>

<h2>unlinks don&#8217;t complete while files are open</h2>

<p>If there are open file descriptors, an unlinked file won&#8217;t be totally
deleted. Hence the common practice to create and unlink a temporary file
so that you&#8217;re guaranteed when the handle closes, the file will be
totally deleted. This also makes it possible to happily unlink a file
and not worry whether someone else is using it&#8230; but don&#8217;t we sometimes
get file-in-use errors, like &#8220;can&#8217;t delete this file since it&#8217;s being
used be process X&#8221; stuff? Ah, you can delete a file out from under
someone else. They might still save it if they&#8217;ve buffered the data
(think Vim&#8217;s OMG THE FILE NO LONGER EXISTS warning).</p>

<h2>NTFS vs FAT32</h2>

<p>Windows 7 prefers NTFS. Earlier versions prefer FAT. FAT doesn&#8217;t have
security-related features (I&#8217;m guessing permissions?). FAT was
originally designed for floppies, and then used on hard drives. It&#8217;s
still often used for thumb drives.</p>

<h2>Transducers</h2>

<p>http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming</p>

<p>http://jlongster.com/Transducers.js&#8211;A-JavaScript-Library-for-Transformation-of-Data</p>

<p>God damn it. Guess I have to learn a new thing.</p>

<h2>Reentrant</h2>

<p><a href="wiki">http://en.wikipedia.org/wiki/Reentrancy_(computing)</a></p>

<p>Reentrancy means the function can be interrupted and re-entered without
harm. Actually I think more accurately it means it can be re-entered,
even within the same thread, without causing issue. A function can be
threadsafe and not re-entrant. e.g.</p>

<pre><code>trap :INT do
  puts "int"
end
Process.kill :INT, Process.pid
puts "boom"
</code></pre>

<p>This yields a <code>deadlock; recursive locking (ThreadError)</code> every time,
the same that would happen if you tried to double lock a mutex within
the same thread. The reason is that <code>puts</code> calls <code>write</code>, and <code>write</code>
mutexes around Ruby&#8217;s IO buffer. The signal handler doesn&#8217;t get called
synchronously at <code>Process.kill</code>, but for whatever reason, the kernel
decides to fire the signal handler during <code>write</code> in <code>puts "boom"</code>,
after the write mutex has already been locked. Then the signal handler
runs, tries to the do the same lock, and BOOM. Weird shit. But proof
that <code>puts</code> and <code>write</code> aren&#8217;t re-entrant even though they&#8217;re thread
safe. If you did <code>$stdout.sync = true</code>, they would be reentrant though
and the above error would go away.</p>

<h2>Process cwd</h2>

<p>Hmmm a process&#8217;s cwd does contain a pathname to the cwd; I thought it&#8217;d
just be some kind of inode pointer and not know the name? That said, the
beginning of the path is truncated if there&#8217;s not enough space, though
this is probably rare. That said, you can <code>readlink</code> the symlink at
<code>/proc/PID/cwd</code> (this doesn&#8217;t exist on Mac though).</p>

<h2><code>chroot</code> jail</h2>

<ul>
<li>FTP uses this so that anonymous users can&#8217;t just browse the whole
system</li>
<li>Linux doesn&#8217;t have hard-linked directories, but some UNIX varieties
do, and a hardlink outside of a <code>chroot</code> jail will compromise the jail</li>
<li>Most programs can&#8217;t run in <code>chroot</code> jail because they rely on
dynamically linked libraries, and the link targets are often absolute
and expect <code>/usr</code> to live.</li>
</ul>


<h2>Symlink perfs</h2>

<p>I was enouraged by the fact that on ext2+, symlinks whose targets are
less than 60 chars long get written into the inode rather than into a
separate data block (it&#8217;s basically written where the data block
pointers are usually saved, which amounts to ~60 bytes). Sooo I did a
quick benchmark to see if ember-cli would benefit, and alas (and go
figure?) 60 char perfs that apply to ext2 (and 3 and 4) don&#8217;t apply to
OS X&#8217;s HFS+.</p>

<p>I ran a test creating 100,000 symlinks, one where each symlink target
was 59 chars, and another with 60, and the difference is dramatic:</p>

<pre><code>Linux, ext4, 60 char symlink, 100,000

real    0m19.642s
user    0m0.468s
sys     0m3.988s

Linux, ext4, 59 char symlink, 100,000

real    0m2.005s
user    0m0.360s
sys     0m1.436s
</code></pre>

<p>I&#8217;m sad, because most ember-cli / broccoli symlinks are above the 60
limit, and this would have brought us some insane perfs, but alas alas
alas, most people do ember-cli work on OS X, not Linux. Oh god damn
well.</p>

<h2>ARS Technica on why HFS+ blows</h2>

<p>http://arstechnica.com/apple/2011/07/mac-os-x-10-7/12/#file-system</p>

<ul>
<li>HFS is from 1985</li>
<li>HFS+ (aka Mac OS Extended) released in 1998.</li>
<li>OS X (2001) unfortunately kept HFS+ around even though it sucks</li>
</ul>


<p>So yusuck:</p>

<ul>
<li>16 bit processing resolution in Mac&#8217;s implementation (Motorola 68000
hangover). Not really sure what this means, not sure why if the data
is 32+ bit why 16 is the resolution? So many things I don&#8217;t know.</li>
<li>Time resolution of HFS+ is only a second, shitty for file timing logic
in <code>make</code>, et al. Most modern fs&#8217;s have nanosecond resolution.</li>
<li>Global file lock on metadata; only one process can write to a file at a time. NOPE
that&#8217;s obviously incorrect. It means only one process can update the
file system at a time&#8230; I don&#8217;t understand? Can&#8217;t add multiple files to
a directory at the same time? Metadata?</li>
<li><p>No sparse files. The following ruby code will instantly eat a gig on
HFS+. On ext2 you could still use that gig space for other files, and
intermediate blocks would only be allocated as needed:</p>

<p>  File.open(&#8216;bigassfile_DELETE_ME&#8217;, &#8216;w&#8217;) do |f|</p>

<pre><code>f.seek(1000000000)
f.puts('l')
</code></pre>

<p>  end</p></li>
</ul>


<p>Result: Finder reports -=1gig of freespace, as expected (if there are no
sparse files). Weird though: <code>ls -l</code> reports only 87 block usage; I
would have expected a lot more give the fact that the file system is
allegedly allocating a bunch of zeroes? ANSWER: I think it&#8217;s because OS
X uses a VFS that pretends to support sparse files over HFS+, which
doesn&#8217;t, but I need to confirm this.</p>

<ul>
<li>Hard links to directories is supported, which is kinda weird.
Internally it works by hiding them in hidden a hidden directory (my
guess is to avoid cycles?)</li>
<li>No concern about data integrity. Easy for meta data to get corrupted.
Report showed that ~30% of HFS+ systems had mismatched checksum. Data
loss likely.</li>
</ul>


<h2>Endianness</h2>

<p>Not to be confused with &#8220;bit-endianness&#8221;, the atomic unit Endianness
as people normally talk about it is the byte, rather than the order of
bits. Endianness is concerned with the byte order in a &#8220;word&#8221; (the unit
of a CPU processing, e.g. 32 bit machine vs 64 bit machine refers to the
word size and has implications on the size limits of integers, memory
addresses, etc.). Whether the most significant bits appear at the
beginning or end of a word is the endianness, big-endian or
little-endian. You have to care about this stuff if you&#8217;re manually
converting bytestreams between different systems with different
endianness. Some processors are bi-endian.</p>

<h2>POODLE attack</h2>

<p>https://tools.ietf.org/html/draft-ietf-tls-downgrade-scsv-00</p>

<p>SSL 3.0 preceded TLS 1.0 and has vulnerabilities that allow an attacker
to see plaintext communication. Some clients go beyond the TLS handshake
specifications and will fall back to SSL 3.0 if a TLS handshake fails in
order to support the case where they&#8217;re talking to a legacy ass server
that runs SSL 3.0. A man in the middle can fuck with a handshake,
causing it to fail, and causing the TLS client to try communication with
SSL 3.0, and many servers supporting legacy SSL 3.0 will just happily
fall back to that, exposing the connection to SSL 3.0&#8217;s weaknesses.</p>

<p>The solution is for clients falling back to SSL 3.0 to send a backwards
compatible cipher suggestion that newer patched servers will look out
for and will treat as a signal that &#8220;this SSL 3.0 connection attempt is
only a fallback, and you shouldn&#8217;t accept this and start speaking in
3.0&#8221;. Legacy servers will just see it as a suggested cipher that they
don&#8217;t understand and will just choose another cipher that they do, and
continue communicating in their legacy 3.0 way. Case in point: upgrade
your servers.</p>

<p>The name for the patch is the TLS Signaling Cipher Suite Value (SCSV).</p>

<h2>inotify config</h2>

<p>On Linux you can configure aspects of <code>inotify</code> by writing values in to
various files in <code>/proc/sys/fs/inotify</code>. Question: why this format of
config? Why a single value in each? Why not <code>sysctl</code>?</p>

<h2>ssh escape char</h2>

<p><code>~</code> at the beginning of a line is an ssh escape char; if your connection
is hung, you can do <code>~.</code> (at the beginning of a line; you can force this
by pressing enter) and that&#8217;ll disconnect.</p>

<pre><code>Supported escape sequences:
 ~.   - terminate connection (and any multiplexed sessions)
 ~B   - send a BREAK to the remote system
 ~C   - open a command line
 ~R   - request rekey
 ~V/v - decrease/increase verbosity (LogLevel)
 ~^Z  - suspend ssh
 ~#   - list forwarded connections
 ~&amp;   - background ssh (when waiting for connections to terminate)
 ~?   - this message
 ~~   - send the escape character by typing it twice
</code></pre>

<p>Mmmm command line, this seems interesting if you&#8217;re using ssh for
proxying:</p>

<pre><code> ~C      Open command line.  Currently this allows the addition of port for-
         wardings using the -L, -R and -D options (see above).  It also
         allows the cancellation of existing port-forwardings with
         -KL[bind_address:]port for local, -KR[bind_address:]port for remote
         and -KD[bind_address:]port for dynamic port-forwardings.  !command
         allows the user to execute a local command if the PermitLocalCommand
         option is enabled in ssh_config(5).  Basic help is available, using
         the -h option.
</code></pre>

<h2>Signals</h2>

<p>Signals are sync; between generation and delivery, they are pending.
Most often sent immediately if running, or once it&#8217;s next scheduled to
run. Signals can be temporarily blocked by signal masks til the process
is ready to handle them (to prevent rude interruptions). Signals can
yield the following reponse from processes:</p>

<ul>
<li>ignore: kernel never tells process about it</li>
<li>terminate: &#8220;abnormal&#8221; (non <code>exit</code>) termination</li>
<li>core dump and terminate: core dump is image of virtual memory</li>
<li>stop execution (suspend)</li>
<li>resume execution</li>
</ul>


<p>I guess the thing I&#8217;ve learned here is that by setting signal masks,
you&#8217;re telling the kernel ahead of time what should happen when signals
are sent to this process, rather than setting some process-level
handlers that server as callbacks when these signals arrive;
particularly ignore &#8211; I thought it&#8217;d still make it to some low level
library in the process, but sounds like it&#8217;s rather just the kernel
knowing ahead of time not to even send it.</p>

<p>You can&#8217;t tell a signal that doesn&#8217;t by default terminate/core dump to
terminate/core dump, but you can install a signal handler and then call
<code>abort()</code> or whatever to send yourself a signal that <em>will</em> cause such a
thing to happen.</p>

<p>On Linux you can look up a process&#8217;s signal mask via <code>/proc/PID/status</code>.
You can also use <code>ps -p $$ -o sigmask,sig</code> to get info about signal
masks.</p>

<p><code>SEGV</code> means segmentation violation.</p>

<p><code>SIGBUS</code> is like <code>SIGSEGV</code> but in very particular situations, such as
reading past the end of a memory mapped file. BUS is hipster SEGV.</p>

<p>Send signals with <code>kill</code> (whose name is a hangover from a time in which
most if not all signals terminated a process). Different <code>pid</code> values do
different things:</p>

<ul>
<li>positive: send signal to pid</li>
<li><code>0</code>: send signal to processing group of calling process, including
calling process</li>
<li><code>&lt; -1</code>: send signal to process group dilineated by the absolute value of
the supplied pid</li>
<li><code>-1</code>: send to all processes you have permission to send to, excluding
<code>init</code>/<code>launchd</code> (pid 1) and the calling process. AKA broadcast
signals.</li>
</ul>


<p><code>init</code>/<code>launchd</code> can only be sent signals for which it has signal
handlers, to prevent accidental killing of this precious process.</p>

<p><code>CAP_KILL</code> is the privilege that allows a process to send a signal to
whatever.</p>

<p><code>SIG_CONT</code> is special in that it can be sent by unprivileged processes
to any process in the same session to allow for job-control shells to
restart stopped jobs that have changed their user IDs.</p>

<p>Null signal 0 can be used to test if a process exists and we can send a
signal to it. Of course you might troll yourself given that pids are
recycled.</p>

<p>Plain ol (non-realtime) signals don&#8217;t queue; they just internally set a
bit in the kernel and fire next time that process runs. This is why
multiple signals fired very quickly might get coalesced into one</p>

<h2>How to check if process is still running</h2>

<ul>
<li>(for child processes) <code>wait</code></li>
<li>semaphores and exclusive file locks; if you know a process locks a
file, and you can acquire the lock or semaphore, you know it must be
dead. I&#8217;m guessing pidfiles work this way?</li>
<li><code>stat</code> <code>/proc/PID</code>.</li>
<li>IPC sockets go dead, etc</li>
</ul>


<h2>async-signal-safe</h2>

<p>A function is async-signal-safe if</p>

<ol>
<li>it&#8217;s reentrant</li>
<li>it can&#8217;t be interrupted by signal handler</li>
</ol>


<p>Signal handlers should ideally only call async-signal-safe functions.
Note that if you have a signal handler installed for multiple signals,
or if you have <code>SA_NODEFER</code>, a handler function might get simultaneously
and might not be reentrant relative to itself if it mucks with globals,
even if the main app code doesn&#8217;t touch those globals.</p>

<p>In C-land (and Java) you can use the volatile keyword to prevent
variables from living in registers, or at least guaranteeing that a
write to a variable goes all the way to memory, which is an important
guarantee for when threads share and write to a global variable.</p>

<p>In JRuby, the Atomic gem is useful as a lock-free test-and-set way of
performing an update to a value, and detecting/retrying if someone else
interrupted your operation.</p>

<p>http://moonbase.rydia.net/mental/blog/programming/atomic-operations-in-ruby.html</p>

<p>Parallelism is moronically hard.</p>

<h2>Signal handler recovery</h2>

<p>You can do <code>setjmp</code> and <code>longjmp</code>&#8230; cray crqy.</p>

<p>When the kernel runs a handler, it adds a bit to the <code>sa_mask</code> and then
removes it when handler returns, so if you <code>longjmp</code>, you need to make
sure to unset it. (Presumably NODEFER prevents this bit from being set,
allowing the handler to be called multiple times?)</p>

<h2>Signal Stack</h2>

<p>Processes have a signal stack separate from the regular stack for
running signal handlers. You can use <code>sigaltstack</code> to change it to some
preallocated region, useful for when you want to catch SIGSEGV on a
stack overflow (which means there&#8217;s no more room on the process stack
for a signal frame/stack).</p>

<h2>Interrupted syscalls</h2>

<p>Blocking reads are syscalls, and if a signal hits an app during such a
syscall, when the signal handler returns and normal process flow
continues, that syscall will yield an <code>EINTR</code>. In Ruby this results in a
<code>Interrupt &lt; SignalException &lt; Exception</code> being raised. So you have to
jump through some hoops, put things in loops, to handle/ignore <code>EINTR</code>
on blocking syscalls. Orrr you can use <code>SA_RESTART</code> when establishing
signal handlers w <code>sigaction</code> so that the kernel automatically restarts
interrupted syscalls post signal handler.</p>

<p>Just realized something: on Ruby, if you don&#8217;t trap a (non-terminating)
signal handle, it&#8217;ll fire as an exception that you can caught. That&#8217;s
crazy. This must mean Ruby internally traps all trappable signals and
then checks them against any registered trap handlers in Rubyland.</p>

<h2>Framework blackboxing</h2>

<p>Exceptions in blackboxed frameworks don&#8217;t pause. So you can add
blackboxed frameworks in Chrome inspector options to ignore a framework
that keeps throwing shit. Probably other thins too; TODO: research!</p>

<h2>Cryptonomicomicon</h2>

<p>Page 17.</p>

<p>lambent: glowing, gleaming, or flickering with a soft radiance</p>

<p>gnomon: the projecting piece on a sundial that shows the time by the
position of its shadow.</p>

<h2>Why <code>sysctl</code>?</h2>

<p>In my Linux learnings I was curious as to why some settings seemed to be
stored in files while others seemed to be stored via separate utilities
like <code>sysctl</code>. Turns out <code>sysctl</code> is just a wrapper around <code>/proc/sys</code>,
and the setting variable names e.g. <code>a.b.c.d</code> would map to
<code>/proc/sys/a/b/c/d</code> and the setting would be the contents of that file.</p>

<p>So as far as global structures go, I know about:</p>

<ul>
<li>Environment variables</li>
<li>Shell variables (basically non-exported env vars, don&#8217;t get shared
with child processes).</li>
<li>Shell settings (e.g. ulimit, a per-shell set of defaults used when
invoking child processes <em>from that shell</em>; it&#8217;s not like you&#8217;re
setting system-wide settings when you set <code>ulimit</code>)</li>
<li><code>procfs</code></li>
<li>Probably lots more file based stuff I&#8217;m not thinking of. If it&#8217;s
really system-wide global stuff, it must live in a file somewhere,
right? Everything is a file?</li>
</ul>


<h2><code>TASK_UNINTERRUPTIBLE</code></h2>

<p>A risky process state, usually for operations that are quick to
complete, like waiting for a syscall to finish flushing data to disk,
but if there&#8217;s some issue, NFS or kernel bug, it might result in a hung
task that cannot be killed without a system restart. Another state,
<code>TASK_KILLABLE</code>, was added to prevent this, and it was first applied to
hung NFS.</p>

<h2>Hardware signals</h2>

<p>Linux force-delivers hardware signals even if they&#8217;re set to ignore.
Returning from a signal handler means undefined behavior because likely
the machine instruction that caused the interrupt is going to be
retried, likely causing an infinite loop. You need to <code>_exit</code> (because
<code>exit</code> flushes stdio buffers which might be locked and cause problems,
not to mention that <code>exit</code> fires <code>onexit</code> hooks) or do a <code>siglongjmp</code> to
guarantee that you&#8217;re skipping over the broke ass machine instruction.</p>

<h2>Timers</h2>

<p>3 types of timers per process: real (wall clock), virtual (user CPU),
and profile (user+kernel).</p>

<p>You can set timers on read operations by omitting <code>SA_RESTART</code> from the
<code>SIGALRM</code> registration.</p>

<h2><code>sleep</code></h2>

<p>This could be implemented otherwise with <code>sigsuspend()</code> (which allows
you to specify a temporary signal mask and block until a signal arrives
to wake it up).</p>

<p>You can use <code>sleep</code>, or <code>nanosleep</code>. <code>nanosleep</code> is limited by
resolution of software clock which uses units of time called jiffies&#8230;?
Jiffies are the amount of time the kernel lets a process execute, in HZ.
For a long time it was 100 HZ. So a jiffy was 1% of a second, or 10ms.
That seems crazy big! Maybe not? But if video games wanna run at 60 fps,
and they have to share the CPU with other things, they&#8217;re gonna need
larger jiffy. Because if the video game is alternating with only a
single other process (and in real life it should be many other
processes), then it couldn&#8217;t achieve more than 50 fps. Soooo sounds like
there&#8217;s something I don&#8217;t understand. The Jiffies did get smaller.</p>

<h2>SSLMate</h2>

<p>Buy SSL certs from your terminal. Why the fuck not?</p>

<p>https://sslmate.com/</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Crowley Journal]]></title>
    <link href="http://machty.github.com/blog/2014/10/06/crowley-journal/"/>
    <updated>2014-10-06T21:15:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/06/crowley-journal</id>
    <content type="html"><![CDATA[<h2>Comptroller</h2>

<p>Basically the CFO of public institutions. In charge of audits,
accountability, etc.</p>

<h2>Fetch remote branch</h2>

<p>http://stackoverflow.com/questions/945654/git-checkout-on-a-remote-branch-does-not-work</p>

<p>I was burned by this.</p>

<p>Basically, I had <code>origin</code> in the <code>+fetch</code> section when I wanted the new
branch in there</p>

<pre><code>fetch = +refs/heads/*:refs/remotes/origin/*
</code></pre>

<p>TODO: understand this fetch crap. I still don&#8217;t grok it.</p>

<h2>Amazon SES</h2>

<p>http://aws.amazon.com/ses/</p>

<p>Simple Email Service.</p>

<h2>Bash run last command substitute</h2>

<pre><code>$ ls lol
ls: wat: No such file or directory
$ ^wat^lol
ls lol
ls: lol: No such file or directory
</code></pre>

<h2>Harp. What is it?</h2>

<p>I don&#8217;t know. You tell me.</p>

<p>http://harpjs.com/</p>

<p>Static web server with built in preprocessing.</p>

<p>https://github.com/silentrob/harp-editor</p>

<h2>Candy kid</h2>

<p><img src="http://kandipatterns.com/images/kandikids.jpg" alt="" /></p>

<h2>Lissajous curve</h2>

<p>http://en.wikipedia.org/wiki/Lissajous_curve</p>

<pre><code>x=A\sin(at+\delta),\quad y=B\sin(bt),
</code></pre>

<p>Family of curves that seem bounded by a (-1,-1)x(1,1) box. What am I
doing with my life.</p>

<p>Source: cryptonomicon, bumble bee doing Lissajous across the ceiling.</p>

<h2>CSS history theft</h2>

<p>http://lcamtuf.coredump.cx/css_calc/?utm_source=html5weekly&amp;utm_medium=email</p>

<ul>
<li>Til mid 2010, a crappy site could use the <code>:visited</code> pseudo class to
render a bunch of URLs and then check the url to see if the <code>:visited</code>
styles were applied.</li>
<li>this loophole was closed by only letting specify color attributes, and
prevent color lookup via <code>getComputedStyle</code>.</li>
<li>loophole remains open for clicking a single URL, which doesn&#8217;t scale
too well.</li>
</ul>


<h2>Milquetoast</h2>

<p>Based on comic cartoon character Caspar Milquetoast, a timid, weak-willed old man,
whose namesake is breakfast comfort food consisting of toast and a white
milk-based sauce.</p>

<p>Millhouse is milqtoast, kinda, but more dweeby. Edward Norton in Fight
Club is milquetoast.</p>

<h2>Gone Girl</h2>

<p>I should see it. Once book, now David Fincher movie that makes people
not want to get married.</p>

<h2>504 Gateway Timeout</h2>

<p>Never knew what this meant, but this can be returned by an intermediary
or proxy server that doesn&#8217;t get a timely response from a backend server
it&#8217;s talking to in order to prepare a response to the client. DOS&#8217;d
servers might see this, wherein their nginx/whatever reverse proxy can
handle the load just fine, but the backend server is inundated and can&#8217;t
keep up with the load.</p>

<h2>Cairns</h2>

<p>City in far north of Queensland, Australia. Scuba.</p>

<h2>FastCGI</h2>

<p>http://en.wikipedia.org/wiki/FastCGI</p>

<p>CGI applications are processes spun up by a web server to handle an
incoming request. Unscalable since spinning up processes all the time
takes a toll on the OS, not to mention that there&#8217;s no way to do
resource sharing (DB connection sharing, in-memory caching (because
the process dies at the end of request)).</p>

<p>With FastCGI, there&#8217;s a persisting FastCGI server that owns all of the
CGI programs, and webservers interact with FastCGI via a binary protocol
(over a socket (local) or TCP connection (remote)).</p>

<p>This decoupling also allows smaller components to be restarted (rather
than having to restart the entire web server).</p>

<p><code>mod_php</code> is another answer to this problem, which takes the approach to
embedding the PHP interpreter inside Apache itself. But each Apache
child then needs to load the interpreter. FastCGI is better on memory
and not constrained to Apache (nginx implements it).</p>

<p>TODO: reverse proxy rails configuration vs Rails attached to FastCGI.</p>

<h2>Copy as curl</h2>

<p>You can copy a resource request made by Chrome as a curl command.</p>

<h2>HAR format</h2>

<p>You can save network requests (request data/headers and response etc) as
a HAR files from the Chrome debugger.</p>

<p>Stands for &#8220;HTTP ARchive&#8221;:
http://www.softwareishard.com/blog/har-12-spec/</p>

<p>HARs are JSON documents containing</p>

<h2>bcrypt stretches</h2>

<p><code>Devise.stretches</code> controls the internal bcrypt stretching amount for
bcrypt password hashing. When <code>RACK_ENV=test</code>, <code>Devise.stretches</code> is 1,
else it is 10 (or some higher number).</p>

<p>Increasing stretches drastically increases the CPU time required for
hashing passwords, which seems like an obvious anti-perf except for the
nice fact that if you&#8217;re server&#8217;s being bombed by a dictionary attack,
which sequentially tries to hash many passwords, the attack will be
thwarted by just how long the server takes to hash the password.</p>

<p>http://en.wikipedia.org/wiki/Key_stretching</p>

<h2>ActiveRecord connection pooling</h2>

<p>http://blog.plataformatec.com.br/2011/12/three-tips-to-improve-the-performance-of-your-test-suite/</p>

<p>Connection pools allow for connection reuse (rather than, say, the
shitty alternative of opening/closing a connection for every query).
The ActiveRecord connection pool has the additional stipulation that
connections are not shared between threads. This makes sense, since
otherwise if you had a multithreaded Rails application and connections
were shared between threads, then only one thread at a time could be
querying the database, and locking would occur, making shit slow.</p>

<p>So by letting each thread have its own connection to a shared resource,
threads won&#8217;t get blocked on each other. Happy fucking day.</p>

<h2>Rails dbconsole</h2>

<p>http://guides.rubyonrails.org/command_line.html#rails-dbconsole</p>

<pre><code>rails dbconsole
# or
rails db
</code></pre>

<h2>dRuby</h2>

<p>COBRA for Ruby basically.</p>

<p>http://www.ruby-doc.org/stdlib-1.9.3/libdoc/drb/rdoc/DRb.html</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Smelly Journal]]></title>
    <link href="http://machty.github.com/blog/2014/10/01/smelly-journal/"/>
    <updated>2014-10-01T08:29:00-04:00</updated>
    <id>http://machty.github.com/blog/2014/10/01/smelly-journal</id>
    <content type="html"><![CDATA[<h2>TCP/UDP/Sockets/wat</h2>

<p>What&#8217;s a socket?  Depends on who you ask.</p>

<p>The <a href="file:///Users/machty/code/machty.github.com/rfc793.html">TCP RFC</a>
defines a &#8220;socket&#8221; as an IP address (supplied in IP packet) combined
with a port (supplied in the TCP packet). So a &#8220;socket&#8221; is
(IP,PORT). Socket + Socket = Connection. Hence, in this sense, you can
use the same socket for multiple connections.</p>

<blockquote><p>To allow for many processes within a single Host to use TCP
communication facilities simultaneously, the TCP provides a set of
addresses or ports within each host.  Concatenated with the network
and host addresses from the internet communication layer, this forms
a socket.  A pair of sockets uniquely identifies each connection.
That is, a socket may be simultaneously used in multiple
connections.</p></blockquote>

<ul>
<li>Socket: an operating system abstraction referring to a communication
endpoint. You can read from it. You can write to it.</li>
<li>Note that raw sockets don&#8217;t have ports (ports are a UDP/TCP concept).</li>
</ul>


<h2>Built-ins aren&#8217;t sudo-able</h2>

<pre><code>sudo: cd: command not found
</code></pre>

<p>:). Makes sense. You either need to use absolute paths or <code>sudo bash</code>.</p>

<h2>Switch into another user</h2>

<pre><code>sudo su - machty
</code></pre>

<h2>socket bind vs listen vs accept</h2>

<p>Why so many steps? What does each one do?</p>

<h3>Bind</h3>

<p>Registers a socket with the kernel. Even if you do nothing with that
socket, once you bind, anyone else who tries will get an EADDRINUSE.
This code gets an EADDRINUSE every time.</p>

<pre><code>require 'socket'
def wat
  socket = Socket.new(:INET, :STREAM)
  addr = Socket.pack_sockaddr_in(4485, '0.0.0.0')

  socket.bind(addr)
end

fork { wat }
wat
Process.wait
</code></pre>

<p>If the socket had been created, then a new file descriptor pointing to
the same socket could be used by the forked child process, but you can&#8217;t
just create a totally separate one that points to the same already-bound
port.</p>

<h3>Listen</h3>

<p><code>listen()</code> indicates a &#8220;willingness to accept incoming connections&#8221;.
You provide <code>listen</code> with a <code>backlog</code> integer referring to the max
number of queued connections allowed before you ECONNREFUSED.</p>

<p>It&#8217;s separate from <code>bind</code> because maybe you want to grab a port right at
application start, but you still need to do some thing before you know
a) whether to actually start listening or b) how big the queue size
should be.</p>

<p>It&#8217;s separate from <code>accept</code> because <code>accept</code> kind of a dead end as far
as application initialization goes; once you start <code>accept</code>ing, you must
be all set up and fine with the fact that you&#8217;ll be blocked on IO.</p>

<p>SOMAXCONN is the max backlog number. Probably best to just set
<code>listen</code>&#8217;s backlog to that number. If you&#8217;ve got people queued up for
connections, you should probably be fixing whatever&#8217;s causing the wait.</p>

<h3>Accept</h3>

<p><code>accept</code> will block.</p>

<p>So basically here&#8217;s what happens if you try to connect to a server
that&#8217;s ran through the following steps (followed by a sleep unless
<code>accept</code> is called):</p>

<ul>
<li>no <code>bind</code>: ECONNREFUSED</li>
<li><code>bind</code>: client blocks. If you kill the server, ECONNREFUSED</li>
<li><code>listen</code>: client blocks, reports a successful connection (which means
that <code>listen</code> makes the kernel start accepting connections even if no
one&#8217;s accepting&#8230; this makes sense since at this point we&#8217;ve provided
the kernel a max queue size)</li>
</ul>


<p>If a connection comes in, the kernel will do the handshake, but the
connection will remain in the queue and future connection attempts won&#8217;t
get the handshake until the first one is <code>accept</code>ed.</p>

<p>Does that make sense? The alternative would be the kernel not responding
to the handshake until <code>accept</code>, which seems like a major performance
hit.</p>

<h2>Elastic Beanstalk</h2>

<p>Tries to do what Heroku does. Apparently a clunkier CLI. But neato
features.</p>

<h2>Kernel buffers</h2>

<p>C and Ruby provide buffering on top of a kernel buffers. Why not just
have one buffer? Because the kernel definitely needs to provide
buffering in general so that you can get the perfs for free. But
application specific stuff can benefit from buffering on top of the
kernel&#8217;s defaults, but unfortunately the system calls are megaslow.</p>

<p>Kernel maintains a kernel thread that makes sure that nothing remains
unflushed for more than 30 seconds and will flush things.</p>

<p>You can <code>fadvise</code> the kernel about how reads/writes are likely to
occur so that it can select the best strategy for what you&#8217;re doing.</p>

<p>There&#8217;s an <code>madvise</code> memory equivalent. Wouldn&#8217;t you just write your own
allocator at that point? Guess I should probably learn my shit.</p>

<p>Different types of strategies:</p>

<ul>
<li>NORMAL: no special patterns to report, default kernel behavior.
Read-ahead buffer set to 128k.</li>
<li>SEQUENTIAL: you&#8217;re gonna be reading from lower to higher offsets.
Large files, streaming large music files, blah blah blah, etc., on
linux this yields read-ahead window size 2x the default.</li>
<li>RANDOM: scattershot reads, hence read-ahead is likely a waste, hence
it is disabled.</li>
<li>WILLNEED: notify kernel that you&#8217;ll need a segment of memory soon, so
kernel loads it into the buffer cache (redundant much?). Memory
presurre from other processes might eject these buffers from the
cache, so it&#8217;s good to make sure you <code>read</code> soonish after this
fadvise.</li>
<li>WONTNEED: flush a region if possible</li>
<li>NOREUSE: WILLNEED + WONTNEED, basically. You only expect to read the
region once.</li>
</ul>


<h2>Unzip a curled tar</h2>

<pre><code>curl https://opensource.apple.com/tarballs/bash/bash-92.tar.gz | tar zxf -
</code></pre>

<p>Pretty amazing.</p>

<h2><code>O_DIRECT</code>: bypass the buffer cache</h2>

<p>Often this means slower performance, but if you reaaaally know what
you&#8217;re doing, you can use it. If you use <code>O_DIRECT</code>, here&#8217;s what you
lose:</p>

<ul>
<li>sensible read-aheads</li>
<li>sharing buffers between processes using the same files</li>
<li>other things</li>
</ul>


<p>Database software probably wants <code>O_DIRECT</code> because databases have
unique IO requirements, maintain their own caches, etc.</p>

<p>You have to start paying attention to things like alignment
restrictions. The databuffer must begin on a memory boundary that is a
multiple of block size. I guess that means you have to shift things in
memory before writing to disk. File offset must also be multiple of
block size. Length of data transferred must also be multiple. Else
<code>EINVAL</code>.</p>

<h2>Write-ahead log</h2>

<p>http://en.wikipedia.org/wiki/Write-ahead_logging</p>

<p>Postgres et al use this to provide atomicity / durability. With WAL, all
modifications are written to a log before applied. Redo/undo information
is written to the log.</p>

<p>http://en.wikipedia.org/wiki/Point-in-time_recovery</p>

<p>Restore a previous state in time. Time Machine is OS X is an example of
this (also why they allow directory hard-linking).</p>

<h2>Virtual devices and device files</h2>

<p>Expose universal IO API.</p>

<p>There are character devices like TTYs which can handle a character at a
time, and there are block devices, which can handle IO in (often 512b)
blocks.</p>

<p>Use <code>mknod</code> for making device files. Used to be used for other things,
now it&#8217;s just device files.</p>

<p><code>ls -l</code> displays major or minor number of special device fields in place
of the size (from <code>man ls</code>)</p>

<pre><code> If the file is a character special or block special file, the major and
 minor device numbers for the file are displayed in the size field. 
</code></pre>

<h2>Cryptonomicomicon</h2>

<p>Words.</p>

<p><a href="http://en.wikipedia.org/wiki/Coolie">Coolie</a>: a laborer from Southern
China, Indian subcontinent, Phillipines or Indonesia. Nowadays a
racial slur for people of Asian descent (mostly in South African
vernacular). Etymology not agreed upon, could be from an Urdu word for
&#8220;slave&#8221; or in reference to the Koli, or a Tamil word that meant payment
for work. Coolie trade pretty much was like slave trade, with indentured
servitude, etc. In Crypto they&#8217;re the workers who trade various Shanghai
banks silver-backed paper money in for silver, transporting large boxes
of currency hanging from bamboo sticks.</p>

<p><a href="http://en.wikipedia.org/wiki/Claque">Claque</a>: people paid to applaud
(or heckle) a performance. Basically, real-time proto-laugh tracks,
originating from France. There were laughers (Rieurs), criers
(Pleureurs), ticklers (Chatouilleurs, who kept the audience in high
spirits), and encore-ers (bisseurs). North Korea&#8217;s got lots of claquers.
Entrenched claque-masters also extorted money from performers in
exchange for not booing their performance. Fucking dickweeds.</p>

<p>Swabbie: &#8220;a member of the navy, typically one who is of low rank.&#8221;</p>

<p>Estuary (esturial): the tidal mouth of a large river, where the tide
meets the stream.</p>

<h2>Number authorities</h2>

<p>Kinda like the IANA and port numbers, there&#8217;s a Linux
http://www.lanana.org for controlling things like major version numbers
for device files.</p>

<h2>Silicon Valley</h2>

<p>Turns out I never knew where it was: it&#8217;s the southern portion of the
San Francisco bay area.</p>

<h2>Disk Partitions</h2>

<p>Hard disks have platters, platters have tracks of data, split into
sectors, split into physical blocks.</p>

<p>Platters > data > sectors > blocks.</p>

<p>Blocks are the smallest unit of data a driver can read/write. They&#8217;re
usually 512 bytes.</p>

<p>Disk partitions are treated by the kernel as separate devices in
<code>/dev</code>.</p>

<p>Partitions might consist of</p>

<ul>
<li>a file system</li>
<li>totally raw data (some databases do this)</li>
<li>swap area used by kernel for VM</li>
</ul>


<p>File system on disk is composed of</p>

<ul>
<li>Boot block: how to boot the OS</li>
<li>Superblock: single block saying:

<ul>
<li>size of inode table</li>
<li>logical block size (always a multiple of physical block)</li>
<li>size of the system in logical blocks</li>
</ul>
</li>
<li>inode table / ilist</li>
<li>data blocks</li>
</ul>


<p><code>ext2</code> is special in that it breaks down data blocks into block groups
and prepends a copy of the super block to each to minimize seeks.</p>

<p><code>ls -li</code> shows inode number in first column. inodes contain:</p>

<ul>
<li>file type</li>
<li>owner</li>
<li>group</li>
<li>access perms</li>
<li>timestamps

<ul>
<li>last access</li>
<li>last mod</li>
<li>last status change</li>
<li>(but not a created at? WEIRD. must be a linux thing; mac has this)</li>
</ul>
</li>
<li>number of hardlinks</li>
<li>size of file in bytes</li>
<li>number of blocks allocated (might be less than expected due to holes)</li>
<li>pointers to blocks</li>
</ul>


<p>Note that there is a tradeoff between fragmenting freespace into small
chunks that are too small to use vs fragmenting data blocks so that all
free small chunks can be used.</p>

<p>ext2 has 15 pointers, 12 of which are data block pointers, the next
which points to a block of pointers, the next which points to a an array
of block pointers, and then a tertiary one for large fucking files.</p>

<p>This (using blocks for inode indirection) allows for fixed size
of inode table.</p>

<h2>TLDP</h2>

<p>The Linux Documentation Project</p>

<blockquote><p>LDP is a loosely knit team of volunteers who provide documentation for many aspects of Linux. There are several forms of documentation: Guides, HOWTOs, man pages, and FAQs.</p></blockquote>

<p>That&#8217;s nice.</p>

<h2>McRouter</h2>

<p>https://www.youtube.com/watch?v=EYhcumt8YyI</p>

<p>A &#8220;memcache protocol router, for scaling memcache deployments&#8221;.</p>

<p>Two forms of caching at facebook:</p>

<ul>
<li>demand-filled cache

<ul>
<li>fetch from memcache with key. if not there, fetch from db, then set
memcache w key. all of this logic on webserver</li>
</ul>
</li>
<li>read-through / write-through cache

<ul>
<li>TAO system; query TAO, and if it&#8217;s not there, <em>it</em> fetches from DB
on your behalf</li>
</ul>
</li>
</ul>


<p>Both share the following in common: two orders of magnitude more reads
than writes.</p>

<p>Cache becomes bottleneck, extremely crucial or whole site fails.</p>

<p>Having inconsistent implementations of memcache libs might result in
data loss. So, anyway, McRouter to the rescue. It:</p>

<ul>
<li>is a proxy between memcache client and server (adheres to same ol API,
makes for good drop-in replacement)</li>
<li>also an embedded mode for low latency</li>
<li>a load balancer</li>
<li><a href="http://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a>
(where hash table resizes minimize the amount of remapping:

<ul>
<li>Consistent hashing maps objects to the same cache machine, as
far as possible. It means when a cache machine is added,
it takes its share of objects from all the other cache
machines and when it is removed, its objects are shared
between the remaining machines.</li>
</ul>
</li>
<li>connection pooling (presumably this means connections are maintained
by mcrouter to the memcache instances and loaned out when mcrouter
clients make queries, rather than slowly reopening connections each
time)</li>
<li>Server pools&#8230; I guess this means multiple instances of McRouter?</li>
<li>Automatic failover</li>
<li>Cold cache warmup</li>
<li>Broadcast operations</li>
<li>Replicated Data Sets</li>
</ul>


<p>Connection Pooling:</p>

<p>Application threads share McRouter&#8217;s N connections to the memcache
instances, rather then each thread creating N connections resulting in
<code>N*T</code> total connections.</p>

<p>Heterogeneous workloads:</p>

<p>Prefixed keys get routed to specific memcache instances, I guess, rather
than having them all on the same things getting clobbered by others.</p>

<p>Automatic failover:</p>

<p>Normal server, fail over to backup when error returned, timeout,
ECONNREFUSED, etc, with probing to see when it can be reconnected.</p>

<p>Twitter <code>twemproxy</code> is similar to McRouter.</p>

<h4>Reddit infrastructure</h4>

<ul>
<li>AWS</li>
<li>170-300 servers daily (scales in peak hours)</li>
<li>73 cache nodes with 1TB memory</li>
<li>App code fragile-y uses memcache</li>
</ul>


<h4>Reddit Scaling Issues</h4>

<p>AWS constantly releases lovely new instance types, but Reddit can&#8217;t just
hop onto them without testing them out.</p>

<h2>NTFS</h2>

<p>New Technology File System, as in, Windows NT.</p>

<h2>VFS</h2>

<p>Virtual File System: the unified kernel abstraction, providing familiar
things like <code>read()</code>, <code>write()</code>, etc.</p>

<h2>Journaling</h2>

<p>If an OS running ext2 crashes, you need to run <code>fsck</code> (filesystem check)
to make sure inode entries point to real things, etc., else you run the
risk of breaking more shiznittletons.</p>

<p>Journaling FS&#8217;s imply basic database transactions. All intended metadata
writes are written to a journal file first before they&#8217;re performed, and
if the operation is interrupted by a crash, recovery is quick. Actually,
could just be metadata or full on everything data.</p>

<h2>OS X</h2>

<p>Macs use something called HFS Plus, or Mac OS Extended.</p>

<ul>
<li>lots of metadata</li>
<li><p>case-preserving though case-insensitive</p>

<p>  machty.github.com :: cat woot
  lol
  machty.github.com :: cat WOOT
  lol</p></li>
<li><p>journaling</p></li>
</ul>


<h2>mount</h2>

<p><code>mount</code> and <code>unmount</code> will attach a file system to a specified
directory.</p>

<p>KEEP IN MIND DINGUS this &#8220;mount&#8221; concept is all over the place. Mount
routes. Mount external thing blah.</p>

<p><code>unmount2</code> is <code>unmount</code> with flags.</p>

<p>You can mount in multiple mount points.</p>

<pre><code>mount /dev/sdva123 /dumbness
</code></pre>

<p>A device is not accessible as a file system until you mount it. You
couldn&#8217;t just do <code>cd /dev/sdva123</code>. You have to mount it first.</p>

<p>Just realized something: the EBS provided to your EC2 instance could be
formatted into any file system. FUCK IT let&#8217;s do ext4.</p>

<p>mounts can be stacked on top of the same mount point. This makes it
possible to migrate off of an old mount; old processes maintain their
file handles on the old mount, new processes use the new fs, and
eventually you can retire the old mounted fs that&#8217;s not at the top of
the stack. Kinda nifty, weird as it sounds.</p>

<h2>Basic <code>cat</code> shit</h2>

<p>If you want to type some random shit into a new file, do</p>

<pre><code>cat &gt; lolz
</code></pre>

<p>and then you can type into cat&#8217;s STDIN and then Control D to signal EOF.</p>

<h2>kitchen sink of disk utils</h2>

<p>What&#8217;s the difference between all these shits?</p>

<ul>
<li>fdisk: manipulate disk partition table</li>
<li>df: report file system disk space usage</li>
<li>mkfs: build a linux filesystem</li>
</ul>


<h2>tmpfs</h2>

<p>Virtual memory file system. But the thing about virtual memory is that
unused portions might get paged to a swap file on the disk&#8230; so it&#8217;s
not just strictly memory.</p>

<pre><code>sudo mkdir ./wat
sudo mount -t tmpfs StupidName ./wat
</code></pre>

<p><code>tmpfs</code> doesn&#8217;t exist on Mac OS X though some other form probably does.</p>

<p>It&#8217;s used to speed up applications like compilers that make heavy use of
<code>/tmp</code>.</p>

<p><code>tmpfs</code> has also had other use cases, such as implementing shared
memory (System V) and the <code>glibc</code> implementation of POSIX shared memory
and POSIX semaphores.</p>

<h2>Ableton Live</h2>

<ul>
<li>Opt-shift-B: hide Browser</li>
</ul>


<p>Arrangement view is the classic multi track view. You can disable
portions of the mixer that you don&#8217;t care about.</p>

<ul>
<li>IO (opt + command + i): input/output for this track

<ul>
<li>Input type (device, e.g. M-Audio Whatchufuck)</li>
<li>Input channel (devices often have multiple)</li>
<li>Monitoring (radio button basically)

<ul>
<li>In (enable monitoring of input)</li>
<li>Auto (only enable when track is armed - not just when actively
recording)</li>
<li>Off</li>
</ul>
</li>
</ul>
</li>
<li>Delay (no shortcut)

<ul>
<li>delay/predelay to compensate for device/hardware/whatever latency</li>
</ul>
</li>
<li>Mixer (opt + command + m)

<ul>
<li>Activate/deactivate (mute) the track</li>
</ul>
</li>
<li>Return tracks

<ul>
<li>Note that if you tab into clip view (?) then the R button splits
into S and R.</li>
</ul>
</li>
</ul>


<p>Note the master track at the bottom. The Preview/Cue volume controls the
metrononme. Not sure what else Cue refers to&#8230;</p>

<p>Count-in is attached to the metronome menu.</p>

<p>Opt+unfold (triangle button) to unfold all them shits.</p>

<p>You can arm multiple tracks with command+click.</p>

<p>Impulse is an Ableton instrument. Instrument Racks are combinations of
the Impulse instrument with certain pre-saved audio effects.</p>

<h3>Quantize</h3>

<p>This doesn&#8217;t seem to refer to individual notes in a midi recording but
rather for playback between clips; when you press the play button by a
clip, it doesn&#8217;t immediately play unless you have it turned off (which
can result in crazy timing issues). Also, even if you Command-0 to turn
it off and get shit out of sync, if you Command-9 back to one bar and
play a new track, it&#8217;ll make sure it starts in sync with the metronome
(what else could it be? clips don&#8217;t have meter, just a length).</p>

<p>Ah but if you do want per note quantization you can do it a) while
recording, by going to Edit > Record Quantization.</p>

<p>Then if you already recorded something unquantized, you can Command-U.</p>

<h3>Recording clips</h3>

<p>If you create an empty clip, that clip has a size, as all do, so you&#8217;ll
be recording within a loop.</p>

<p>Whereas if you pressed the slot&#8217;s record button, there would be no fixed
length until you stopped recording. The Notes menu associated with that
clip (which has length info) doesn&#8217;t even appear until after the new
recording is finished. How grand!</p>

<h3>Looping</h3>

<ul>
<li>One-indexing: the quickly set things to the beginning of a clip, you
do 1 tab 1 tab 1, not 0 tab 0 tab 0 which&#8217;ll normalize to -1. But
lengths can have zero. 2 0 0 means two measures.</li>
<li>Known trick: 0 0 16 will normalize into 1. If it&#8217;s more convenient to
think in terms of odd meters, this could be useful&#8230; INSTANT
MESHUGGAH.</li>
<li>When loop enabled, normal end is meaningless. When loop disabled, all
loop info is useless. (Though these details still seem to be
displayed.</li>
</ul>


<p>Place insert markers just by clicking the grid.</p>

<p>You can manually copy and paste or you could DuplLoop. DuplLoop also
conveniently works such that if you duplicate into notes you&#8217;ve already
written, they&#8217;re overwritten, rather than dubbed or something cray cray.</p>

<h3>Hotswapping</h3>

<p>Constrasted with double-clicking a new instrument in the Browser,
hotswapping seems to:</p>

<ul>
<li>preserve the rest of the devices you might have attached to the
instrument you&#8217;re hotswapping.</li>
<li>open the browser to the current devices folder; relevant for quickly
finding the thing you want to switch out.</li>
</ul>


<p>Weird that they make such a big deal out of it (do they, or am i just
saying).</p>

<h2>SHIT IS EBS BACKED, DUMMY</h2>

<p>Root device type: ebs</p>

<p>Root device: <code>/dev/xvda</code></p>

<p><code>#NotAllEC2InstancesStartOffWithEphemeralStorage</code></p>

<h2>Helicopter parent</h2>

<p>http://en.wikipedia.org/wiki/Helicopter_parent</p>

<p>A hovery overbearing parent who&#8217;s still around even when kids have gone
to college.</p>

<p>Thought it might mean when you&#8217;re a parent who drops in every so often,
or drops in on other kids. Nope.</p>

<h2>Geocoder can&#8217;t have radiuses be stored on the row</h2>

<p>The bounding box is calculated before the SQL is fired&#8230; then again
there&#8217;s all sorts of sin/cos shit going on, but all to translate into
something that can be compared against that box.</p>

<h2>College</h2>

<p>http://www.nytimes.com/2014/05/18/magazine/who-gets-to-graduate.html</p>

<p>Community College:</p>

<p>Means different things depending on the country, but in America it
usually means a two-year public institution, often granting certificates
and associate&#8217;s degrees (two-year degrees).</p>

<p>Dropout rate is apparently 40%, and if you include community college
dropouts, we&#8217;re the worst other than Hungary.</p>

<p>Rich kids are more likely to graduate (90% of the upper quartile of
income), whereas 25% of the lower half will expect to graduate by 24.</p>

<p>College students who scored the same on standardized tests still have
educational outcomes (graduation rates) that correlate with family
wealth.</p>
]]></content>
  </entry>
  
</feed>
